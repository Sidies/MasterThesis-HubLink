{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taxonomy Increments Analysis\n",
    "\n",
    "The following Notebook provides an analysis of the taxonomy increments. It summarizes for each iteration the categories and their metadata allowing precise tracking of the sources for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "tax_inc_dir = os.path.join(\"./taxonomy_increments.json\")\n",
    "cluster_inc_dir = os.path.join(\"../clustering/cluster_increments.json\")\n",
    "extraction_dir = os.path.join(\"../extraction/extraction.json\")\n",
    "\n",
    "with open(os.path.join(current_dir, cluster_inc_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    cluster_increments = json.load(f)\n",
    "\n",
    "with open(os.path.join(current_dir, tax_inc_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    taxonomy_increments = json.load(f)\n",
    "\n",
    "with open(os.path.join(current_dir, extraction_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    extraction = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of subclusters: 96\n"
     ]
    }
   ],
   "source": [
    "# First we prepare the metadata from the literature research\n",
    "# We can then get the metadata from a paper by using the title as a key\n",
    "metadata_mapping = {}\n",
    "for paper in extraction:\n",
    "    title = paper.get(\"title\", \"No title\")\n",
    "    year = paper.get(\"year\", \"No year\")\n",
    "    authors = paper.get(\"authors\", \"No authors\")\n",
    "    doi = paper.get(\"doi\", \"No doi\")\n",
    "    category = paper.get(\"category\", \"No category\")\n",
    "    domain = paper.get(\"domain\", \"No domain\")\n",
    "    metadata_mapping[title] = {\n",
    "        \"year\": year,\n",
    "        \"authors\": authors,\n",
    "        \"doi\": doi,\n",
    "        \"category\": category,\n",
    "        \"domain\": domain,\n",
    "    }\n",
    "\n",
    "# Here we prepare the summaries for the clusters. Note that the Clustering process has\n",
    "# two iterations and only in the second iteration the clusters are final. This is why\n",
    "# we only take the clusters from the second iteration.\n",
    "# Note that Clustering Iteration != Taxonomy Iteration\n",
    "cluster_summaries = {}\n",
    "amount_subclusters = 0\n",
    "for iteration in cluster_increments:\n",
    "    if not iteration.get(\"iteration\", -1) == 2:\n",
    "        continue\n",
    "\n",
    "    for cluster in iteration.get(\"clusters\", []):\n",
    "        cluster_id = cluster.get(\"id\", \"unknown\")\n",
    "        cluster_name = cluster.get(\"name\", \"unknown\")\n",
    "        cluster_info = cluster.get(\"info\", \"No info\")\n",
    "        subclusters = cluster.get(\"subclusters\", [])\n",
    "        subcluster_names = set()\n",
    "        cluster_sources = set()\n",
    "        cluster_domains = set()\n",
    "        cluster_years = set()\n",
    "        cluster_categories = set()\n",
    "        for subcluster in subclusters:\n",
    "            subcluster_types = subcluster.get(\"types\", [])\n",
    "            subcluster_id = subcluster.get(\"id\", \"unknown\")\n",
    "            subcluster_info = subcluster.get(\"info\", \"No info\")\n",
    "            subcluster_name = subcluster.get(\"name\", \"unknown\")\n",
    "            subcluster_names.add(subcluster_name)\n",
    "            subcluster_sources = set()\n",
    "            subcluster_domains = []\n",
    "            subcluster_years = []\n",
    "            subcluster_categories = []\n",
    "            amount_subclusters += 1\n",
    "            for type_dict in subcluster_types:\n",
    "                name = type_dict.get(\"name\", \"unknown\")\n",
    "                source = type_dict.get(\"source\", \"unknown\")\n",
    "                domain = metadata_mapping.get(source, {}).get(\"domain\", \"unknown\")\n",
    "                year = metadata_mapping.get(source, {}).get(\"year\", -1)\n",
    "                category = metadata_mapping.get(source, {}).get(\"category\", \"unknown\")\n",
    "                subcluster_domains.append(domain)\n",
    "                subcluster_years.append(year)\n",
    "                subcluster_sources.add(source)\n",
    "                subcluster_categories.append(category)\n",
    "            cluster_sources.update(subcluster_sources)\n",
    "            cluster_domains.update(subcluster_domains)\n",
    "            cluster_years.update(subcluster_years)\n",
    "            cluster_categories.update(subcluster_categories)\n",
    "            cluster_summaries[subcluster_id] = {\n",
    "                \"name\": subcluster_name,\n",
    "                \"parent_id\": cluster_id,\n",
    "                \"info\": subcluster_info,\n",
    "                \"sources\": subcluster_sources,\n",
    "                \"domains\": subcluster_domains,\n",
    "                \"years\": subcluster_years,\n",
    "                \"categories\": subcluster_categories,\n",
    "            }\n",
    "        cluster_summaries[cluster_id] = {\n",
    "            \"name\": cluster_name,\n",
    "            \"info\": cluster_info,\n",
    "            \"sources\": cluster_sources,\n",
    "            \"domains\": cluster_domains,\n",
    "            \"years\": cluster_years,\n",
    "            \"categories\": cluster_categories,\n",
    "        }\n",
    "print(\"Amount of subclusters:\", amount_subclusters)\n",
    "\n",
    "def print_category_summary(categories, tab_depth=1):\n",
    "    for dim_cat in categories:\n",
    "        cat_name = dim_cat.get(\"name\", \"Unknown Category\")\n",
    "        cat_desc = dim_cat.get(\"description\", \"No description\")\n",
    "        cat_clusters = dim_cat.get(\"based_on_clusters\", [])\n",
    "        print(\"\\t\" * tab_depth, \"++++++++++++++++\")\n",
    "        print(\"\\t\" * tab_depth, \"Category Name:\", cat_name)\n",
    "        print(\"\\t\" * tab_depth, \"Description:\", cat_desc)\n",
    "        if cat_clusters:\n",
    "            print(\"\\t\" * tab_depth, \"Based On Clusters:\")\n",
    "            for cl_id in cat_clusters:\n",
    "                cl_summary = cluster_summaries.get(cl_id, {})\n",
    "                print(\"\\t\" * (tab_depth + 1), \"****************\")\n",
    "                print(\"\\t\" * (tab_depth + 1), \"Cluster Name:\", cl_summary.get(\"name\", \"Unknown Cluster\"))\n",
    "                print(\"\\t\" * (tab_depth + 1), \"Sources:\", cl_summary.get(\"sources\", []))\n",
    "                print(\"\\t\" * (tab_depth + 1), \"Domains:\", cl_summary.get(\"domains\", []))\n",
    "                print(\"\\t\" * (tab_depth + 1), \"Years:\", sorted(cl_summary.get(\"years\", [])))\n",
    "                print(\"\\t\" * (tab_depth + 1), \"Literature Categories:\", cl_summary.get(\"categories\", []))\n",
    "        else:\n",
    "            print(\"\\t\" * tab_depth, \"Based On Clusters: None, added manually\")\n",
    "            \n",
    "        if dim_cat.get(\"categories\", []):\n",
    "            print(\"\\t\" * tab_depth, \"Subcategories:\")\n",
    "            print_category_summary(dim_cat.get(\"categories\", []), tab_depth + 1)\n",
    "\n",
    "def get_category_metadata(categories, dim_metadata):\n",
    "    \"\"\"\n",
    "    Recursively gets the metadata for a category and its subcategories\n",
    "    \"\"\"\n",
    "    for dim_cat in categories:\n",
    "        cat_clusters = dim_cat.get(\"based_on_clusters\", [])\n",
    "        if cat_clusters:\n",
    "            for cl_id in cat_clusters:\n",
    "                cl_summary = cluster_summaries.get(cl_id, {})\n",
    "                dim_metadata[\"sources\"].update(cl_summary.get(\"sources\", []))\n",
    "                dim_metadata[\"domains\"].update(cl_summary.get(\"domains\", []))\n",
    "                dim_metadata[\"years\"].update(cl_summary.get(\"years\", []))\n",
    "                dim_metadata[\"categories\"].update(cl_summary.get(\"categories\", []))\n",
    "        if dim_cat.get(\"categories\", []):\n",
    "            get_category_metadata(dim_cat.get(\"categories\", []), dim_metadata)\n",
    "\n",
    "def print_taxonomy_summary(iteration):\n",
    "    \"\"\"\n",
    "    Prints a summary for the given taxonomy iteration\n",
    "    \"\"\"\n",
    "    for tax_it in taxonomy_increments:\n",
    "        if not tax_it.get(\"iteration\") == iteration:\n",
    "            continue\n",
    "        iteration_info = tax_it.get(\"info\", \"No info\")\n",
    "        print(f\"Iteration description: {iteration_info}\")\n",
    "        print(\"Taxonomy:\")\n",
    "\n",
    "        for dim in tax_it.get(\"taxonomy\"):\n",
    "            dim_name = dim.get(\"name\", \"Unknown Dimension\")\n",
    "            dim_desc = dim.get(\"description\", \"No description\")\n",
    "            dim_metadata = {\n",
    "                \"sources\": set(),\n",
    "                \"domains\": set(),\n",
    "                \"years\": set(),\n",
    "                \"categories\": set(),\n",
    "            }\n",
    "            get_category_metadata(dim.get(\"categories\", []), dim_metadata)\n",
    "            print(\"________________________\")\n",
    "            print(\"Dimension Name:\", dim_name)\n",
    "            print(\"Description:\", dim_desc)\n",
    "            print(\"Overall Sources:\", len(dim_metadata.get(\"sources\", [])))\n",
    "            print(\"Overall Domains:\", len(set(dim_metadata.get(\"domains\", []))))\n",
    "            print(\"Overall Years:\", len(set(dim_metadata.get(\"years\", []))))\n",
    "            print(\"Overall literature Categories:\", len(set(dim_metadata.get(\"categories\", []))))\n",
    "            print(\"Number of Clusters:\", len(dim.get(\"categories\", [])))\n",
    "            print_category_summary(dim.get(\"categories\", []))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration description: No info\n",
      "Taxonomy:\n",
      "________________________\n",
      "Dimension Name: Graph Representation\n",
      "Description: Classifies questions based on how the information that is required to answer the question is organized in the Knowledge Graph.\n",
      "Overall Sources: 6\n",
      "Overall Domains: 3\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 2\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Single Fact\n",
      "\t Description: To fully answer the question, only a single fact has to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Single Fact\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'General', 'Scholarly', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2015, 2017, 2019, 2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multi Fact\n",
      "\t Description: To fully answer the question, multiple facts have to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multi Fact\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Type\n",
      "Description: Classifies the question by the expected answer format or data type. As such it classifies what the question excepts to be in the answer.\n",
      "Overall Sources: 24\n",
      "Overall Domains: 10\n",
      "Overall Years: 16\n",
      "Overall literature Categories: 4\n",
      "Number of Clusters: 28\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Undefined\n",
      "\t Description: Groups questions for which the expected answer type is unclear or cannot be determined by standard classification rules.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Undefined\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'The Classification of Research Questions', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'General']\n",
      "\t\t Years: [1984, 1999, 2000, 2021]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other\n",
      "\t Description: Classifies those questions that expect an answer type that does not fit in any of the other categories.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Other\n",
      "\t\t Sources: {'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2009, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Date\n",
      "\t Description: Classifies those questions where the expected answer is a date.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Date\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2021, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Distance Measurement\n",
      "\t Description: Classifies those questions where the expected answer is a linear measure such as distance or length.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Distance Measurement\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [1999, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Actor\n",
      "\t Description: Classifies those questions where the expected answer is an actor in a certain action which can be a individual, team, project, organization, or industry.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Actor\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Technology\n",
      "\t Description: Classifies those questions where the expected answer should be a technology which can be a process model, method, technique, tool, or language that is applied on the software system.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Technology\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Definition\n",
      "\t Description: Classifies those questions where the expected answer is a definition of a term, concept, or technology.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Definition\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning foci for Question Answering over Topic Maps'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2000, 2009, 2017]\n",
      "\t\t Literature Categories: ['Other', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Time\n",
      "\t Description: Classifies those questions where the expected answer is a time.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Time\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [2000, 2000, 2009, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Name\n",
      "\t Description: Classifies those questions where the expected answer is a name.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Name\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Title\n",
      "\t Description: Classifies those questions where the expected answer is a title such as the title of a publication.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Title\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Bibliometric Numbers\n",
      "\t Description: Classifies those questions where the expected answer is a bibliometric number such as publication counts, citation numbers, h-index, or i10-index.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Bibliometric Numbers\n",
      "\t\t Sources: {'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2024]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Software System\n",
      "\t Description: Classifies those questions where the expected answer describes the features of a software system such as its size, complexity, or application domain\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Software System\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Monetary\n",
      "\t Description: Classifies those questions where the expected answer is a monetary value.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Monetary\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General']\n",
      "\t\t Years: [1999, 2000, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Abbreviation\n",
      "\t Description: Classifies question where the expeceted answer is a long or short form of an abbreviation.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Abbreviation\n",
      "\t\t Sources: {'Learning Question Classifiers'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2002]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Instructional\n",
      "\t Description: Classifies those questions where the expected answer is a step-by-step procedure or instruction to achieve a specified task.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instructional\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Procedure/Technique\n",
      "\t Description: Given a question, the expected result is a concept for implementation, representation, management, or analysis.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Procedure/Technique\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Organization\n",
      "\t Description: Classifies those questions where the expected answer is an organization such as universities, research centers, laboratories, or other entities.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Organization\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'The Structure and Performance of an Open-Domain Question Answering System', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'Spoken Natural Language Processing', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2000, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Duration\n",
      "\t Description: Classifies those questions where the expected answer is a time duration such as 'three years' or 'few hours'.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Duration\n",
      "\t\t Sources: {'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1999]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Boolean\n",
      "\t Description: Classifies those questions where the expected answer is an affirmative or negative response.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Boolean\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Question Answering on Scholarly Knowledge Graphs', 'A Comparative Study of Question Answering over Knowledge Bases', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly', 'Covid', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2017, 2017, 2017, 2017, 2019, 2020, 2021, 2022, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Entity\n",
      "\t Description: Classifies those questions where the expected answer is expected to name one or more entities or objects.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Entity\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'Spoken Natural Language Processing']\n",
      "\t\t Years: [2002, 2015, 2017]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Description\n",
      "\t Description: Classifies those questions where the expected answer is a description.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Description\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research', 'Linguistically Motivated Question Classification', 'Types of research questions: descriptive, predictive, or causal', 'Learning Question Classifiers', 'Formulation of Research Question - Stepwise Approach'}\n",
      "\t\t Domains: ['General', 'Software Engineering', 'General', 'Spoken Natural Language Processing', 'Healthcare']\n",
      "\t\t Years: [2002, 2008, 2015, 2019, 2020]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Research Questions', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Properties\n",
      "\t Description: Classifies those questions where the expected answer is expected to list or detail the properties or characteristics inherent to a phenomenon, object, or entity.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Properties\n",
      "\t\t Sources: {'The Classification of Research Questions'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1984]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Human/Person\n",
      "\t Description: Classifies those questions where the expected answer is a human or person.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Human/Person\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Social Science', 'General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Location\n",
      "\t Description: Classifies those questions where the expected answer is a location which are geographic in nature.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Location\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Spoken Natural Language Processing', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Quantitative\n",
      "\t Description: Classifies those questions where the expected answer is a numerical information, counts, or quantitative measure\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Quantitative\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning Question Classifiers', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Requirements Engineering', 'General', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1999, 2000, 2002, 2009, 2017, 2017, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other', 'Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Tool/Notation\n",
      "\t Description: Classifies those questions where the expected answer is a specific software tool that embodies a technique or formal language that should have a calculus, semantics, or other basis for computing or inference.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Tool/Notation\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Solution\n",
      "\t Description: Classifies those questions where the expected answer is a specific solution, design prototype, or evaluative judgment based on established software engineering principles.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Solution\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Theoretical Framework\n",
      "\t Description: Classifies those questions where the expected answer is expected to be theoretical statements that support a research. These are categorized into no specific theory, kernel theory, formal theory, or testable theory.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Theoretical Framework\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "________________________\n",
      "Dimension Name: Question Type\n",
      "Description: Classifies questions by their actions that are to be performed on the information to answer the question.\n",
      "Overall Sources: 14\n",
      "Overall Domains: 6\n",
      "Overall Years: 9\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 12\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Negation\n",
      "\t Description: Classifies those questions where the retriever is required to negate the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Negation\n",
      "\t\t Sources: {'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly']\n",
      "\t\t Years: [2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Dependency\n",
      "\t Description: Classifies those questions where the retriever is required to identify a dependency of the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Clause\n",
      "\t\t Sources: {'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['Vietnamese Language']\n",
      "\t\t Years: [2017]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Contingency\n",
      "\t Description: Classifies those questions where the retriever is required to identify a contingency of the information in the Knowledge Graph to answer the question. This involves identifying cause effect relationships but also correlations.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Contingencies\n",
      "\t\t Sources: {'The Classification of Research Questions', 'Types of research questions: descriptive, predictive, or causal', 'Formulation of Research Question - Stepwise Approach', 'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering', 'Software Engineering', 'Software Engineering', 'General', 'General', 'General', 'Healthcare']\n",
      "\t\t Years: [1984, 2008, 2008, 2008, 2019, 2019, 2020]\n",
      "\t\t Literature Categories: ['Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Counting\n",
      "\t Description: Classifies those questions where the retriever is required to count the number of occurrences of a certain information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Counts\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Selecting Empirical Methods for Software Engineering Research', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', '10th Question Answering over Linked Data (QALD) Challenge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'Software Engineering', 'General', 'General', 'General']\n",
      "\t\t Years: [2008, 2019, 2021, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Superlative\n",
      "\t Description: Classifies those questions where the retriever is required to identify the most or least of a certain information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Superlatives\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2023, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Comparison\n",
      "\t Description: Classifies those questions where the retriever is required to compare two or more pieces of information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Comparison\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', '10th Question Answering over Linked Data (QALD) Challenge', 'The Classification of Research Questions', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1984, 2017, 2019, 2022, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Listing\n",
      "\t Description: Classifies those questions where the retriever is required to list the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Listing\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Vietnamese Language', 'General', 'General']\n",
      "\t\t Years: [2015, 2017, 2020, 2021, 2021, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multiple Intentions\n",
      "\t Description: Classifies those questions that contain multiple intentions which could be broken up into multiple different questions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multiple Intents\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2017, 2019, 2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Ranking\n",
      "\t Description: Classifies those questions where the retriever is required to rank the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Ranking\n",
      "\t\t Sources: {'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General']\n",
      "\t\t Years: [2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Temporal\n",
      "\t Description: Classifies those questions where the retriever is required to consider time series or trends over time to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Temporal\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Requirements Engineering', 'General', 'General']\n",
      "\t\t Years: [2019, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Aggregation\n",
      "\t Description: Classifies those questions where the retriever is required to quantitatively or qualitatively aggregate the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Aggregation\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Relationship\n",
      "\t Description: Classifies those questions where the retriever is required to identify a relationship between two or more pieces of information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Relationship\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'Large-scale Simple Question Answering with Memory Networks'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2015, 2017, 2019]\n",
      "\t\t Literature Categories: ['Research Questions', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Research Focus\n",
      "Description: Classifies questions by their goal towards the literature research process.\n",
      "Overall Sources: 1\n",
      "Overall Domains: 1\n",
      "Overall Years: 1\n",
      "Overall literature Categories: 1\n",
      "Number of Clusters: 6\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Development Methods\n",
      "\t Description: Classifies questions that request details on the methods, processes, or procedures used during the development of software solutions including aspects such as design patterns, best practices, and algorithmic strategies.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Development Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Analytical Methods\n",
      "\t Description: Classifies questions that request details on strategies, techniques, or frameworks used for analyzing or evaluating software tools, technologies, or approaches.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytical Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Generalization\n",
      "\t Description: Classifies questions that seek broad concepts, taxonomies, or general principles in software engineering.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Generalization \n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Qualitative Modeling\n",
      "\t Description: Classifies questions that expect qualitative models which cover a wide variety of different types of models, including structures or taxonomies, architecture designs, frameworks, design patterns, and other non quantitative models.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Qualitative Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Empirical Modeling\n",
      "\t Description: Classifies questions that expect empirical models which are predictive models that are based on data.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Empirical Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Analytic Modeling\n",
      "\t Description: Classifies questions that expect analytic models which are models that are based on mathematical equations.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytic Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "________________________\n",
      "Dimension Name: Answer Credibility\n",
      "Description: Classifies questions by the credibility of the answer that is expected.\n",
      "Overall Sources: 4\n",
      "Overall Domains: 4\n",
      "Overall Years: 4\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 4\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Factual\n",
      "\t Description: Classifies those questions where the expected answer is expected to evidence-based.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Factual\n",
      "\t\t Sources: {'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['Requirements Engineering', 'General', 'Social Science']\n",
      "\t\t Years: [2015, 2022, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Opinion\n",
      "\t Description: Classifies those questions where the expected answer is expected to be based on personal opinions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Opinion\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['General', 'Social Science']\n",
      "\t\t Years: [2015, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Debate\n",
      "\t Description: Classifies those questions where the expected answer is intended to provoke discussion or argumentation rather than factual evidence.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Debate\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Predictive\n",
      "\t Description: Classifies those questions where the expected answer is expected to be a prediction.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Predictive\n",
      "\t\t Sources: {'Types of research questions: descriptive, predictive, or causal'}\n",
      "\t\t Domains: ['Healthcare']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "________________________\n",
      "Dimension Name: Question Goal\n",
      "Description: Classifies questions by their goal towards the literature research process.\n",
      "Overall Sources: 6\n",
      "Overall Domains: 3\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 6\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Exploratory\n",
      "\t Description: Classifies questions that are exploratory in nature and aim to discover new information or insights.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Exploratory\n",
      "\t\t Sources: {'Writing good software engineering research papers', 'Formulation of Research Question - Stepwise Approach', 'Construction of Design Science Research Questions', 'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering', 'Software Engineering', 'Software Engineering', 'Design Science', 'General', 'Software Engineering']\n",
      "\t\t Years: [2003, 2008, 2008, 2008, 2019, 2019]\n",
      "\t\t Literature Categories: ['Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Reasoning\n",
      "\t Description: Classifies questions that seek reasons or causes for a certain phenomenon.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Reasoning\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [2000, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problem Solving\n",
      "\t Description: Classifies questions that seek to find solutions to a specific issue.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problem Solving\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Gap Spotting\n",
      "\t Description: Classifies questions that aim to identify gaps in existing literature or research, thereby highlighting areas needing further investigation.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Gap Spotting\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problematization\n",
      "\t Description: Classifies questions that aim to articulate deficiencies or problems in current theories or practices suggesting a need for additional research.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problematization\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Method Improvement\n",
      "\t Description: Classifies questions that aim to discover improved methods for software improvement and design.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Design\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2008]\n",
      "\t\t Literature Categories: ['Research Questions']\n"
     ]
    }
   ],
   "source": [
    "print_taxonomy_summary(iteration=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration description: No info\n",
      "Taxonomy:\n",
      "________________________\n",
      "Dimension Name: Graph Representation\n",
      "Description: Classifies questions based on how the information that is required to answer the question is organized in the Knowledge Graph.\n",
      "Overall Sources: 6\n",
      "Overall Domains: 3\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 2\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Single Fact\n",
      "\t Description: To fully answer the question, only a single fact has to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Single Fact\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'General', 'Scholarly', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2015, 2017, 2019, 2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multi Fact\n",
      "\t Description: To fully answer the question, multiple facts have to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multi Fact\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Type\n",
      "Description: Classifies the question by the types that the terms inside of the answer are expected to have.\n",
      "Overall Sources: 23\n",
      "Overall Domains: 10\n",
      "Overall Years: 16\n",
      "Overall literature Categories: 4\n",
      "Number of Clusters: 6\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Named Entity\n",
      "\t Description: Classifies questions where the expected answer is a specific named entity. This includes people, organizations, locations, systems, technologies, and other concrete subjects or objects that are usually capitalized.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Name\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Title\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Entity\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'Spoken Natural Language Processing']\n",
      "\t\t Years: [2002, 2015, 2017]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Abbreviation\n",
      "\t\t Sources: {'Learning Question Classifiers'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2002]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Human/Person\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Social Science', 'General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Location\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Spoken Natural Language Processing', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Actor\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Technology\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Organization\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'The Structure and Performance of an Open-Domain Question Answering System', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'Spoken Natural Language Processing', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2000, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Software System\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Solution\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Development Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytical Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instance-Specific\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Generalization \n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Qualitative Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Empirical Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytic Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Procedure/Technique\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Description\n",
      "\t Description: Classifies questions where the expected answer provides an explanation, definition, theoretical construct, or conceptual overview. These answers typically describe what something is or how it works.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Description\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research', 'Linguistically Motivated Question Classification', 'Types of research questions: descriptive, predictive, or causal', 'Learning Question Classifiers', 'Formulation of Research Question - Stepwise Approach'}\n",
      "\t\t Domains: ['General', 'Software Engineering', 'General', 'Spoken Natural Language Processing', 'Healthcare']\n",
      "\t\t Years: [2002, 2008, 2015, 2019, 2020]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Research Questions', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Definition\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning foci for Question Answering over Topic Maps'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2000, 2009, 2017]\n",
      "\t\t Literature Categories: ['Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Tool/Notation\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Theoretical Framework\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Temporal\n",
      "\t Description: Classifies questions where the expected answer refers to a specific point or range in time. This includes dates, timestamps, or general time expressions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Date\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2021, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Duration\n",
      "\t\t Sources: {'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1999]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Time\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [2000, 2000, 2009, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Quantitative\n",
      "\t Description: Classifies questions where the expected answer is a numerical value, metric, or measurable quantity. This includes units of measurement, monetary values, bibliometric indicators, durations, and general counts.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Distance Measurement\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [1999, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Monetary\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General']\n",
      "\t\t Years: [1999, 2000, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Quantitative\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning Question Classifiers', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Requirements Engineering', 'General', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1999, 2000, 2002, 2009, 2017, 2017, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other', 'Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Bibliometric Numbers\n",
      "\t\t Sources: {'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2024]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Boolean\n",
      "\t Description: Classifies questions where the expected answer is a binary affirmation or negation.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Boolean\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Question Answering on Scholarly Knowledge Graphs', 'A Comparative Study of Question Answering over Knowledge Bases', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly', 'Covid', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2017, 2017, 2017, 2017, 2019, 2020, 2021, 2022, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Type\n",
      "\t Description: Classifies questions where the expected answer type does not fit within any of the other defined categories.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Undefined\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'The Classification of Research Questions', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'General']\n",
      "\t\t Years: [1984, 1999, 2000, 2021]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Other\n",
      "\t\t Sources: {'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2009, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Format\n",
      "Description: Classifies the question by the expected format of the answer.\n",
      "Overall Sources: 7\n",
      "Overall Domains: 3\n",
      "Overall Years: 7\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 4\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Simple\n",
      "\t Description: Classifies those questions where the expected answer is brief, straightforward, and minimalistic, often consisting of a one-sentence response, a yes/no answer, or a direct factual statement without additional elaboration.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Explanatory\n",
      "\t Description: Classifies those questions where the expected answer is expected to provide a textual explanation about a certain phenomenon, object, or entity.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Enumerative\n",
      "\t Description: Classifies those questions where the expected answer is expected to list or detail the properties or characteristics inherent to a phenomenon, object, or entity.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Properties\n",
      "\t\t Sources: {'The Classification of Research Questions'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1984]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Listing\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Vietnamese Language', 'General', 'General']\n",
      "\t\t Years: [2015, 2017, 2020, 2021, 2021, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instructional\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Format\n",
      "\t Description: Classifies those questions where the expected answer format does not fit into any of the other categories.\n",
      "\t Based On Clusters: None, added manually\n",
      "________________________\n",
      "Dimension Name: Question Type\n",
      "Description: Classifies questions by their actions that are to be performed on the information to answer the question.\n",
      "Overall Sources: 14\n",
      "Overall Domains: 6\n",
      "Overall Years: 9\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 9\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Negation\n",
      "\t Description: Classifies those questions where the retriever is required to negate the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Negation\n",
      "\t\t Sources: {'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly']\n",
      "\t\t Years: [2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Relationship\n",
      "\t Description: Classifies questions where the retriever must identify any type of interconnection or reliance between pieces of information in the Knowledge Graph. Essentially, it captures all scenarios where one piece of data is influenced by, contingent upon, or systematically linked to another.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Clause\n",
      "\t\t Sources: {'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['Vietnamese Language']\n",
      "\t\t Years: [2017]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Contingencies\n",
      "\t\t Sources: {'The Classification of Research Questions', 'Types of research questions: descriptive, predictive, or causal', 'Formulation of Research Question - Stepwise Approach', 'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering', 'Software Engineering', 'Software Engineering', 'General', 'General', 'General', 'Healthcare']\n",
      "\t\t Years: [1984, 2008, 2008, 2008, 2019, 2019, 2020]\n",
      "\t\t Literature Categories: ['Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Relationship\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'Large-scale Simple Question Answering with Memory Networks'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2015, 2017, 2019]\n",
      "\t\t Literature Categories: ['Research Questions', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Superlative\n",
      "\t Description: Classifies those questions where the retriever is required to identify the most or least of a certain information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Superlatives\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2023, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Counting\n",
      "\t Description: Classifies those questions where the retriever is required to count the number of occurrences of a certain information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Counts\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Selecting Empirical Methods for Software Engineering Research', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', '10th Question Answering over Linked Data (QALD) Challenge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'Software Engineering', 'General', 'General', 'General']\n",
      "\t\t Years: [2008, 2019, 2021, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Ranking\n",
      "\t Description: Classifies those questions where the retriever is required to rank the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Ranking\n",
      "\t\t Sources: {'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General']\n",
      "\t\t Years: [2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Comparison\n",
      "\t Description: Classifies those questions where the retriever is required to compare two or more pieces of information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Comparison\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', '10th Question Answering over Linked Data (QALD) Challenge', 'The Classification of Research Questions', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1984, 2017, 2019, 2022, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multiple Intentions\n",
      "\t Description: Classifies those questions that contain multiple intentions which could be broken up into multiple different questions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multiple Intents\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2017, 2019, 2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Temporal\n",
      "\t Description: Classifies those questions where the retriever is required to consider time series or trends over time to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Temporal\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Requirements Engineering', 'General', 'General']\n",
      "\t\t Years: [2019, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Aggregation\n",
      "\t Description: Classifies those questions where the retriever is required to quantitatively or qualitatively aggregate the information in the Knowledge Graph to answer the question.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Aggregation\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Credibility\n",
      "Description: Classifies questions by the credibility of the answer that is expected.\n",
      "Overall Sources: 3\n",
      "Overall Domains: 3\n",
      "Overall Years: 3\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 2\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Objective\n",
      "\t Description: Classifies those questions where the expected answer is expected to evidence-based.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Factual\n",
      "\t\t Sources: {'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['Requirements Engineering', 'General', 'Social Science']\n",
      "\t\t Years: [2015, 2022, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Subjective\n",
      "\t Description: Answers that are based on personal views or constructed to persuade or provoke discussion.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Opinion\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['General', 'Social Science']\n",
      "\t\t Years: [2015, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Debate\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "________________________\n",
      "Dimension Name: Question Goal\n",
      "Description: Classifies questions by their goal towards the literature research process.\n",
      "Overall Sources: 5\n",
      "Overall Domains: 4\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 5\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Reasoning\n",
      "\t Description: Classifies questions that seek reasons or causes for a certain phenomenon.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Reasoning\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [2000, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problem Solving\n",
      "\t Description: Classifies questions that seek to find solutions to a specific issue.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problem Solving\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problematization\n",
      "\t Description: Classifies questions that aim to articulate deficiencies or problems in current theories or practices suggesting a need for additional research.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problematization\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Gap Spotting\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Improvement\n",
      "\t Description: Classifies questions that aim to discover improved methods for software improvement and design.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Design\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2008]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Prediction\n",
      "\t Description: Classifies those questions where the expected answer is expected to be a prediction.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Predictive\n",
      "\t\t Sources: {'Types of research questions: descriptive, predictive, or causal'}\n",
      "\t\t Domains: ['Healthcare']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Research Questions']\n"
     ]
    }
   ],
   "source": [
    "print_taxonomy_summary(iteration=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration description: No info\n",
      "Taxonomy:\n",
      "________________________\n",
      "Dimension Name: Graph Representation\n",
      "Description: Classifies questions based on how the information that is required to answer the question is organized in the Knowledge Graph.\n",
      "Overall Sources: 6\n",
      "Overall Domains: 3\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 2\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Single Fact\n",
      "\t Description: To fully answer the question, only a single fact has to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Single Fact\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'General', 'Scholarly', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2015, 2017, 2019, 2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multi Fact\n",
      "\t Description: To fully answer the question, multiple facts have to be retrieved from the Knowledge Graph.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multi Fact\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2019, 2020, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Type\n",
      "Description: Classifies the question by the types that the terms inside of the answer are expected to have.\n",
      "Overall Sources: 22\n",
      "Overall Domains: 9\n",
      "Overall Years: 16\n",
      "Overall literature Categories: 4\n",
      "Number of Clusters: 6\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Named Entity\n",
      "\t Description: Classifies questions where the expected answer is a specific named entity. This includes people, organizations, locations, systems, technologies, and other concrete subjects or objects that are usually capitalized.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Name\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Entity\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'Spoken Natural Language Processing']\n",
      "\t\t Years: [2002, 2015, 2017]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Abbreviation\n",
      "\t\t Sources: {'Learning Question Classifiers'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2002]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Human/Person\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Social Science', 'General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Location\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Spoken Natural Language Processing', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Actor\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Technology\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Organization\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'The Structure and Performance of an Open-Domain Question Answering System', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'Spoken Natural Language Processing', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2000, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Software System\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Solution\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Development Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytical Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instance-Specific\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Generalization \n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Qualitative Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Empirical Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytic Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Procedure/Technique\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Description\n",
      "\t Description: Classifies questions where the expected answer provides an explanation, definition, theoretical construct, or conceptual overview. These answers typically describe what something is or how it works.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Description\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research', 'Linguistically Motivated Question Classification', 'Types of research questions: descriptive, predictive, or causal', 'Learning Question Classifiers', 'Formulation of Research Question - Stepwise Approach'}\n",
      "\t\t Domains: ['General', 'Software Engineering', 'General', 'Spoken Natural Language Processing', 'Healthcare']\n",
      "\t\t Years: [2002, 2008, 2015, 2019, 2020]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Research Questions', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Definition\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning foci for Question Answering over Topic Maps'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2000, 2009, 2017]\n",
      "\t\t Literature Categories: ['Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Tool/Notation\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Temporal\n",
      "\t Description: Classifies questions where the expected answer refers to a specific point or range in time. This includes dates, timestamps, or general time expressions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Date\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2021, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Duration\n",
      "\t\t Sources: {'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1999]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Time\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [2000, 2000, 2009, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Quantitative\n",
      "\t Description: Classifies questions where the expected answer is a numerical value, metric, or measurable quantity. This includes units of measurement, monetary values, bibliometric indicators, durations, and general counts.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Distance Measurement\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [1999, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Monetary\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General']\n",
      "\t\t Years: [1999, 2000, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Quantitative\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning Question Classifiers', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Requirements Engineering', 'General', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1999, 2000, 2002, 2009, 2017, 2017, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other', 'Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Bibliometric Numbers\n",
      "\t\t Sources: {'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2024]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Boolean\n",
      "\t Description: Classifies questions where the expected answer is a binary true/false value. These questions are typically framed to elicit confirmation, denial, or the presence or absence of a condition.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Boolean\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'Question Answering on Scholarly Knowledge Graphs', 'A Comparative Study of Question Answering over Knowledge Bases', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly', 'Covid', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2017, 2017, 2017, 2017, 2019, 2020, 2021, 2022, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Type\n",
      "\t Description: Classifies questions where the expected answer type does not fit within any of the other defined categories.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Undefined\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'The Classification of Research Questions', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'General']\n",
      "\t\t Years: [1984, 1999, 2000, 2021]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Other\n",
      "\t\t Sources: {'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2009, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Condition Type\n",
      "Description: Classifies the question by the constraints that are provided in the question that need to be fulfilled to answer the question.\n",
      "Overall Sources: 21\n",
      "Overall Domains: 9\n",
      "Overall Years: 15\n",
      "Overall literature Categories: 4\n",
      "Number of Clusters: 5\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Named Entity\n",
      "\t Description: Classifies questions where the expected answer is constrained by a specific name of an entity. This includes people, organizations, locations, systems, technologies, and other concrete subjects or objects that are usually capitalized.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Name\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Title\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2000]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Entity\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'Spoken Natural Language Processing']\n",
      "\t\t Years: [2002, 2015, 2017]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Abbreviation\n",
      "\t\t Sources: {'Learning Question Classifiers'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2002]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Human/Person\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Social Science', 'General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Location\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Linguistically Motivated Question Classification', 'Learning Question Classifiers', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Spoken Natural Language Processing', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2002, 2009, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Actor\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Technology\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Organization\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'The Structure and Performance of an Open-Domain Question Answering System', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'Spoken Natural Language Processing', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2000, 2015, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Software System\n",
      "\t\t Sources: {'The Future of Empirical Methods in Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2007]\n",
      "\t\t Literature Categories: ['Other']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Solution\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Development Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytical Methods\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instance-Specific\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Generalization \n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Qualitative Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Empirical Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Analytic Modeling\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Procedure/Technique\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Description\n",
      "\t Description: Classifies questions where the expected answer is constraint by an explanation, definition, theoretical construct, or conceptual overview.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Description\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research', 'Linguistically Motivated Question Classification', 'Types of research questions: descriptive, predictive, or causal', 'Learning Question Classifiers', 'Formulation of Research Question - Stepwise Approach'}\n",
      "\t\t Domains: ['General', 'Software Engineering', 'General', 'Spoken Natural Language Processing', 'Healthcare']\n",
      "\t\t Years: [2002, 2008, 2015, 2019, 2020]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Research Questions', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Definition\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning foci for Question Answering over Topic Maps'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2000, 2009, 2017]\n",
      "\t\t Literature Categories: ['Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Tool/Notation\n",
      "\t\t Sources: {'Writing good software engineering research papers'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2003]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Theoretical Framework\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Temporal\n",
      "\t Description: Classifies questions where the expected answer is constrained by a specific point or range in time. This includes dates, timestamps, or general time expressions.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Date\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Scholarly', 'General']\n",
      "\t\t Years: [1999, 2000, 2000, 2021, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Duration\n",
      "\t\t Sources: {'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1999]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Time\n",
      "\t\t Sources: {'Linguistically Motivated Question Classification', 'A Rule-based Question Answering System for Reading Comprehension Tests', 'Learning foci for Question Answering over Topic Maps', 'The Structure and Performance of an Open-Domain Question Answering System'}\n",
      "\t\t Domains: ['General', 'General', 'Spoken Natural Language Processing', 'General']\n",
      "\t\t Years: [2000, 2000, 2009, 2015]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Temporal\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Requirements Engineering', 'General', 'General']\n",
      "\t\t Years: [2019, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Quantitative\n",
      "\t Description: Classifies questions where the expected answer is constrained by a numerical value, metric, or measurable quantity. This includes units of measurement, monetary values, bibliometric indicators, durations, and general counts.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Distance Measurement\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [1999, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Monetary\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General']\n",
      "\t\t Years: [1999, 2000, 2000]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Quantitative\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'Ripple Down Rules for question answering', 'Learning Question Classifiers', 'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'Learning foci for Question Answering over Topic Maps', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Requirements Engineering', 'General', 'Vietnamese Language', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1999, 2000, 2002, 2009, 2017, 2017, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Other', 'Question Classifier', 'Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Bibliometric Numbers\n",
      "\t\t Sources: {'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2024]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Type\n",
      "\t Description: Classifies questions where the expected constraint on the answer does not fit within any of the other defined categories.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Undefined\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'The Classification of Research Questions', 'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'General']\n",
      "\t\t Years: [1984, 1999, 2000, 2021]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Other\n",
      "\t\t Sources: {'Learning foci for Question Answering over Topic Maps', 'Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset', 'AT&T at TREC-8'}\n",
      "\t\t Domains: ['General', 'General', 'Scholarly']\n",
      "\t\t Years: [1999, 2009, 2024]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Other', 'Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Answer Format\n",
      "Description: Classifies the question by the expected format of the answer.\n",
      "Overall Sources: 7\n",
      "Overall Domains: 3\n",
      "Overall Years: 7\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 4\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Simple\n",
      "\t Description: Classifies those questions where the expected answer is brief, straightforward, and minimalistic, often consisting of a one-sentence response, a yes/no answer, or a direct factual statement without additional elaboration.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Explanatory\n",
      "\t Description: Classifies those questions where the expected answer is expected to provide a textual explanation about a certain phenomenon, object, or entity.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Enumerative\n",
      "\t Description: Classifies those questions where the expected answer is expected to list or detail the properties or characteristics inherent to a phenomenon, object, or entity.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Properties\n",
      "\t\t Sources: {'The Classification of Research Questions'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [1984]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Listing\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Ripple Down Rules for question answering', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'Large-scale Simple Question Answering with Memory Networks', 'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Vietnamese Language', 'General', 'General']\n",
      "\t\t Years: [2015, 2017, 2020, 2021, 2021, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Instructional\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Format\n",
      "\t Description: Classifies those questions where the expected answer format does not fit into any of the other categories.\n",
      "\t Based On Clusters: None, added manually\n",
      "________________________\n",
      "Dimension Name: Retrieval Operation\n",
      "Description: Classifies questions by their actions that are to be performed on the retrieved information to answer the question.\n",
      "Overall Sources: 13\n",
      "Overall Domains: 5\n",
      "Overall Years: 9\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 8\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Basic\n",
      "\t Description: The answer can be retrieved directly without applying additional operations.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Negation\n",
      "\t Description: Requires identifying where a condition does not hold, based on explicit negation or missing information.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Negation\n",
      "\t\t Sources: {'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly']\n",
      "\t\t Years: [2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Relationship\n",
      "\t Description: Requires identifying a connection or dependency between pieces of information, such as causalities or correlations.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Clause\n",
      "\t\t Sources: {'Ripple Down Rules for question answering'}\n",
      "\t\t Domains: ['Vietnamese Language']\n",
      "\t\t Years: [2017]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Contingencies\n",
      "\t\t Sources: {'The Classification of Research Questions', 'Types of research questions: descriptive, predictive, or causal', 'Formulation of Research Question - Stepwise Approach', 'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering', 'Software Engineering', 'Software Engineering', 'General', 'General', 'General', 'Healthcare']\n",
      "\t\t Years: [1984, 2008, 2008, 2008, 2019, 2019, 2020]\n",
      "\t\t Literature Categories: ['Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions', 'Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Relationship\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'Large-scale Simple Question Answering with Memory Networks'}\n",
      "\t\t Domains: ['General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [2015, 2017, 2019]\n",
      "\t\t Literature Categories: ['Research Questions', 'Question Classifier', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Superlative\n",
      "\t Description: Requires identifying the most or least of a certain information.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Superlatives\n",
      "\t\t Sources: {'10th Question Answering over Linked Data (QALD) Challenge', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'General', 'Scholarly']\n",
      "\t\t Years: [2023, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Counting\n",
      "\t Description: Requires counting the number of relevant instances in the data.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Counts\n",
      "\t\t Sources: {'What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs', 'Selecting Empirical Methods for Software Engineering Research', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', '10th Question Answering over Linked Data (QALD) Challenge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'Scholarly', 'Software Engineering', 'General', 'General', 'General']\n",
      "\t\t Years: [2008, 2019, 2021, 2023, 2023, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Ranking\n",
      "\t Description: Requires ordering information based on a specific criterion.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Ranking\n",
      "\t\t Sources: {'The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General']\n",
      "\t\t Years: [2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Comparison\n",
      "\t Description: Requires comparing two or more pieces of information based on common attributes.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Comparison\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', '10th Question Answering over Linked Data (QALD) Challenge', 'The Classification of Research Questions', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General', 'General', 'Vietnamese Language', 'General']\n",
      "\t\t Years: [1984, 2017, 2019, 2022, 2023]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier', 'Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Aggregation\n",
      "\t Description: Requires aggregating multiple pieces of information into a single answer.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Aggregation\n",
      "\t\t Sources: {'Question Answering on Scholarly Knowledge Graphs'}\n",
      "\t\t Domains: ['Scholarly']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset']\n",
      "________________________\n",
      "Dimension Name: Intention Count\n",
      "Description: This dimension classifies questions by how many different intentions are contained in the question.\n",
      "Overall Sources: 4\n",
      "Overall Domains: 3\n",
      "Overall Years: 3\n",
      "Overall literature Categories: 3\n",
      "Number of Clusters: 2\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Single Intention\n",
      "\t Description: Classifies those questions that focus on one objective or query. They cannot be meaningfully divided into separate, independent questions without losing their original context.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Multiple Intentions\n",
      "\t Description: Classifies those questions that embed several distinct objectives or queries. Such questions can be broken down into separate components or questions, each addressing a different aspect.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Multiple Intents\n",
      "\t\t Sources: {'Ripple Down Rules for question answering', 'Formulation of Research Question - Stepwise Approach', 'DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph', 'LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia'}\n",
      "\t\t Domains: ['Scholarly', 'General', 'General', 'Vietnamese Language']\n",
      "\t\t Years: [2017, 2019, 2019, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Knowledge Graph Question Answering Dataset', 'Research Questions', 'Question Classifier']\n",
      "________________________\n",
      "Dimension Name: Answer Credibility\n",
      "Description: Classifies questions by the credibility of the answer that is expected.\n",
      "Overall Sources: 3\n",
      "Overall Domains: 3\n",
      "Overall Years: 3\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 3\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Normative\n",
      "\t Description: This category applies to questions where the answer is expected to provide value-based judgments, recommendations, or ethical prescriptions. In these cases, the answer is about how things ought to be based on norms, ethics, or policies.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Objective\n",
      "\t Description: This category covers questions where answers are expected to be grounded in verified data, facts, or empirical evidence. The focus is on reporting measurable, observable, or documented information without interpretation or personal bias.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Factual\n",
      "\t\t Sources: {'Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering', 'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['Requirements Engineering', 'General', 'Social Science']\n",
      "\t\t Years: [2015, 2022, 2023]\n",
      "\t\t Literature Categories: ['Knowledge Graph Question Answering Dataset', 'Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Subjective\n",
      "\t Description: This category includes questions where the answer is influenced by personal experience, preferences, or interpretation. Answers here are not strictly bound by objective evidence, as they are often intended to share opinions, interpretations, or subjective assessments.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Opinion\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy', 'A Taxonomy for Classifying Questions Asked in Social Question and Answering'}\n",
      "\t\t Domains: ['General', 'Social Science']\n",
      "\t\t Years: [2015, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Debate\n",
      "\t\t Sources: {'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General']\n",
      "\t\t Years: [2022]\n",
      "\t\t Literature Categories: ['Question Classifier']\n",
      "________________________\n",
      "Dimension Name: Question Goal\n",
      "Description: Classifies questions by their goal towards the literature research process.\n",
      "Overall Sources: 5\n",
      "Overall Domains: 4\n",
      "Overall Years: 5\n",
      "Overall literature Categories: 2\n",
      "Number of Clusters: 7\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Information Lookup\n",
      "\t Description: Questions that seek to know how something is.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Reasoning\n",
      "\t Description: Classifies questions aimed at uncovering underlying causes, mechanisms, or explanations for observed phenomena. These questions seek to understand why something occurs, not necessarily how to fix it.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Reasoning\n",
      "\t\t Sources: {'The Structure and Performance of an Open-Domain Question Answering System', 'A Non-Factoid Question-Answering Taxonomy'}\n",
      "\t\t Domains: ['General', 'General']\n",
      "\t\t Years: [2000, 2022]\n",
      "\t\t Literature Categories: ['Question Classifier', 'Question Classifier']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problem Solving\n",
      "\t Description: Classifies questions that aim to identify practical solutions, strategies, or methods to overcome a challenge or limitation.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problem Solving\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Problematization\n",
      "\t Description: Classifies questions that aim to articulate deficiencies or problems in current theories or practices suggesting a need for additional research.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Problematization\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Gap Spotting\n",
      "\t\t Sources: {'Construction of Design Science Research Questions'}\n",
      "\t\t Domains: ['Design Science']\n",
      "\t\t Years: [2019]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Improvement\n",
      "\t Description: Classifies questions focused on enhancing existing tools, methods, or practices. These questions often seek refinement, efficiency, or effectiveness improvements.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Design\n",
      "\t\t Sources: {'Selecting Empirical Methods for Software Engineering Research'}\n",
      "\t\t Domains: ['Software Engineering']\n",
      "\t\t Years: [2008]\n",
      "\t\t Literature Categories: ['Research Questions']\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Other Goal\n",
      "\t Description: Classifies questions that have a goal, that does not fit into any of the other categories.\n",
      "\t Based On Clusters: None, added manually\n",
      "\t ++++++++++++++++\n",
      "\t Category Name: Prediction\n",
      "\t Description: Classifies questions that aim to anticipate future developments, outcomes, or trends based on current knowledge or evidence.\n",
      "\t Based On Clusters:\n",
      "\t\t ****************\n",
      "\t\t Cluster Name: Predictive\n",
      "\t\t Sources: {'Types of research questions: descriptive, predictive, or causal'}\n",
      "\t\t Domains: ['Healthcare']\n",
      "\t\t Years: [2020]\n",
      "\t\t Literature Categories: ['Research Questions']\n"
     ]
    }
   ],
   "source": [
    "print_taxonomy_summary(iteration=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
