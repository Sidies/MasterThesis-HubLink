
@article{auer_sciqa_2023,
	title = {The {SciQA} Scientific Question Answering Benchmark for Scholarly Knowledge},
	volume = {13},
	rights = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-33607-z},
	doi = {10.1038/s41598-023-33607-z},
	abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering ({QA}) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as {DBpedia} and Wikidata. We present {SciQA} a scientific {QA} benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph ({ORKG}) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the {ORKG}. The questions cover a range of research fields and question types and are translated into corresponding {SPARQL} queries over the {ORKG}. Based on two preliminary evaluations, we show that the resulting {SciQA} benchmark represents a challenging task for next-generation {QA} systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data ({QALD}) Challenge.},
	pages = {7240},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
	urldate = {2024-08-26},
	date = {2023-05-04},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EELWCWD3\\Auer et al. - 2023 - The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge.pdf:application/pdf},
}

@inproceedings{karras_divide_2023,
	title = {Divide and Conquer the {EmpiRE}: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering},
	url = {https://ieeexplore.ieee.org/document/10304795},
	doi = {10.1109/ESEM56168.2023.10304795},
	shorttitle = {Divide and Conquer the {EmpiRE}},
	abstract = {[Background.] Empirical research in requirements engineering ({RE}) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its “current” state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph ({ORKG}) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in {RE} ({KG}-{EmpiRE}) whose data is openly available. Our long-term goal is to continuously maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. [Method.] We conduct a literature review using the {ORKG} to build and publish {KG}-{EmpiRE} which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020–2025. [Results.] From 570 papers of the {IEEE} International Requirements Engineering Conference (2000–2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The {ORKG} is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
	eventtitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	pages = {1--12},
	booktitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
	urldate = {2024-09-28},
	date = {2023-10},
	keywords = {domain\_specific, finished, notion},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2SUXDABT\\Karras et al. - 2023 - Divide and Conquer the EmpiRE A Community-Maintainable Knowledge Graph of Empirical Research in Req.pdf:application/pdf},
}

@inproceedings{shaw_writing_2003,
	title = {Writing good software engineering research papers},
	url = {https://ieeexplore.ieee.org/document/1201262},
	doi = {10.1109/ICSE.2003.1201262},
	abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to {XSE} 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
	eventtitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	pages = {726--736},
	booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	author = {Shaw, M.},
	urldate = {2024-10-02},
	date = {2003-05},
	note = {{ISSN}: 0270-5257},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\N3KSK3ZY\\Shaw - 2003 - Writing good software engineering research papers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\GVGKBD9L\\1201262.html:text/html},
}

@incollection{easterbrook_selecting_2008,
	location = {London},
	title = {Selecting Empirical Methods for Software Engineering Research},
	isbn = {978-1-84800-044-5},
	url = {https://doi.org/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	pages = {285--311},
	booktitle = {Guide to Advanced Empirical Software Engineering},
	publisher = {Springer},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	urldate = {2024-10-02},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-1-84800-044-5_11},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KC9B8WYM\\Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineering Research.pdf:application/pdf},
}

@inproceedings{li_learning_2002,
	title = {Learning Question Classifiers},
	url = {https://aclanthology.org/C02-1150},
	doi = {10.3115/1072228.107237},
	eventtitle = {{COLING} 2002},
	booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
	author = {Li, Xin and Roth, Dan},
	urldate = {2024-10-26},
	date = {2002},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RU9HI9XC\\Li and Roth - 2002 - Learning Question Classifiers.pdf:application/pdf},
}

@inproceedings{singhal_att_1999,
	title = {{AT}\&T at {TREC}-8},
	abstract = {In 1999, {AT}\&T participated in the ad-hoc task and the Question Answering {QA}, Spoken Document Retrieval {SDR}, and Web tracks. Most of our eeort for {TREC}-8 focused on the {QA} and {SDR} tracks. Results from {SDR} track show that our document expansion techniques, presented in 99, are very eeective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can the correct answer for about 45 of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.},
	author = {Singhal, Amit and Abney, Steve and Bacchiani, Michiel and Collins, Michael and Hindle, Donald and Pereira, Fernando},
	date = {1999-11-01},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BH2UE3PD\\Singhal et al. - 1999 - AT&T at TREC-8.pdf:application/pdf},
}
