
@misc{bordes_large-scale_2015,
	title = {Large-scale Simple Question Answering with Memory Networks},
	url = {http://arxiv.org/abs/1506.02075},
	abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
	number = {{arXiv}:1506.02075},
	publisher = {{arXiv}},
	author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
	urldate = {2024-09-23},
	date = {2015-06-05},
	eprinttype = {arxiv},
	eprint = {1506.02075 [cs]},
	keywords = {finished, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\KXD2CVA2\\Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NCZKK6ME\\1506.html:text/html},
}

@inproceedings{feng_multi-hop_2022,
	location = {Seattle, United States},
	title = {Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge},
	url = {https://aclanthology.org/2022.findings-naacl.12/},
	doi = {10.18653/v1/2022.findings-naacl.12},
	abstract = {Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as {DEHG}, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. {DEHG} employs a graph constructor to integrate structured and unstructured information, a context encoder to represent nodes and question, a heterogeneous information reasoning layer to conduct multi-hop reasoning on both information sources, and an answer decoder to generate answers for the question. Experimental results on {HybirdQA} dataset show that {DEHG} outperforms the state-of-the-art methods.},
	eventtitle = {Findings 2022},
	pages = {151--156},
	booktitle = {Findings of the Association for Computational Linguistics: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Yue and Han, Zhen and Sun, Mingming and Li, Ping},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	urldate = {2025-01-26},
	date = {2022-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3FQGH4C2\\Feng et al. - 2022 - Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge.pdf:application/pdf},
}

@inproceedings{garcia_aqualog_2006,
	location = {New York City, {USA}},
	title = {{AquaLog}: An ontology-driven Question Answering System to interface the Semantic Web},
	url = {https://aclanthology.org/N06-4005/},
	shorttitle = {{AquaLog}},
	pages = {269--272},
	booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Garcia, Vanessa Lopez and Motta, Enrico and Uren, Victoria},
	editor = {Rudnicky, Alex and Dowding, John and Milic-Frayling, Natasa},
	urldate = {2025-01-26},
	date = {2006-06},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JNKRABN4\\Garcia et al. - 2006 - AquaLog An ontology-driven Question Answering System to interface the Semantic Web.pdf:application/pdf},
}

@article{gu_beyond_2021,
	title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
	url = {https://dl.acm.org/doi/10.1145/3442381.3449992},
	doi = {10.1145/3442381.3449992},
	shorttitle = {Beyond I.I.D.},
	abstract = {Existing studies on question answering on knowledge bases ({KBQA}) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale {KBs} because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that {KBQA} models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of {KBQA} models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, {GrailQA}, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel {BERT}-based {KBQA} model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like {BERT} in the generalization of {KBQA}.1},
	pages = {3477--3488},
	journaltitle = {Proceedings of the Web Conference 2021},
	author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
	urldate = {2025-01-26},
	date = {2021-04-19},
	langid = {english},
	note = {Conference Name: {WWW} '21: The Web Conference 2021
{ISBN}: 9781450383127
Place: Ljubljana Slovenia
Publisher: {ACM}},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\XK9QHNZ8\\Gu et al. - 2021 - Beyond I.I.D. Three Levels of Generalization for Question Answering on Knowledge Bases.pdf:application/pdf},
}

@inproceedings{yih_value_2016,
	location = {Berlin, Germany},
	title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
	url = {http://aclweb.org/anthology/P16-2033},
	doi = {10.18653/v1/P16-2033},
	abstract = {We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering.},
	eventtitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages = {201--206},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	urldate = {2025-01-26},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HP2EE935\\Yih et al. - 2016 - The Value of Semantic Parse Labeling for Knowledge Base Question Answering.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural Questions: A Benchmark for Question Answering Research},
	volume = {7},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43518},
	doi = {10.1162/tacl_a_00276},
	shorttitle = {Natural Questions},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	pages = {453--466},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	urldate = {2025-01-26},
	date = {2019-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\WK6R49PT\\Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answering Research.pdf:application/pdf},
}

@article{lopez_evaluating_2013,
	title = {Evaluating question answering over linked data},
	volume = {21},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S157082681300022X},
	doi = {10.1016/j.websem.2013.05.006},
	series = {Special Issue on Evaluation of Semantic Technologies},
	abstract = {The availability of large amounts of open, distributed, and structured semantic data on the web has no precedent in the history of computer science. In recent years, there have been important advances in semantic search and question answering over {RDF} data. In particular, natural language interfaces to online semantic data have the advantage that they can exploit the expressive power of Semantic Web data models and query languages, while at the same time hiding their complexity from the user. However, despite the increasing interest in this area, there are no evaluations so far that systematically evaluate this kind of systems, in contrast to traditional question answering and search interfaces to document spaces. To address this gap, we have set up a series of evaluation challenges for question answering over linked data. The main goal of the challenge was to get insight into the strengths, capabilities, and current shortcomings of question answering systems as interfaces to query linked data sources, as well as benchmarking how these interaction paradigms can deal with the fact that the amount of {RDF} data available on the web is very large and heterogeneous with respect to the vocabularies and schemas used. Here, we report on the results from the first and second of such evaluation campaigns. We also discuss how the second evaluation addressed some of the issues and limitations which arose from the first one, as well as the open issues to be addressed in future competitions.},
	pages = {3--13},
	journaltitle = {Journal of Web Semantics},
	shortjournal = {Journal of Web Semantics},
	author = {Lopez, Vanessa and Unger, Christina and Cimiano, Philipp and Motta, Enrico},
	urldate = {2025-01-26},
	date = {2013-08-01},
	keywords = {Evaluation, Linked data, Natural language, Question answering, Semantic Web},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\HJIV7U9D\\Lopez et al. - 2013 - Evaluating question answering over linked data.pdf:application/pdf},
}

@inproceedings{chen_open-domain_2020,
	location = {Online},
	title = {Open-Domain Question Answering},
	url = {https://aclanthology.org/2020.acl-tutorials.8/},
	doi = {10.18653/v1/2020.acl-tutorials.8},
	abstract = {This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering ({QA}), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain {QA}, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain {QA} research in the next phase.},
	pages = {34--37},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Danqi and Yih, Wen-tau},
	editor = {Savary, Agata and Zhang, Yue},
	urldate = {2025-01-26},
	date = {2020-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\G8PYH4NM\\Chen und Yih - 2020 - Open-Domain Question Answering.pdf:application/pdf},
}

@inproceedings{ferret_finding_2001,
	title = {Finding An Answer Based on the Recognition of the Question Focus},
	url = {https://www.semanticscholar.org/paper/Finding-An-Answer-Based-on-the-Recognition-of-the-Ferret-Grau/5fd3474b58acece35004ac4a08f742be8d58d1cc},
	abstract = {In this report we describe how the {QALC} system (the Question-Answering program of the {LIR} group at {LIMSI}-{CNRS}, already involved in the {QA}-track evaluation at {TREC}9), was improved in order to better extract the very answer in selected sentences. The purpose of the main Question-Answering track in {TREC}10 was to find text sequences no longer than 50 characters or to produce a "no answer" response in case of a lack of answer in the {TREC} corpus.},
	eventtitle = {Text Retrieval Conference},
	author = {Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, A.},
	urldate = {2025-01-25},
	date = {2001},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PAPDKUMU\\Ferret et al. - 2001 - Finding An Answer Based on the Recognition of the Question Focus.pdf:application/pdf},
}

@article{chakraborty_introduction_2021,
	title = {Introduction to neural network-based question answering over knowledge graphs},
	volume = {11},
	rights = {© 2021 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1389},
	doi = {10.1002/widm.1389},
	abstract = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs ({KGQA}) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for {KGQA}. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for {KGQA}, and ease their process of making informed decisions while creating their own {QA} systems. This article is categorized under: Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Technologies {\textgreater} Artificial Intelligence},
	pages = {e1389},
	number = {3},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
	urldate = {2025-01-25},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1389},
	keywords = {deep learning, knowledge graphs, no type information, question answering},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GD8N2ZSF\\Chakraborty et al. - 2021 - Introduction to neural network-based question answering over knowledge graphs.pdf:application/pdf},
}

@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2025-01-25},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\YFT9UH6M\\Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on Open-domain Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8KVXAFE2\\2101.html:text/html},
}

@inproceedings{zhang_question_2003,
	location = {New York, {NY}, {USA}},
	title = {Question classification using support vector machines},
	isbn = {978-1-58113-646-3},
	url = {https://dl.acm.org/doi/10.1145/860435.860443},
	doi = {10.1145/860435.860443},
	series = {{SIGIR} '03},
	abstract = {Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors ({NN}), Naive Bayes ({NB}), Decision Tree ({DT}), Sparse Network of Winnows ({SNoW}), and Support Vector Machines ({SVM}) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the {SVM} outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the {SVM} to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the {TREC} {QA} track.},
	pages = {26--32},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dell and Lee, Wee Sun},
	urldate = {2025-01-25},
	date = {2003-07-28},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\V9Q4AG85\\Zhang und Lee - 2003 - Question classification using support vector machines.pdf:application/pdf},
}

@misc{jauhar_tabmcq_2016,
	title = {{TabMCQ}: A Dataset of General Knowledge Tables and Multiple-choice Questions},
	url = {http://arxiv.org/abs/1602.03960},
	doi = {10.48550/arXiv.1602.03960},
	shorttitle = {{TabMCQ}},
	abstract = {We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.},
	number = {{arXiv}:1602.03960},
	publisher = {{arXiv}},
	author = {Jauhar, Sujay Kumar and Turney, Peter and Hovy, Eduard},
	urldate = {2025-01-25},
	date = {2016-02-12},
	eprinttype = {arxiv},
	eprint = {1602.03960 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\BZIEL7IT\\Jauhar et al. - 2016 - TabMCQ A Dataset of General Knowledge Tables and Multiple-choice Questions.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\24MCNUGZ\\1602.html:text/html},
}

@inproceedings{bloehdorn_ontology-based_2007,
	location = {Berlin, Heidelberg},
	title = {Ontology-Based Question Answering for Digital Libraries},
	isbn = {978-3-540-74851-9},
	doi = {10.1007/978-3-540-74851-9_2},
	abstract = {In this paper we present an approach to question answering over heterogeneous knowledge sources that makes use of different ontology management components within the scenario of a digital library application. We present a principled framework for integrating structured metadata and unstructured resource content in a seamless manner which can then be flexibly queried using structured queries expressed in natural language. The novelty of the approach lies in the combination of different semantic technologies providing a clear benefit for the application scenario considered. The resulting system is implemented as part of the digital library of British Telecommunications ({BT}). The original contribution of our paper lies in the architecture we present allowing for the non-straightforward integration of the different components we consider.},
	pages = {14--25},
	booktitle = {Research and Advanced Technology for Digital Libraries},
	publisher = {Springer},
	author = {Bloehdorn, Stephan and Cimiano, Philipp and Duke, Alistair and Haase, Peter and Heizmann, Jörg and Thurlow, Ian and Völker, Johanna},
	editor = {Kovács, László and Fuhr, Norbert and Meghini, Carlo},
	date = {2007},
	langid = {english},
	keywords = {Conjunctive Query, Digital Library, Intellectual Capital, Knowledge Source, Resource Description Framework},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XUA7RHGF\\Bloehdorn et al. - 2007 - Ontology-Based Question Answering for Digital Libraries.pdf:application/pdf},
}

@inproceedings{soleimani_nlquad_2021,
	location = {Online},
	title = {{NLQuAD}: A Non-Factoid Long Question Answering Data Set},
	url = {https://aclanthology.org/2021.eacl-main.106/},
	doi = {10.18653/v1/2021.eacl-main.106},
	shorttitle = {{NLQuAD}},
	abstract = {We introduce {NLQuAD}, the first data set with baseline methods for non-factoid long question answering, a task requiring document-level language understanding. In contrast to existing span detection question answering data sets, {NLQuAD} has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ({IoU}), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare {BERT}, {RoBERTa}, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. {NLQuAD}`s samples exceed the input limitation of most pre-trained Transformer-based models, encouraging future research on long sequence language models.},
	eventtitle = {{EACL} 2021},
	pages = {1245--1255},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Soleimani, Amir and Monz, Christof and Worring, Marcel},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	urldate = {2025-01-25},
	date = {2021-04},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M7TLBDYR\\Soleimani et al. - 2021 - NLQuAD A Non-Factoid Long Question Answering Data Set.pdf:application/pdf},
}

@misc{bajaj_ms_2018,
	title = {{MS} {MARCO}: A Human Generated {MAchine} Reading {COmprehension} Dataset},
	url = {http://arxiv.org/abs/1611.09268},
	doi = {10.48550/arXiv.1611.09268},
	shorttitle = {{MS} {MARCO}},
	abstract = {We introduce a large scale {MAchine} Reading {COmprehension} dataset, which we name {MS} {MARCO}. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the {MS} {MARCO} dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes {MS} {MARCO} from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	number = {{arXiv}:1611.09268},
	publisher = {{arXiv}},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and {McNamara}, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	urldate = {2025-01-25},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1611.09268 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\JSFLTXCA\\Bajaj et al. - 2018 - MS MARCO A Human Generated MAchine Reading COmprehension Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\6GPZ3U5B\\1611.html:text/html},
}

@inproceedings{hashemi_performance_2019,
	location = {New York, {NY}, {USA}},
	title = {Performance Prediction for Non-Factoid Question Answering},
	isbn = {978-1-4503-6881-0},
	url = {https://dl.acm.org/doi/10.1145/3341981.3344249},
	doi = {10.1145/3341981.3344249},
	series = {{ICTIR} '19},
	abstract = {Estimating the quality of a result list, often referred to as query performance prediction ({QPP}), is a challenging and important task in information retrieval. It can be used as feedback to users, search engines, and system administrators. Although predicting the performance of retrieval models has been extensively studied for the ad-hoc retrieval task, the effectiveness of performance prediction methods for question answering ({QA}) systems is relatively unstudied. The short length of answers, the dominance of neural models in {QA}, and the re-ranking nature of most {QA} systems make performance prediction for {QA} a unique, important, and technically interesting task. In this paper, we introduce and motivate the task of performance prediction for non-factoid question answering and propose a neural performance predictor for this task. Our experiments on two recent datasets demonstrate that the proposed model outperforms competitive baselines in all settings.},
	pages = {55--58},
	booktitle = {Proceedings of the 2019 {ACM} {SIGIR} International Conference on Theory of Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Hashemi, Helia and Zamani, Hamed and Croft, W. Bruce},
	urldate = {2025-01-25},
	date = {2019-09-26},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\H7C4YCFS\\Hashemi et al. - 2019 - Performance Prediction for Non-Factoid Question Answering.pdf:application/pdf},
}

@inproceedings{bu_function-based_2010,
	location = {Cambridge, {MA}},
	title = {Function-Based Question Classification for General {QA}},
	url = {https://aclanthology.org/D10-1109/},
	eventtitle = {{EMNLP} 2010},
	pages = {1119--1128},
	booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Bu, Fan and Zhu, Xingwei and Hao, Yu and Zhu, Xiaoyan},
	editor = {Li, Hang and Màrquez, Lluís},
	urldate = {2025-01-25},
	date = {2010-10},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HBB3WH6L\\Bu et al. - 2010 - Function-Based Question Classification for General QA.pdf:application/pdf},
}

@inproceedings{suzuki_question_2003,
	location = {Sapporo, Japan},
	title = {Question Classification using {HDAG} Kernel},
	url = {https://aclanthology.org/W03-1208/},
	doi = {10.3115/1119312.1119320},
	eventtitle = {{MultiLing} 2003},
	pages = {61--68},
	booktitle = {Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering},
	publisher = {Association for Computational Linguistics},
	author = {Suzuki, Jun and Taira, Hirotoshi and Sasaki, Yutaka and Maeda, Eisaku},
	urldate = {2025-01-25},
	date = {2003-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\VTNT9RLL\\Suzuki et al. - 2003 - Question Classification using HDAG Kernel.pdf:application/pdf},
}

@inproceedings{zafar_formal_2018,
	location = {Cham},
	title = {Formal Query Generation for Question Answering over Knowledge Bases},
	isbn = {978-3-319-93417-4},
	doi = {10.1007/978-3-319-93417-4_46},
	abstract = {Question answering ({QA}) systems often consist of several components such as Named Entity Disambiguation ({NED}), Relation Extraction ({RE}), and Query Generation ({QG}). In this paper, we focus on the {QG} process of a {QA} pipeline on a large-scale Knowledge Base ({KB}), with noisy annotations and complex sentence structures. We therefore propose {SQG}, a {SPARQL} Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional {QA} pipeline. {SQG} can be used on large open-domain {KBs} and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the {NED} and {RE} components. This ability allows {SQG} to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the {QG} component. The captured subgraph covers multiple candidate walks, which correspond to {SPARQL} queries. To enhance the accuracy, we present a ranking model based on Tree-{LSTM} that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. {SQG} outperforms the baseline systems and achieves a macro F1-measure of 75\% on the {LC}-{QuAD} dataset.},
	pages = {714--728},
	booktitle = {The Semantic Web},
	publisher = {Springer International Publishing},
	author = {Zafar, Hamid and Napolitano, Giulio and Lehmann, Jens},
	editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Raphaël and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
	date = {2018},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3HMWC8A9\\Zafar et al. - 2018 - Formal Query Generation for Question Answering over Knowledge Bases.pdf:application/pdf},
}

@misc{serban_generating_2016,
	title = {Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus},
	url = {http://arxiv.org/abs/1603.06807},
	doi = {10.48550/arXiv.1603.06807},
	shorttitle = {Generating Factoid Questions With Recurrent Neural Networks},
	abstract = {Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.},
	number = {{arXiv}:1603.06807},
	publisher = {{arXiv}},
	author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-01-25},
	date = {2016-05-29},
	eprinttype = {arxiv},
	eprint = {1603.06807 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PI2THVTF\\Serban et al. - 2016 - Generating Factoid Questions With Recurrent Neural Networks The 30M Factoid Question-Answer Corpus.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8FZLF5NM\\1603.html:text/html},
}

@inproceedings{usbeck_9th_2018,
	title = {9th Challenge on Question Answering over Linked Data ({QALD}-9) (invited paper)},
	url = {https://www.semanticscholar.org/paper/9th-Challenge-on-Question-Answering-over-Linked-Usbeck-Gusmita/4f83e1b64f57ae0d546076279426e85c0e60298b},
	abstract = {Recent years have seen a growing amount of research on question answering ({QA}) over Semantic Web data, shaping an interaction paradigm that allows end users to profit from the expressive power of Semantic Web standards. At the same time, {QA} systems hide their complexity behind an intuitive and easy-touse interface. However, the growing amount of data available on the Semantic Web has led to a heterogeneous data landscape where {QA} systems struggle to keep up with the volume, variety and veracity of the underlying knowledge. The Question Answering over Linked Data ({QALD}) challenges aim to provide up-to-date benchmarks for assessing and comparing state-of-the-art systems that mediate between a user, expressing his or her information need in natural language, and {RDF} data. In the past few years, more than 40 research groups and their systems have taken part in the last nine {QALD} challenges. The {QALD} challenge targets all researchers and practitioners working on querying Linked Data, natural language processing for question answering, multilingual information retrieval and related topics. The main goal is to gain insights into the strengths and shortcomings of different approaches and into possible solutions for coping with the large, heterogeneous and distributed nature of Semantic Web data. {QALD} has a 8-year history. The challenge began in 2011 and is developing benchmarks that are increasingly being used as a standard evaluation venue for question answering over Linked Data. Overviews of past instantiations of the challenge are available from the {CLEF} Working Notes, {CEUR} workshop notes as well as {ESWC} proceedings, see Table 1. This article will give a technical overview of the task and results of the 9th Question Answering over Linked Data challenge.},
	eventtitle = {Semdeep/{NLIWoD}@{ISWC}},
	author = {Usbeck, Ricardo and Gusmita, Ria Hari and Ngomo, A. and Saleem, Muhammad},
	urldate = {2025-01-25},
	date = {2018},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\W3TV3V9D\\Usbeck et al. - 2018 - 9th Challenge on Question Answering over Linked Data (QALD-9) (invited paper).pdf:application/pdf},
}

@inproceedings{dubey_asknow_2016,
	location = {Cham},
	title = {{AskNow}: A Framework for Natural Language Query Formalization in {SPARQL}},
	isbn = {978-3-319-34129-3},
	doi = {10.1007/978-3-319-34129-3_19},
	shorttitle = {{AskNow}},
	abstract = {Natural Language Query Formalization involves semantically parsing queries in natural language and translating them into their corresponding formal representations. It is a key component for developing question-answering ({QA}) systems on {RDF} data. The chosen formal representation language in this case is often {SPARQL}. In this paper, we propose a framework, called {AskNow}, where users can pose queries in English to a target {RDF} knowledge base (e.g. {DBpedia}), which are first normalized into an intermediary canonical syntactic form, called Normalized Query Structure ({NQS}), and then translated into {SPARQL} queries. {NQS} facilitates the identification of the desire (or expected output information) and the user-provided input information, and establishing their mutual semantic relationship. At the same time, it is sufficiently adaptive to query paraphrasing. We have empirically evaluated the framework with respect to the syntactic robustness of {NQS} and semantic accuracy of the {SPARQL} translator on standard benchmark datasets.},
	pages = {300--316},
	booktitle = {The Semantic Web. Latest Advances and New Domains},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Dasgupta, Sourish and Sharma, Ankit and Höffner, Konrad and Lehmann, Jens},
	editor = {Sack, Harald and Blomqvist, Eva and d'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
	date = {2016},
	langid = {english},
	keywords = {{DBpedia}, Natural Language Query Formalization ({NLQF}), Query Paraphrases, Semantic Accuracy, {SPARQL} Translation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3W9R43XE\\Dubey et al. - 2016 - AskNow A Framework for Natural Language Query Formalization in SPARQL.pdf:application/pdf},
}
