@article{agee_developing_2009,
 abstract = {The reflective and interrogative processes required for developing effective qualitative research questions can give shape and direction to a study in ways that are often underestimated. Good research questions do not necessarily produce good research, but poorly conceived or constructed questions will likely create problems that affect all subsequent stages of a study. In qualitative studies, the ongoing process of questioning is an integral part of understanding the unfolding lives and perspectives of others. This article addresses both the development of initial research questions and how the processes of generating and refining questions are critical to the shaping of a qualitative study.},
 author = {Agee, Jane},
 date = {2009-07-01},
 doi = {10.1080/09518390902736512},
 issn = {0951-8398},
 journaltitle = {International Journal of Qualitative Studies in Education},
 keywords = {no type information, notion},
 note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09518390902736512},
 number = {4},
 pages = {431--447},
 shorttitle = {Developing qualitative research questions},
 title = {Developing qualitative research questions: a reflective process},
 url = {https://doi.org/10.1080/09518390902736512},
 urldate = {2024-11-26},
 volume = {22}
}

@inproceedings{allam_question_2016,
 abstract = {Question Answering ({QA}) is a specialized area in the field of Information Retrieval ({IR}). The {QA} systems are concerned with providing relevant answers in response to questions proposed in natural language. {QA} is therefore composed of three distinct modules, each of which has a core component beside other supplementary components. These three core components are: question classification, information retrieval, and answer extraction. Question classification plays an essential role in {QA} systems by classifying the submitted question according to its type. Information retrieval is very important for question answering, because if no correct answers are present in a document, no further processing could be carried out to find an answer. Finally, answer extraction aims to retrieve the answer for a question asked by the user. This survey paper provides an overview of Question-Answering and its system architecture, as well as the previous related work comparing each research against the others with respect to the components that were covered and the approaches that were followed. At the end, the survey provides an analytical discussion of the proposed {QA} models, along with their main contributions, experimental results, and limitations.},
 author = {Allam, Ali Mohamed Nabil and Haggag, Mohamed H.},
 date = {2016},
 shorttitle = {The Question Answering Systems},
 title = {The Question Answering Systems : A Survey .},
 url = {https://www.semanticscholar.org/paper/The-Question-Answering-Systems-%3A-A-Survey-.-Allam-Haggag/b2944a85b9cb428a28e30bdd236471e712667e91},
 urldate = {2025-02-05}
}

@article{auer_sciqa_2023,
 abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering ({QA}) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as {DBpedia} and Wikidata. We present {SciQA} a scientific {QA} benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph ({ORKG}) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the {ORKG}. The questions cover a range of research fields and question types and are translated into corresponding {SPARQL} queries over the {ORKG}. Based on two preliminary evaluations, we show that the resulting {SciQA} benchmark represents a challenging task for next-generation {QA} systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data ({QALD}) Challenge.},
 author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
 date = {2023-05-04},
 doi = {10.1038/s41598-023-33607-z},
 issn = {2045-2322},
 journaltitle = {Scientific Reports},
 keywords = {finished, notion, related\_dataset},
 langid = {english},
 note = {Publisher: Nature Publishing Group},
 number = {1},
 pages = {7240},
 rights = {2023 The Author(s)},
 shortjournal = {Sci Rep},
 title = {The {SciQA} Scientific Question Answering Benchmark for Scholarly Knowledge},
 url = {https://www.nature.com/articles/s41598-023-33607-z},
 urldate = {2024-08-26},
 volume = {13}
}

@misc{bajaj_ms_2018,
 abstract = {We introduce a large scale {MAchine} Reading {COmprehension} dataset, which we name {MS} {MARCO}. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the {MS} {MARCO} dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes {MS} {MARCO} from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
 author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and {McNamara}, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
 date = {2018-10-31},
 doi = {10.48550/arXiv.1611.09268},
 eprint = {1611.09268 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
 number = {{arXiv}:1611.09268},
 publisher = {{arXiv}},
 shorttitle = {{MS} {MARCO}},
 title = {{MS} {MARCO}: A Human Generated {MAchine} Reading {COmprehension} Dataset},
 url = {http://arxiv.org/abs/1611.09268},
 urldate = {2025-01-25}
}

@misc{banerjee_dblp-quad_2023,
 abstract = {In this work we create a question answering dataset over the {DBLP} scholarly knowledge graph ({KG}). {DBLP} is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding {SPARQL} queries which can be executed over the {DBLP} {KG} to fetch the correct answer. {DBLP}-{QuAD} is the largest scholarly question answering dataset.},
 author = {Banerjee, Debayan and Awale, Sushil and Usbeck, Ricardo and Biemann, Chris},
 date = {2023-03-29},
 keywords = {finished, notion, related\_dataset},
 langid = {english},
 number = {{arXiv}:2303.13351},
 publisher = {{arXiv}},
 shorttitle = {{DBLP}-{QuAD}},
 title = {{DBLP}-{QuAD}: A Question Answering Dataset over the {DBLP} Scholarly Knowledge Graph},
 url = {http://arxiv.org/abs/2303.13351},
 urldate = {2024-07-09}
}

@incollection{beck_chapter_2023,
 abstract = {Regardless of their level of training, clinicians encounter questions relating to patient care and outcomes on a daily basis. Clarifying an answer to these questions not only benefits the clinicians' immediate patients, but also those of his or her colleagues by adding to the existing scientific literature. Fine tuning one’s initial question into a hypothesis and eventual study and analysis sets the stage for a successful project. Utilizing the acronyms {PICOT}, {FINER}, and {SPIDER} helps researchers set reasonable and measurable objectives and aims, thereby avoiding the common pitfalls in question development.},
 author = {Beck, Lauren L.},
 booktitle = {Translational Surgery},
 date = {2023-01-01},
 doi = {10.1016/B978-0-323-90300-4.00107-5},
 editor = {Eltorai, Adam E. M. and Bakal, Jeffrey A. and Newell, Paige C. and Osband, Adena J.},
 isbn = {978-0-323-90300-4},
 keywords = {{FINER}, no type information, {PICOT}, Research aim, Research objective, Research questions, Study development},
 pages = {111--120},
 publisher = {Academic Press},
 series = {Handbook for Designing and Conducting Clinical and Translational Research},
 shorttitle = {Chapter 18 - The question},
 title = {Chapter 18 - The question: types of research questions and how to develop them},
 url = {https://www.sciencedirect.com/science/article/pii/B9780323903004001075},
 urldate = {2025-01-25}
}

@inproceedings{bloehdorn_ontology-based_2007,
 abstract = {In this paper we present an approach to question answering over heterogeneous knowledge sources that makes use of different ontology management components within the scenario of a digital library application. We present a principled framework for integrating structured metadata and unstructured resource content in a seamless manner which can then be flexibly queried using structured queries expressed in natural language. The novelty of the approach lies in the combination of different semantic technologies providing a clear benefit for the application scenario considered. The resulting system is implemented as part of the digital library of British Telecommunications ({BT}). The original contribution of our paper lies in the architecture we present allowing for the non-straightforward integration of the different components we consider.},
 author = {Bloehdorn, Stephan and Cimiano, Philipp and Duke, Alistair and Haase, Peter and Heizmann, Jörg and Thurlow, Ian and Völker, Johanna},
 booktitle = {Research and Advanced Technology for Digital Libraries},
 date = {2007},
 doi = {10.1007/978-3-540-74851-9_2},
 editor = {Kovács, László and Fuhr, Norbert and Meghini, Carlo},
 isbn = {978-3-540-74851-9},
 keywords = {Conjunctive Query, Digital Library, Intellectual Capital, Knowledge Source, Resource Description Framework},
 langid = {english},
 location = {Berlin, Heidelberg},
 pages = {14--25},
 publisher = {Springer},
 title = {Ontology-Based Question Answering for Digital Libraries}
}

@inproceedings{bolotova_non-factoid_2022,
 abstract = {Non-factoid question answering ({NFQA}) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - {NFQs}. There is still little understanding of the categories of {NFQs} that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are.  This work presents the first comprehensive taxonomy of {NFQ} categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised {NFQs} and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major {NFQA} datasets, showing that the {NFQ} categories that are the most challenging for current {NFQA} systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy, along with the category classifier, will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories.},
 author = {Bolotova, Valeriia and Blinov, Vladislav and Scholer, Falk and Croft, W. Bruce and Sanderson, Mark},
 booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
 date = {2022-07-07},
 doi = {10.1145/3477495.3531926},
 isbn = {978-1-4503-8732-3},
 keywords = {finished},
 location = {New York, {NY}, {USA}},
 pages = {1196--1207},
 publisher = {Association for Computing Machinery},
 series = {{SIGIR} '22},
 title = {A Non-Factoid Question-Answering Taxonomy},
 url = {https://dl.acm.org/doi/10.1145/3477495.3531926},
 urldate = {2025-01-25}
}

@misc{bordes_large-scale_2015,
 abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
 author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
 date = {2015-06-05},
 eprint = {1506.02075 [cs]},
 eprinttype = {arxiv},
 keywords = {finished, notion},
 number = {{arXiv}:1506.02075},
 publisher = {{arXiv}},
 title = {Large-scale Simple Question Answering with Memory Networks},
 url = {http://arxiv.org/abs/1506.02075},
 urldate = {2024-09-23}
}

@misc{bordes_large-scale_2015,
 abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
 author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
 date = {2015-06-05},
 eprint = {1506.02075 [cs]},
 eprinttype = {arxiv},
 keywords = {finished, notion},
 number = {{arXiv}:1506.02075},
 publisher = {{arXiv}},
 title = {Large-scale Simple Question Answering with Memory Networks},
 url = {http://arxiv.org/abs/1506.02075},
 urldate = {2024-09-23}
}

@inproceedings{britto_blooms_2015,
 abstract = {Designing and assessing learning outcomes could be a challenging activity for any Software Engineering ({SE}) educator. To support the process of designing and assessing {SE} courses, educators have been applied the cognitive domain of Bloom's taxonomy. However, to the best of our knowledge, the evidence on the usage of Bloom's taxonomy in {SE} higher education has not yet been systematically aggregated or reviewed. Therefore, in this paper we report the state of the art on the usage of Bloom's taxonomy in {SE} education, identified by conducted a systematic mapping study. As a result of the performed systematic mapping study, 26 studies were deemed as relevant. The main findings from these studies are: i) Bloom's taxonomy has mostly been applied at undergraduate level for both design and assessment of software engineering courses; ii) software construction is the leading {SE} subarea in which Bloom's taxonomy has been applied. The results clearly point out the usefulness of Bloom's taxonomy in the {SE} education context. We intend to use the results from this systematic mapping study to develop a set of guidelines to support the usage of Bloom's taxonomy cognitive levels to design and assess {SE} courses.},
 author = {Britto, Ricardo and Usman, Muhammad},
 booktitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
 date = {2015-10},
 doi = {10.1109/FIE.2015.7344084},
 eventtitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
 keywords = {Context, Data mining, Education, no type information, Software, Software engineering, Systematics, Taxonomy},
 pages = {1--8},
 shorttitle = {Bloom's taxonomy in software engineering education},
 title = {Bloom's taxonomy in software engineering education: A systematic mapping study},
 url = {https://ieeexplore.ieee.org/abstract/document/7344084},
 urldate = {2025-01-25}
}

@article{britto_extended_2016,
 abstract = {In Global Software Engineering ({GSE}), the need for a common terminology and knowledge classification has been identified to facilitate the sharing and combination of knowledge by {GSE} researchers and practitioners. A {GSE} taxonomy was recently proposed to address such a need, focusing on a core set of dimensions; however its dimensions do not represent an exhaustive list of relevant {GSE} factors. Therefore, this study extends the existing taxonomy, incorporating new {GSE} dimensions that were identified by means of two empirical studies conducted recently.},
 author = {Britto, Ricardo and Wohlin, Claes and Mendes, Emilia},
 date = {2016-06-07},
 doi = {10.1186/s40411-016-0029-2},
 issn = {2195-1721},
 journaltitle = {Journal of Software Engineering Research and Development},
 keywords = {Global software engineering, Knowledge classification, no type information, Software engineering management, Taxonomy},
 langid = {english},
 number = {1},
 pages = {3},
 shortjournal = {J Softw Eng Res Dev},
 title = {An extended global software engineering taxonomy},
 url = {https://doi.org/10.1186/s40411-016-0029-2},
 urldate = {2025-01-25},
 volume = {4}
}

@inproceedings{bu_function-based_2010,
 author = {Bu, Fan and Zhu, Xingwei and Hao, Yu and Zhu, Xiaoyan},
 booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
 date = {2010-10},
 editor = {Li, Hang and Màrquez, Lluís},
 eventtitle = {{EMNLP} 2010},
 location = {Cambridge, {MA}},
 pages = {1119--1128},
 publisher = {Association for Computational Linguistics},
 title = {Function-Based Question Classification for General {QA}},
 url = {https://aclanthology.org/D10-1109/},
 urldate = {2025-01-25}
}

@misc{chakraborty_introduction_2019,
 abstract = {Question answering has emerged as an intuitive way of querying structured data sources, and has attracted significant advancements over the years. In this article, we provide an overview over these recent advancements, focusing on neural network based question answering systems over knowledge graphs. We introduce readers to the challenges in the tasks, current paradigms of approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point, and ease their process of making informed decisions while creating their own {QA} system.},
 author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
 date = {2019-07-22},
 doi = {10.48550/arXiv.1907.09361},
 eprint = {1907.09361 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
 number = {{arXiv}:1907.09361},
 publisher = {{arXiv}},
 title = {Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs},
 url = {http://arxiv.org/abs/1907.09361},
 urldate = {2025-01-24}
}

@article{chakraborty_introduction_2021,
 abstract = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs ({KGQA}) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for {KGQA}. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for {KGQA}, and ease their process of making informed decisions while creating their own {QA} systems. This article is categorized under: Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Technologies {\textgreater} Artificial Intelligence},
 author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
 date = {2021},
 doi = {10.1002/widm.1389},
 issn = {1942-4795},
 journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
 keywords = {deep learning, knowledge graphs, no type information, question answering},
 langid = {english},
 note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1389},
 number = {3},
 pages = {e1389},
 rights = {© 2021 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
 title = {Introduction to neural network-based question answering over knowledge graphs},
 url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1389},
 urldate = {2025-01-25},
 volume = {11}
}

@article{chakraborty_introduction_2021,
 abstract = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs ({KGQA}) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for {KGQA}. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for {KGQA}, and ease their process of making informed decisions while creating their own {QA} systems. This article is categorized under: Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Technologies {\textgreater} Artificial Intelligence},
 author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
 date = {2021},
 doi = {10.1002/widm.1389},
 issn = {1942-4795},
 journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
 keywords = {deep learning, knowledge graphs, no type information, question answering},
 langid = {english},
 note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1389},
 number = {3},
 pages = {e1389},
 rights = {© 2021 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
 title = {Introduction to neural network-based question answering over knowledge graphs},
 url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1389},
 urldate = {2025-01-25},
 volume = {11}
}

@inproceedings{chen_open-domain_2020,
 abstract = {This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering ({QA}), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain {QA}, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain {QA} research in the next phase.},
 author = {Chen, Danqi and Yih, Wen-tau},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
 date = {2020-07},
 doi = {10.18653/v1/2020.acl-tutorials.8},
 editor = {Savary, Agata and Zhang, Yue},
 location = {Online},
 pages = {34--37},
 publisher = {Association for Computational Linguistics},
 title = {Open-Domain Question Answering},
 url = {https://aclanthology.org/2020.acl-tutorials.8/},
 urldate = {2025-01-26}
}

@inproceedings{chen_open-domain_2020,
 abstract = {This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering ({QA}), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain {QA}, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain {QA} research in the next phase.},
 author = {Chen, Danqi and Yih, Wen-tau},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
 date = {2020-07},
 doi = {10.18653/v1/2020.acl-tutorials.8},
 editor = {Savary, Agata and Zhang, Yue},
 location = {Online},
 pages = {34--37},
 publisher = {Association for Computational Linguistics},
 title = {Open-Domain Question Answering},
 url = {https://aclanthology.org/2020.acl-tutorials.8/},
 urldate = {2025-01-26}
}

@article{dillon_classification_1984,
 abstract = {What are the kinds of questions that may be posed for research? A dozen schemes proposing to classify research questions are surveyed, analyzed, and applied to the understanding and practice of inquiry. The extent to which the various schemes account for questions found in educational journals is estimated. Some principles and issues are identified to stimulate work on the classification of research questions in education and other enterprises of inquiry. On the whole, little is known about the kinds of questions that may be posed for research.},
 author = {Dillon, J. T.},
 date = {1984-09-01},
 doi = {10.3102/00346543054003327},
 issn = {0034-6543},
 journaltitle = {Review of Educational Research},
 keywords = {finished, notion},
 langid = {english},
 note = {Publisher: American Educational Research Association},
 number = {3},
 pages = {327--361},
 title = {The Classification of Research Questions},
 url = {https://doi.org/10.3102/00346543054003327},
 urldate = {2024-11-10},
 volume = {54}
}

@inproceedings{dubey_asknow_2016,
 abstract = {Natural Language Query Formalization involves semantically parsing queries in natural language and translating them into their corresponding formal representations. It is a key component for developing question-answering ({QA}) systems on {RDF} data. The chosen formal representation language in this case is often {SPARQL}. In this paper, we propose a framework, called {AskNow}, where users can pose queries in English to a target {RDF} knowledge base (e.g. {DBpedia}), which are first normalized into an intermediary canonical syntactic form, called Normalized Query Structure ({NQS}), and then translated into {SPARQL} queries. {NQS} facilitates the identification of the desire (or expected output information) and the user-provided input information, and establishing their mutual semantic relationship. At the same time, it is sufficiently adaptive to query paraphrasing. We have empirically evaluated the framework with respect to the syntactic robustness of {NQS} and semantic accuracy of the {SPARQL} translator on standard benchmark datasets.},
 author = {Dubey, Mohnish and Dasgupta, Sourish and Sharma, Ankit and Höffner, Konrad and Lehmann, Jens},
 booktitle = {The Semantic Web. Latest Advances and New Domains},
 date = {2016},
 doi = {10.1007/978-3-319-34129-3_19},
 editor = {Sack, Harald and Blomqvist, Eva and d'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
 isbn = {978-3-319-34129-3},
 keywords = {{DBpedia}, Natural Language Query Formalization ({NLQF}), Query Paraphrases, Semantic Accuracy, {SPARQL} Translation},
 langid = {english},
 location = {Cham},
 pages = {300--316},
 publisher = {Springer International Publishing},
 shorttitle = {{AskNow}},
 title = {{AskNow}: A Framework for Natural Language Query Formalization in {SPARQL}}
}

@inproceedings{dubey_lc-quad_2019,
 abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like {WebQuestions}, {QALD} and {LCQuAD} have been published in the past. The biggest data set available for complex questions ({LCQuAD}) over knowledge graphs contains five thousand questions. We now provide {LC}-{QuAD} 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding {SPARQL} queries. {LC}-{QuAD} 2.0 is compatible with both Wikidata and {DBpedia} 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
 author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
 booktitle = {The Semantic Web – {ISWC} 2019},
 date = {2019},
 doi = {10.1007/978-3-030-30796-7_5},
 editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
 isbn = {978-3-030-30796-7},
 keywords = {finished, notion, related\_dataset},
 langid = {english},
 location = {Cham},
 pages = {69--78},
 publisher = {Springer International Publishing},
 shorttitle = {{LC}-{QuAD} 2.0},
 title = {{LC}-{QuAD} 2.0: A Large Dataset for Complex Question Answering over Wikidata and {DBpedia}}
}

@incollection{easterbrook_selecting_2008,
 abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
 author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
 booktitle = {Guide to Advanced Empirical Software Engineering},
 date = {2008},
 doi = {10.1007/978-1-84800-044-5_11},
 editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
 isbn = {978-1-84800-044-5},
 keywords = {domain\_specific, finished, notion},
 langid = {english},
 location = {London},
 pages = {285--311},
 publisher = {Springer},
 title = {Selecting Empirical Methods for Software Engineering Research},
 url = {https://doi.org/10.1007/978-1-84800-044-5_11},
 urldate = {2024-10-02}
}

@article{fandino_formulating_2019,
 abstract = {The process of formulating a good research question can be challenging and frustrating. While a comprehensive literature review is compulsory, the researcher usually encounters methodological difficulties in the conduct of the study, particularly if the primary study question has not been adequately selected in accordance with the clinical dilemma that needs to be addressed. Therefore, optimising time and resources before embarking in the design of a clinical protocol can make an impact on the final results of the research project. Researchers have developed effective ways to convey the message of how to build a good research question that can be easily recalled under the acronyms of {PICOT} (population, intervention, comparator, outcome, and time frame) and {FINER} (feasible, interesting, novel, ethical, and relevant). In line with these concepts, this article highlights the main issues faced by clinicians, when developing a research question.},
 author = {Fandino, Wilson},
 date = {2019-08},
 doi = {10.4103/ija.IJA_198_19},
 issn = {0019-5049},
 journaltitle = {Indian Journal of Anaesthesia},
 keywords = {no type information, notion},
 number = {8},
 pages = {611--616},
 pmcid = {PMC6691636},
 pmid = {31462805},
 shortjournal = {Indian J Anaesth},
 shorttitle = {Formulating a good research question},
 title = {Formulating a good research question: Pearls and pitfalls},
 url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6691636/},
 urldate = {2024-11-26},
 volume = {63}
}

@inproceedings{feng_multi-hop_2022,
 abstract = {Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as {DEHG}, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. {DEHG} employs a graph constructor to integrate structured and unstructured information, a context encoder to represent nodes and question, a heterogeneous information reasoning layer to conduct multi-hop reasoning on both information sources, and an answer decoder to generate answers for the question. Experimental results on {HybirdQA} dataset show that {DEHG} outperforms the state-of-the-art methods.},
 author = {Feng, Yue and Han, Zhen and Sun, Mingming and Li, Ping},
 booktitle = {Findings of the Association for Computational Linguistics: {NAACL} 2022},
 date = {2022-07},
 doi = {10.18653/v1/2022.findings-naacl.12},
 editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
 eventtitle = {Findings 2022},
 location = {Seattle, United States},
 pages = {151--156},
 publisher = {Association for Computational Linguistics},
 title = {Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge},
 url = {https://aclanthology.org/2022.findings-naacl.12/},
 urldate = {2025-01-26}
}

@inproceedings{ferret_finding_2001,
 abstract = {In this report we describe how the {QALC} system (the Question-Answering program of the {LIR} group at {LIMSI}-{CNRS}, already involved in the {QA}-track evaluation at {TREC}9), was improved in order to better extract the very answer in selected sentences. The purpose of the main Question-Answering track in {TREC}10 was to find text sequences no longer than 50 characters or to produce a "no answer" response in case of a lack of answer in the {TREC} corpus.},
 author = {Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, A.},
 date = {2001},
 eventtitle = {Text Retrieval Conference},
 title = {Finding An Answer Based on the Recognition of the Question Focus},
 url = {https://www.semanticscholar.org/paper/Finding-An-Answer-Based-on-the-Recognition-of-the-Ferret-Grau/5fd3474b58acece35004ac4a08f742be8d58d1cc},
 urldate = {2025-01-25}
}

@article{frank_question_2007,
 abstract = {We present an implemented approach for domain-restricted question answering from structured knowledge sources, based on robust semantic analysis in a hybrid {NLP} system architecture. We perform question interpretation and answer extraction in an architecture that builds on a lexical-conceptual structure for question interpretation, which is interfaced with domain-specific concepts and properties in a structured knowledge base. Question interpretation involves a limited amount of domain-specific inferences, and accounts for higher-level quantificational questions. Question interpretation and answer extraction are modular components that interact in clearly defined ways. We derive so-called proto queries from the linguistic representations, which provide partial constraints for answer extraction from the underlying knowledge sources. The search queries we construct from proto queries effectively compute minimal spanning trees from the underlying knowledge sources. Our approach naturally extends to multilingual question answering, and has been developed as a prototype system for two application domains: the domain of Nobel prize winners, and the domain of Language Technology, on the basis of the large ontology underlying the information portal {LT} World.},
 author = {Frank, Anette and Krieger, Hans-Ulrich and Xu, Feiyu and Uszkoreit, Hans and Crysmann, Berthold and Jörg, Brigitte and Schäfer, Ulrich},
 date = {2007-03-01},
 doi = {10.1016/j.jal.2005.12.006},
 issn = {1570-8683},
 journaltitle = {Journal of Applied Logic},
 keywords = {Data base queries, Hybrid {NLP}, Multilinguality, no type information, Ontology modeling, {QA}, Question semantics, {RMRS}},
 number = {1},
 pages = {20--48},
 series = {Questions and Answers: Theoretical and Applied Perspectives},
 shortjournal = {Journal of Applied Logic},
 title = {Question answering from structured knowledge sources},
 url = {https://www.sciencedirect.com/science/article/pii/S157086830500090X},
 urldate = {2025-01-26},
 volume = {5}
}

@inproceedings{garcia_aqualog_2006,
 author = {Garcia, Vanessa Lopez and Motta, Enrico and Uren, Victoria},
 booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Demonstrations},
 date = {2006-06},
 editor = {Rudnicky, Alex and Dowding, John and Milic-Frayling, Natasa},
 location = {New York City, {USA}},
 pages = {269--272},
 publisher = {Association for Computational Linguistics},
 shorttitle = {{AquaLog}},
 title = {{AquaLog}: An ontology-driven Question Answering System to interface the Semantic Web},
 url = {https://aclanthology.org/N06-4005/},
 urldate = {2025-01-26}
}

@misc{giglou_scholarly_2024,
 abstract = {This paper introduces a scholarly Question Answering ({QA}) system on top of the {NFDI}4DataScience Gateway, employing a Retrieval Augmented Generation-based ({RAG}) approach. The {NFDI}4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search. The {RAG}-based scholarly {QA}, powered by a Large Language Model ({LLM}), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search. The effectiveness of both the Gateway and the scholarly {QA} system is demonstrated through experimental analysis.},
 author = {Giglou, Hamed Babaei and Taffa, Tilahun Abedissa and Abdullah, Rana and Usmanova, Aida and Usbeck, Ricardo and D'Souza, Jennifer and Auer, Sören},
 date = {2024-06-11},
 doi = {10.48550/arXiv.2406.07257},
 eprint = {2406.07257 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, no type information},
 number = {{arXiv}:2406.07257},
 publisher = {{arXiv}},
 title = {Scholarly Question Answering using Large Language Models in the {NFDI}4DataScience Gateway},
 url = {http://arxiv.org/abs/2406.07257},
 urldate = {2025-01-18}
}

@incollection{gruninger_role_1995,
 abstract = {We present a logical framework for representing activities, states, time, and cost in an enterprise integration architecture. We define ontologies for these concepts in first-order logic and consider the problems of temporal projection and reasoning about the occurrence of actions. We characterize the ontology with the use of competency questions. The ontology must contain a necessary and sufficient set of axioms to represent and solve these questions. These questions not only characterize existing ontologies for enterprise engineering, but also drive the development of new ontologies that are required to solve the competency questions.},
 author = {Grüninger, Michael and Fox, Mark S.},
 booktitle = {Benchmarking — Theory and Practice},
 date = {1995},
 doi = {10.1007/978-0-387-34847-6_3},
 editor = {Rolstadås, Asbjørn},
 isbn = {978-0-387-34847-6},
 keywords = {Enterprise Engineering, Enterprise Model, no type information, notion, Quality Function Deployment, Situation Calculus, Temporal Projection},
 langid = {english},
 location = {Boston, {MA}},
 pages = {22--31},
 publisher = {Springer {US}},
 title = {The Role of Competency Questions in Enterprise Engineering},
 url = {https://doi.org/10.1007/978-0-387-34847-6_3},
 urldate = {2024-11-27}
}

@article{gu_beyond_2021,
 abstract = {Existing studies on question answering on knowledge bases ({KBQA}) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale {KBs} because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that {KBQA} models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of {KBQA} models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, {GrailQA}, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel {BERT}-based {KBQA} model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like {BERT} in the generalization of {KBQA}.1},
 author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
 date = {2021-04-19},
 doi = {10.1145/3442381.3449992},
 journaltitle = {Proceedings of the Web Conference 2021},
 langid = {english},
 note = {Conference Name: {WWW} '21: The Web Conference 2021
{ISBN}: 9781450383127
Place: Ljubljana Slovenia
Publisher: {ACM}},
 pages = {3477--3488},
 shorttitle = {Beyond I.I.D.},
 title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
 url = {https://dl.acm.org/doi/10.1145/3442381.3449992},
 urldate = {2025-01-26}
}

@inproceedings{hashemi_performance_2019,
 abstract = {Estimating the quality of a result list, often referred to as query performance prediction ({QPP}), is a challenging and important task in information retrieval. It can be used as feedback to users, search engines, and system administrators. Although predicting the performance of retrieval models has been extensively studied for the ad-hoc retrieval task, the effectiveness of performance prediction methods for question answering ({QA}) systems is relatively unstudied. The short length of answers, the dominance of neural models in {QA}, and the re-ranking nature of most {QA} systems make performance prediction for {QA} a unique, important, and technically interesting task. In this paper, we introduce and motivate the task of performance prediction for non-factoid question answering and propose a neural performance predictor for this task. Our experiments on two recent datasets demonstrate that the proposed model outperforms competitive baselines in all settings.},
 author = {Hashemi, Helia and Zamani, Hamed and Croft, W. Bruce},
 booktitle = {Proceedings of the 2019 {ACM} {SIGIR} International Conference on Theory of Information Retrieval},
 date = {2019-09-26},
 doi = {10.1145/3341981.3344249},
 isbn = {978-1-4503-6881-0},
 location = {New York, {NY}, {USA}},
 pages = {55--58},
 publisher = {Association for Computing Machinery},
 series = {{ICTIR} '19},
 title = {Performance Prediction for Non-Factoid Question Answering},
 url = {https://dl.acm.org/doi/10.1145/3341981.3344249},
 urldate = {2025-01-25}
}

@inproceedings{hermjakob_parsing_2001,
 author = {Hermjakob, Ulf},
 booktitle = {Proceedings of the {ACL} 2001 Workshop on Open-Domain Question Answering},
 date = {2001},
 keywords = {no type information},
 title = {Parsing and Question Classification for Question Answering},
 url = {https://aclanthology.org/W01-1203/},
 urldate = {2025-01-24}
}

@misc{jaradeh_question_2020,
 abstract = {Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present {JarvisQA}, a {BERT} based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, {JarvisQA} is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.},
 author = {Jaradeh, Mohamad Yaser and Stocker, Markus and Auer, Sören},
 date = {2020-06-02},
 doi = {10.48550/arXiv.2006.01527},
 eprint = {2006.01527},
 eprinttype = {arxiv},
 keywords = {finished, notion, related\_dataset},
 number = {{arXiv}:2006.01527},
 publisher = {{arXiv}},
 title = {Question Answering on Scholarly Knowledge Graphs},
 url = {http://arxiv.org/abs/2006.01527},
 urldate = {2024-11-27}
}

@misc{jauhar_tabmcq_2016,
 abstract = {We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.},
 author = {Jauhar, Sujay Kumar and Turney, Peter and Hovy, Eduard},
 date = {2016-02-12},
 doi = {10.48550/arXiv.1602.03960},
 eprint = {1602.03960 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Computation and Language},
 number = {{arXiv}:1602.03960},
 publisher = {{arXiv}},
 shorttitle = {{TabMCQ}},
 title = {{TabMCQ}: A Dataset of General Knowledge Tables and Multiple-choice Questions},
 url = {http://arxiv.org/abs/1602.03960},
 urldate = {2025-01-25}
}

@misc{jia_leveraging_2024,
 abstract = {The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science ({CS}) researchers at the Australian National University ({ANU}). The system integrates Large Language Models ({LLMs}) with the {ANU} Scholarly Knowledge Graph ({ASKG}), a structured repository of all research-related artifacts produced at {ANU} in the {CS} field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph ({KG}). To address the limitations of traditional scholarly {KG} construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model ({DDM}) for comprehensive document representation and the {KG}-enhanced Query Processing ({KGQP}) for optimized complex query handling. {DDM} enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while {KGQP} leverages the {KG} structure to improve query accuracy and efficiency with {LLMs}. By combining the {ASKG} with {LLMs}, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic {LLM}-{SPARQL} fusion to retrieve relevant facts and textual nodes from the {ASKG}. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency. We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with {LLMs}.},
 author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio J. Rodríguez and Omran, Pouya G.},
 date = {2024-05-24},
 doi = {10.48550/arXiv.2405.15374},
 eprint = {2405.15374 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, no type information},
 number = {{arXiv}:2405.15374},
 publisher = {{arXiv}},
 title = {Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph},
 url = {http://arxiv.org/abs/2405.15374},
 urldate = {2025-01-26}
}

@article{kamper_types_2020,
 abstract = {A previous Evidence in Practice article explained why a specific and answerable research question is important for clinicians and researchers. Determining whether a study aims to answer a descriptive, predictive, or causal question should be one of the first things a reader does when reading an article. Any type of question can be relevant and useful to support evidence-based practice, but only if the question is well defined, matched to the right study design, and reported correctly. J Orthop Sports Phys Ther 2020;50(8):468–469. doi:10.2519/jospt.2020.0703},
 author = {Kamper, Steven J.},
 date = {2020-08},
 doi = {10.2519/jospt.2020.0703},
 issn = {0190-6011},
 journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
 keywords = {clinical practice, evidence-based practice, finished, research, study quality},
 note = {Publisher: Journal of Orthopaedic \& Sports Physical Therapy},
 number = {8},
 pages = {468--469},
 shorttitle = {Types of Research Questions},
 title = {Types of Research Questions: Descriptive, Predictive, or Causal},
 url = {https://www.jospt.org/doi/full/10.2519/jospt.2020.0703},
 urldate = {2025-01-25},
 volume = {50}
}

@inproceedings{karras_divide_2023,
 abstract = {[Background.] Empirical research in requirements engineering ({RE}) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its “current” state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph ({ORKG}) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in {RE} ({KG}-{EmpiRE}) whose data is openly available. Our long-term goal is to continuously maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. [Method.] We conduct a literature review using the {ORKG} to build and publish {KG}-{EmpiRE} which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020–2025. [Results.] From 570 papers of the {IEEE} International Requirements Engineering Conference (2000–2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The {ORKG} is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
 author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
 booktitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
 date = {2023-10},
 doi = {10.1109/ESEM56168.2023.10304795},
 eventtitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
 keywords = {domain\_specific, finished, notion},
 pages = {1--12},
 shorttitle = {Divide and Conquer the {EmpiRE}},
 title = {Divide and Conquer the {EmpiRE}: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering},
 url = {https://ieeexplore.ieee.org/document/10304795},
 urldate = {2024-09-28}
}

@inproceedings{kotonya_towards_2003,
 abstract = {Accurate and timely information is a key motivator in the widespread adoption of {CBSE} technology in Europe. Although there are overlaps and informal communications between researchers and adopters of {CBSE} technology in Europe, there is no systematic mechanism for information interchange between the two. {CBSEnet} is a European Union initiative to create an Internet-based forum for the exchange of information between researchers and adopters of {CBSE}. We describe a proposed classification model for {CBSE} research that will form the basis for structuring the {CBSEnet} knowledge base.},
 author = {{Kotonya} and {Sommerville} and {Hall}},
 booktitle = {2003 Proceedings 29th Euromicro Conference},
 date = {2003-09},
 doi = {10.1109/EURMIC.2003.1231566},
 eventtitle = {2003 Proceedings 29th Euromicro Conference},
 keywords = {Electronic data interchange, Internet, no type information, Object oriented programming, Software development management},
 note = {{ISSN}: 1089-6503},
 pages = {43--52},
 title = {Towards a classification model for component-based software engineering research},
 url = {https://ieeexplore.ieee.org/abstract/document/1231566},
 urldate = {2025-01-25}
}

@article{kwiatkowski_natural_2019,
 abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
 author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
 date = {2019-11},
 doi = {10.1162/tacl_a_00276},
 issn = {2307-387X},
 journaltitle = {Transactions of the Association for Computational Linguistics},
 langid = {english},
 pages = {453--466},
 shortjournal = {Transactions of the Association for Computational Linguistics},
 shorttitle = {Natural Questions},
 title = {Natural Questions: A Benchmark for Question Answering Research},
 url = {https://direct.mit.edu/tacl/article/43518},
 urldate = {2025-01-26},
 volume = {7}
}

@inproceedings{lehmann_large_2024,
 abstract = {The {SciQA} benchmark for scientific question answering aims to represent a challenging task for next-generation question-answering systems on which vanilla large language models fail. In this article, we provide an analysis of the performance of language models on this benchmark including prompting and fine-tuning techniques to adapt them to the {SciQA} task. We show that both fine-tuning and prompting techniques with intelligent few-shot selection allow us to obtain excellent results on the {SciQA} benchmark. We discuss the valuable lessons and common error categories, and outline their implications on how to optimise large language models for question answering over knowledge graphs.},
 author = {Lehmann, Jens and Meloni, Antonello and Motta, Enrico and Osborne, Francesco and Recupero, Diego Reforgiato and Salatino, Angelo Antonio and Vahdati, Sahar},
 booktitle = {The Semantic Web},
 date = {2024},
 doi = {10.1007/978-3-031-60626-7_11},
 editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
 isbn = {978-3-031-60626-7},
 keywords = {Few-shot learning, Fine-tuning, Knowledge graphs, Language models., Question answering},
 langid = {english},
 location = {Cham},
 pages = {199--217},
 publisher = {Springer Nature Switzerland},
 shorttitle = {Large Language Models for Scientific Question Answering},
 title = {Large Language Models for Scientific Question Answering: An Extensive Analysis of the {SciQA} Benchmark}
}

@inproceedings{lehnert_conceptual_1977,
 abstract = {A theory of Q/A has been proposed from the perspective of natural language processing that relieson ideas in conceptual information processing and theories of human memory organization. This theory of Q/A has been implemented in a computer program, {QUALM}. {QUALM} is currently used by two story understanding systems ({SAM} and {PAM}) to complete a natural language processing system that reads stories and answers questions about what was read.},
 author = {Lehnert, Wendy G.},
 booktitle = {Proceedings of the 5th international joint conference on Artificial intelligence - Volume 1},
 date = {1977-08-22},
 location = {San Francisco, {CA}, {USA}},
 pages = {158--164},
 publisher = {Morgan Kaufmann Publishers Inc.},
 series = {{IJCAI}'77},
 title = {A conceptual theory of question answering},
 urldate = {2025-03-02}
}

@misc{leidner_question_2002,
 abstract = {Information needs are naturally represented as questions. Automatic Natural-Language Question Answering ({NLQA}) has only recently become a practical task on a larger scale and without domain constraints. This paper gives a brief introduction to the field, its history and the impact of systematic evaluation competitions. It is then demonstrated that an {NLQA} system for English can be built and evaluated in a very short time using off-the-shelf parsers and thesauri. The system is based on Robust Minimal Recursion Semantics ({RMRS}) and is portable with respect to the parser used as a frontend. It applies atomic term unification supported by question classification and {WordNet} lookup for semantic similarity matching of parsed question representation and free text.},
 author = {Leidner, Jochen L.},
 date = {2002-07-18},
 doi = {10.48550/arXiv.cs/0207058},
 eprint = {cs/0207058},
 eprinttype = {arxiv},
 keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
 number = {{arXiv}:cs/0207058},
 publisher = {{arXiv}},
 title = {Question Answering over Unstructured Data without Domain Restrictions},
 url = {http://arxiv.org/abs/cs/0207058},
 urldate = {2025-01-24}
}

@inproceedings{li_learning_2002,
 author = {Li, Xin and Roth, Dan},
 booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
 date = {2002},
 doi = {10.3115/1072228.107237},
 eventtitle = {{COLING} 2002},
 keywords = {finished, general, notion},
 title = {Learning Question Classifiers},
 url = {https://aclanthology.org/C02-1150},
 urldate = {2024-10-26}
}

@article{li_learning_2006,
 abstract = {To respond correctly to a free form factual question given a large collection of text data, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This work presents a machine learning approach to question classification. Guided by a layered semantic hierarchy of answer types, we develop a hierarchical classifier that classifies questions into fine-grained classes. This work also performs a systematic study of the use of semantic information sources in natural language classification tasks. It is shown that, in the context of question classification, augmenting the input of the classifier with appropriate semantic category information results in significant improvements to classification accuracy. We show accurate results on a large collection of free-form questions used in {TREC} 10 and 11.},
 author = {Li, Xin and Roth, Dan},
 date = {2006-09},
 doi = {10.1017/S1351324905003955},
 issn = {1469-8110, 1351-3249},
 journaltitle = {Natural Language Engineering},
 keywords = {no type information},
 langid = {english},
 number = {3},
 pages = {229--249},
 shorttitle = {Learning question classifiers},
 title = {Learning question classifiers: the role of semantic information},
 url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/learning-question-classifiers-the-role-of-semantic-information/F3E3EBFC2061BBF74A6EB926A3A3B291},
 urldate = {2025-01-26},
 volume = {12}
}

@misc{li_scigraphqa_2023,
 abstract = {In this work, we present {SciGraphQA}, a synthetic multi-turn question-answer dataset related to academic graphs. {SciGraphQA} is 13 times larger than {ChartVQA}, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart {VQA} dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning {ArXiv} papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. We asked {GPT}-4 to assess the matching quality of our question-answer turns given the paper's context, obtaining an average rating of 8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most popular {MLLM} models such as {LLaVa}, {mPLUGowl}, {BLIP}-2, and {openFlamingo}'s on our dataset, finding {LLaVA}-13B being the most performant with a {CIDEr} score of 0.08. We further enriched the question prompts for {LLAVA} by including the serialized data tables extracted from the graphs using the {DePlot} model, boosting {LLaVA}'s 0-shot {CIDEr} to 0.15. To verify the validity of our dataset, we also fine-tuned {LLaVa} using our dataset, reaching a substantially higher {CIDEr} score of 0.26. We anticipate further accuracy improvement by including segmentation mask tokens and leveraging larger {LLM} backbones coupled with emergent prompting techniques. Our code and data are open-sourced.},
 author = {Li, Shengzhi and Tajbakhsh, Nima},
 date = {2023-08-07},
 doi = {10.48550/arXiv.2308.03349},
 eprint = {2308.03349 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, no type information},
 number = {{arXiv}:2308.03349},
 publisher = {{arXiv}},
 shorttitle = {{SciGraphQA}},
 title = {{SciGraphQA}: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs},
 url = {http://arxiv.org/abs/2308.03349},
 urldate = {2025-01-25}
}

@misc{liang_scemqa_2024,
 abstract = {The paper introduces {SceMQA}, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. {SceMQA} focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of {AI} models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. {SceMQA} also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models ({MLLMs}), across various experimental settings. The results show that further research and development are needed in developing more capable {MLLM}, as highlighted by only 50\% to 60\% accuracy achieved by the strongest models. Our benchmark and analysis will be available at https://scemqa.github.io/},
 author = {Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang},
 date = {2024-02-06},
 doi = {10.48550/arXiv.2402.05138},
 eprint = {2402.05138 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, not related, notion},
 number = {{arXiv}:2402.05138},
 publisher = {{arXiv}},
 shorttitle = {{SceMQA}},
 title = {{SceMQA}: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
 url = {http://arxiv.org/abs/2402.05138},
 urldate = {2024-12-09}
}

@inproceedings{liu_taxonomy_2015,
 abstract = {The rapid advancement of Web2.0 technologies has made social networking sites, such as Facebook and twitter, important venues for individuals to seek and share information. As understanding the information needs of users is crucial for designing and developing tools to support their social Q\&amp;A behaviors, in this paper, we present a new way of classifying questions from a design perspective, with the aim of facilitating the development of question routing systems according to individual's information need. As an attempt to understand the questioner's intent in social question and answering environments, we propose a taxonomy of questions posted on Twitter, called {ASK}. Our taxonomy uncovers three different kinds of questions: accuracy, social, and knowledge. In addition, to enable automatic detection on these three types of information needs, we measured and reported on the differences in {ASK} types of questions reflected at both lexical and syntactic levels.},
 author = {Liu, Zhe and Jansen, Bernard J.},
 booktitle = {Proceedings of the 33rd Annual {ACM} Conference Extended Abstracts on Human Factors in Computing Systems},
 date = {2015-04-18},
 doi = {10.1145/2702613.2732928},
 isbn = {978-1-4503-3146-3},
 keywords = {finished},
 location = {New York, {NY}, {USA}},
 pages = {1947--1952},
 publisher = {Association for Computing Machinery},
 series = {{CHI} {EA} '15},
 title = {A Taxonomy for Classifying Questions Asked in Social Question and Answering},
 url = {https://dl.acm.org/doi/10.1145/2702613.2732928},
 urldate = {2025-01-25}
}

@article{lopez_evaluating_2013,
 abstract = {The availability of large amounts of open, distributed, and structured semantic data on the web has no precedent in the history of computer science. In recent years, there have been important advances in semantic search and question answering over {RDF} data. In particular, natural language interfaces to online semantic data have the advantage that they can exploit the expressive power of Semantic Web data models and query languages, while at the same time hiding their complexity from the user. However, despite the increasing interest in this area, there are no evaluations so far that systematically evaluate this kind of systems, in contrast to traditional question answering and search interfaces to document spaces. To address this gap, we have set up a series of evaluation challenges for question answering over linked data. The main goal of the challenge was to get insight into the strengths, capabilities, and current shortcomings of question answering systems as interfaces to query linked data sources, as well as benchmarking how these interaction paradigms can deal with the fact that the amount of {RDF} data available on the web is very large and heterogeneous with respect to the vocabularies and schemas used. Here, we report on the results from the first and second of such evaluation campaigns. We also discuss how the second evaluation addressed some of the issues and limitations which arose from the first one, as well as the open issues to be addressed in future competitions.},
 author = {Lopez, Vanessa and Unger, Christina and Cimiano, Philipp and Motta, Enrico},
 date = {2013-08-01},
 doi = {10.1016/j.websem.2013.05.006},
 issn = {1570-8268},
 journaltitle = {Journal of Web Semantics},
 keywords = {Evaluation, Linked data, Natural language, Question answering, Semantic Web},
 pages = {3--13},
 series = {Special Issue on Evaluation of Semantic Technologies},
 shortjournal = {Journal of Web Semantics},
 title = {Evaluating question answering over linked data},
 url = {https://www.sciencedirect.com/science/article/pii/S157082681300022X},
 urldate = {2025-01-26},
 volume = {21}
}

@inproceedings{mikhailian_learning_2009,
 author = {Mikhailian, Alexander and Dalmas, Tiphaine and Pinchuk, Rani},
 booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers},
 date = {2009-08},
 editor = {Su, Keh-Yih and Su, Jian and Wiebe, Janyce and Li, Haizhou},
 eventtitle = {{ACL}-{IJCNLP} 2009},
 keywords = {finished, general, notion},
 location = {Suntec, Singapore},
 pages = {325--328},
 publisher = {Association for Computational Linguistics},
 title = {Learning foci for Question Answering over Topic Maps},
 url = {https://aclanthology.org/P09-2082},
 urldate = {2024-10-27}
}

@article{mishra_survey_2016,
 abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
 author = {Mishra, Amit and Jain, Sanjay Kumar},
 date = {2016-07-01},
 doi = {10.1016/j.jksuci.2014.10.007},
 issn = {1319-1578},
 journaltitle = {Journal of King Saud University - Computer and Information Sciences},
 keywords = {Information retrieval, Natural language processing, Natural language understanding, no type information, notion, Question answering system, Search engine},
 number = {3},
 pages = {345--361},
 shortjournal = {Journal of King Saud University - Computer and Information Sciences},
 title = {A survey on question answering systems with classification},
 url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
 urldate = {2024-11-27},
 volume = {28}
}

@inproceedings{moldovan_structure_2000,
 author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
 booktitle = {Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics},
 date = {2000-10},
 doi = {10.3115/1075218.1075289},
 eventtitle = {{ACL} 2000},
 keywords = {finished, notion},
 location = {Hong Kong},
 pages = {563--570},
 publisher = {Association for Computational Linguistics},
 title = {The Structure and Performance of an Open-Domain Question Answering System},
 url = {https://aclanthology.org/P00-1071},
 urldate = {2024-10-27}
}

@article{molla_question_2007,
 abstract = {Automated question answering has been a topic of research and development since the earliest {AI} applications. Computing power has increased since the first such systems were developed, and the general methodology has changed from the use of hand-encoded knowledge bases about simple domains to the use of text collections as the main knowledge source over more complex domains. Still, many research issues remain. The focus of this article is on the use of restricted domains for automated question answering. The article contains a historical perspective on question answering over restricted domains and an overview of the current methods and applications used in restricted domains. A main characteristic of question answering in restricted domains is the integration of domain-specific information that is either developed for question answering or that has been developed for other purposes. We explore the main methods developed to leverage this domain-specific information.},
 author = {Mollá, Diego and Vicedo, José Luis},
 date = {2007-03-01},
 doi = {10.1162/coli.2007.33.1.41},
 issn = {0891-2017},
 journaltitle = {Computational Linguistics},
 keywords = {no type information},
 number = {1},
 pages = {41--61},
 shortjournal = {Computational Linguistics},
 shorttitle = {Question Answering in Restricted Domains},
 title = {Question Answering in Restricted Domains: An Overview},
 url = {https://doi.org/10.1162/coli.2007.33.1.41},
 urldate = {2025-01-25},
 volume = {33}
}

@inproceedings{navarro-almanza_towards_2017,
 abstract = {Software Requirements are the basis of high-quality software development process, each step is related to {SR}, these represent the needs and expectations of the software in a very detailed form. The software requirement classification ({SRC}) task requires a lot of human effort, specially when there are huge of requirements, therefore, the automation of {SRC} have been addressed using Natural Language Processing ({NLP}) and Information Retrieval ({IR}) techniques, however, generally requires human effort to analyze and create features from corpus (set of requirements). In this work, we propose to use Deep Learning ({DL}) to classify software requirements without labor intensive feature engineering. The model that we propose is based on Convolutional Neural Network ({CNN}) that has been state of art in other natural language related tasks. To evaluate our proposed model, {PROMISE} corpus was used, contains a set of labeled requirements in functional and 11 different categories of non-functional requirements. We achieve promising results on {SRC} using {CNN} even without handcrafted features.},
 author = {Navarro-Almanza, Raul and Juarez-Ramirez, Reyes and Licea, Guillermo},
 booktitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
 date = {2017-10},
 doi = {10.1109/CONISOFT.2017.00021},
 eventtitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
 keywords = {Computer architecture, Convolutional Neural Network, Convolutional neural networks, Data models, Machine learning, no type information, Software, Software engineering, Software Engineering, Software Requirement Classification, Task analysis, Word Embedding},
 pages = {116--120},
 shorttitle = {Towards Supporting Software Engineering Using Deep Learning},
 title = {Towards Supporting Software Engineering Using Deep Learning: A Case of Software Requirements Classification},
 url = {https://ieeexplore.ieee.org/abstract/document/8337942},
 urldate = {2025-01-25}
}

@inproceedings{nguyen_semantic_2024,
 abstract = {This paper presents a study to answer the question of how to map a natural language ({NL}) sentence to a semantic representation and its application to question answering over the {DBLP} database. We investigate the deep learning approach using pre-trained models and their fine-tuning on training data for semantic parsing tasks. Experimental results on standard datasets show the effectiveness of pre-trained models in mapping an {NL} sentence to {SPARQL}, a query language for semantic databases. The results also show that the T5 and Flan-T5 models outperform other models in terms of translation accuracy. In addition to the empirical results on pre-trained models, we also consider the problem of examining large language models ({LLMs}) such as Llama and Mistras, or Qwen models for answering questions on the {DBLP} database. Experimental results showed the potentiality of using {LLMs} with chain-of-thought prompting methods. The results indicated that without using training data, we were able to obtain promising results for some types of questions when translating them to {SPARQL}.},
 author = {Nguyen, Le-Minh and Khang, Le-Nguyen and Anh, Kieu Que and Hien, Nguyen Dieu and Nagai, Yukari},
 booktitle = {New Frontiers in Artificial Intelligence},
 date = {2024},
 doi = {10.1007/978-981-97-3076-6_20},
 editor = {Suzumura, Toyotaro and Bono, Mayumi},
 isbn = {978-981-97-3076-6},
 keywords = {Knowledge Graph, Mapping {NL} to {SPARQL}, no type information, Question Answering, Relation search, Semantic parsing, Semantic Representation},
 langid = {english},
 location = {Singapore},
 pages = {284--298},
 publisher = {Springer Nature},
 title = {Semantic Parsing for Question and Answering over Scholarly Knowledge Graph with Large Language Models}
}

@misc{pan_rag_2024,
 abstract = {Competency question ({CQ}) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models ({LLMs}), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to {LLMs}, we present a retrieval-augmented generation ({RAG}) approach that uses {LLMs} for the automatic generation of {CQs} given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the {RAG} and different temperature setting of the {LLM}. We conduct experiments using {GPT}-4 on two domain ontology engineering tasks and compare results against ground-truth {CQs} constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the {RAG} improves the performance of {LLMs} on generating {CQs} for concrete ontology engineering tasks.},
 author = {Pan, Xueli and Ossenbruggen, Jacco van and Boer, Victor de and Huang, Zhisheng},
 date = {2024-09-13},
 doi = {10.48550/arXiv.2409.08820},
 eprint = {2409.08820 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, no type information},
 number = {{arXiv}:2409.08820},
 publisher = {{arXiv}},
 title = {A {RAG} Approach for Generating Competency Questions in Ontology Engineering},
 url = {http://arxiv.org/abs/2409.08820},
 urldate = {2025-01-24}
}

@article{ratan_formulation_2019,
 abstract = {Formulation of research question ({RQ}) is an essentiality before starting any research. It aims to explore an existing uncertainty in an area of concern and points to a need for deliberate investigation. It is, therefore, pertinent to formulate a good {RQ}. The present paper aims to discuss the process of formulation of {RQ} with stepwise approach. The characteristics of good {RQ} are expressed by acronym “{FINERMAPS}” expanded as feasible, interesting, novel, ethical, relevant, manageable, appropriate, potential value, publishability, and systematic. A {RQ} can address different formats depending on the aspect to be evaluated. Based on this, there can be different types of {RQ} such as based on the existence of the phenomenon, description and classification, composition, relationship, comparative, and causality. To develop a {RQ}, one needs to begin by identifying the subject of interest and then do preliminary research on that subject. The researcher then defines what still needs to be known in that particular subject and assesses the implied questions. After narrowing the focus and scope of the research subject, researcher frames a {RQ} and then evaluates it. Thus, conception to formulation of {RQ} is very systematic process and has to be performed meticulously as research guided by such question can have wider impact in the field of social and health research by leading to formulation of policies for the benefit of larger population.},
 author = {Ratan, {SimmiK} and Anand, Tanu and Ratan, John},
 date = {2019-01-01},
 doi = {10.4103/jiaps.JIAPS_76_18},
 journaltitle = {Journal of Indian Association of Pediatric Surgeons},
 keywords = {finished, notion},
 pages = {15},
 shortjournal = {Journal of Indian Association of Pediatric Surgeons},
 title = {Formulation of Research Question – Stepwise Approach},
 volume = {24}
}

@inproceedings{riloff_rule-based_2000,
 author = {Riloff, Ellen and Thelen, Michael},
 booktitle = {{ANLP}-{NAACL} 2000 Workshop: Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems},
 date = {2000},
 doi = {10.3115/1117595.1117598},
 keywords = {finished, general, notion},
 title = {A Rule-based Question Answering System for Reading Comprehension Tests},
 url = {https://aclanthology.org/W00-0603},
 urldate = {2024-10-27}
}

@article{saikh_scienceqa_2022,
 abstract = {Machine Reading Comprehension ({MRC}) of a document is a challenging problem that requires discourse-level understanding. Information extraction from scholarly articles nowadays is a critical use case for researchers to understand the underlying research quickly and move forward, especially in this age of infodemic. {MRC} on research articles can also provide helpful information to the reviewers and editors. However, the main bottleneck in building such models is the availability of human-annotated data. In this paper, firstly, we introduce a dataset to facilitate question answering ({QA}) on scientific articles. We prepare the dataset in a semi-automated fashion having more than 100k human-annotated context–question–answer triples. Secondly, we implement one baseline {QA} model based on Bidirectional Encoder Representations from Transformers ({BERT}). Additionally, we implement two models: the first one is based on Science {BERT} ({SciBERT}), and the second is the combination of {SciBERT} and Bi-Directional Attention Flow (Bi-{DAF}). The best model (i.e., {SciBERT}) obtains an F1 score of 75.46\%. Our dataset is novel, and our work opens up a new avenue for scholarly document processing research by providing a benchmark {QA} dataset and standard baseline. We make our dataset and codes available here at https://github.com/{TanikSaikh}/Scientific-Question-Answering.},
 author = {Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal, Asif and Bhattacharyya, Pushpak},
 date = {2022-09-01},
 doi = {10.1007/s00799-022-00329-y},
 issn = {1432-1300},
 journaltitle = {International Journal on Digital Libraries},
 keywords = {no type information, notion},
 langid = {english},
 number = {3},
 pages = {289--301},
 shortjournal = {Int J Digit Libr},
 shorttitle = {{ScienceQA}},
 title = {{ScienceQA}: a novel resource for question answering on scholarly articles},
 url = {https://doi.org/10.1007/s00799-022-00329-y},
 urldate = {2024-09-24},
 volume = {23}
}

@misc{serban_generating_2016,
 abstract = {Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.},
 author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
 date = {2016-05-29},
 doi = {10.48550/arXiv.1603.06807},
 eprint = {1603.06807 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
 number = {{arXiv}:1603.06807},
 publisher = {{arXiv}},
 shorttitle = {Generating Factoid Questions With Recurrent Neural Networks},
 title = {Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus},
 url = {http://arxiv.org/abs/1603.06807},
 urldate = {2025-01-25}
}

@inproceedings{shaw_writing_2003,
 abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to {XSE} 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
 author = {Shaw, M.},
 booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
 date = {2003-05},
 doi = {10.1109/ICSE.2003.1201262},
 eventtitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
 keywords = {domain\_specific, finished, notion},
 note = {{ISSN}: 0270-5257},
 pages = {726--736},
 title = {Writing good software engineering research papers},
 url = {https://ieeexplore.ieee.org/document/1201262},
 urldate = {2024-10-02}
}

@inproceedings{singhal_att_1999,
 abstract = {In 1999, {AT}\&T participated in the ad-hoc task and the Question Answering {QA}, Spoken Document Retrieval {SDR}, and Web tracks. Most of our eeort for {TREC}-8 focused on the {QA} and {SDR} tracks. Results from {SDR} track show that our document expansion techniques, presented in 99, are very eeective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can the correct answer for about 45 of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.},
 author = {Singhal, Amit and Abney, Steve and Bacchiani, Michiel and Collins, Michael and Hindle, Donald and Pereira, Fernando},
 date = {1999-11-01},
 keywords = {finished, general, notion},
 title = {{AT}\&T at {TREC}-8}
}

@inproceedings{sjoberg_future_2007,
 abstract = {We present the vision that for all fields of software engineering ({SE}), empirical research methods should enable the development of scientific knowledge about how useful different {SE} technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new {SE} technology and is a major input to important {SE} decisions in industry. Major challenges to the pursuit of this vision are: more {SE} research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
 author = {Sjoberg, Dag I. K. and Dyba, Tore and Jorgensen, Magne},
 booktitle = {Future of Software Engineering ({FOSE} '07)},
 date = {2007-05},
 doi = {10.1109/FOSE.2007.30},
 eventtitle = {Future of Software Engineering ({FOSE} '07)},
 keywords = {domain\_specific, finished, notion},
 pages = {358--378},
 title = {The Future of Empirical Methods in Software Engineering Research},
 url = {https://ieeexplore.ieee.org/document/4221632},
 urldate = {2024-11-10}
}

@inproceedings{soleimani_nlquad_2021,
 abstract = {We introduce {NLQuAD}, the first data set with baseline methods for non-factoid long question answering, a task requiring document-level language understanding. In contrast to existing span detection question answering data sets, {NLQuAD} has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ({IoU}), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare {BERT}, {RoBERTa}, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. {NLQuAD}`s samples exceed the input limitation of most pre-trained Transformer-based models, encouraging future research on long sequence language models.},
 author = {Soleimani, Amir and Monz, Christof and Worring, Marcel},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 date = {2021-04},
 doi = {10.18653/v1/2021.eacl-main.106},
 editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
 eventtitle = {{EACL} 2021},
 location = {Online},
 pages = {1245--1255},
 publisher = {Association for Computational Linguistics},
 shorttitle = {{NLQuAD}},
 title = {{NLQuAD}: A Non-Factoid Long Question Answering Data Set},
 url = {https://aclanthology.org/2021.eacl-main.106/},
 urldate = {2025-01-25}
}

@article{steinmetz_what_2021,
 abstract = {Question Answering based on Knowledge Graphs ({KGQA}) still faces difficult challenges when transforming natural language ({NL}) to {SPARQL} queries. Simple questions only referring to one triple are answerable by most {QA} systems, but more complex questions requiring complex queries containing subqueries or several functions are still a tough challenge within this field of research. Evaluation results of {QA} systems therefore also might depend on the benchmark dataset the system has been tested on. For the purpose to give an overview and reveal specific characteristics, we examined currently available {KGQA} datasets regarding several challenging aspects. This paper presents a detailed look into the datasets and compares them in terms of challenges a {KGQA} system is facing.},
 author = {Steinmetz, Nadine and Sattler, Kai-Uwe},
 date = {2021-12-01},
 doi = {10.1007/s13740-021-00128-9},
 issn = {1861-2040},
 journaltitle = {Journal on Data Semantics},
 keywords = {Artificial Intelligence, Dataset analysis, finished, Natural language transformation, Pattern recognition, Question answering on knowledge graphs},
 langid = {english},
 number = {3},
 pages = {241--265},
 shortjournal = {J Data Semant},
 shorttitle = {What is in the {KGQA} Benchmark Datasets?},
 title = {What is in the {KGQA} Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs},
 url = {https://doi.org/10.1007/s13740-021-00128-9},
 urldate = {2025-01-25},
 volume = {10}
}

@article{stol_abc_2018,
 abstract = {A variety of research methods and techniques are available to {SE} researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the {ABC} framework for {SE} research, which offers a holistic view of eight archetypal research strategies. {ABC} refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The {ABC} framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key {SE} domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance {SE} research.},
 author = {Stol, Klaas-Jan and Fitzgerald, Brian},
 date = {2018-09-17},
 doi = {10.1145/3241743},
 issn = {1049-331X},
 journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
 keywords = {no type information},
 number = {3},
 pages = {11:1--11:51},
 title = {The {ABC} of Software Engineering Research},
 url = {https://dl.acm.org/doi/10.1145/3241743},
 urldate = {2025-01-25},
 volume = {27}
}

@inproceedings{suzuki_question_2003,
 author = {Suzuki, Jun and Taira, Hirotoshi and Sasaki, Yutaka and Maeda, Eisaku},
 booktitle = {Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering},
 date = {2003-07},
 doi = {10.3115/1119312.1119320},
 eventtitle = {{MultiLing} 2003},
 location = {Sapporo, Japan},
 pages = {61--68},
 publisher = {Association for Computational Linguistics},
 title = {Question Classification using {HDAG} Kernel},
 url = {https://aclanthology.org/W03-1208/},
 urldate = {2025-01-25}
}

@misc{taffa_hybrid-squad_2024,
 abstract = {Existing Scholarly Question Answering ({QA}) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs ({KGs}). However, scholarly information often spans heterogeneous sources, necessitating the development of {QA} systems that integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-{SQuAD} (Hybrid Scholarly Question Answering Dataset), a novel large-scale {QA} dataset designed to facilitate answering questions incorporating both text and {KG} facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the {KGs} {DBLP} and {SemOpenAlex} alongside corresponding text from Wikipedia. In addition, we propose a {RAG}-based baseline hybrid {QA} model, achieving an exact match score of 69.65 on the Hybrid-{SQuAD} test set.},
 author = {Taffa, Tilahun Abedissa and Banerjee, Debayan and Assabie, Yaregal and Usbeck, Ricardo},
 date = {2024-12-05},
 doi = {10.48550/arXiv.2412.02788},
 eprint = {2412.02788 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
 number = {{arXiv}:2412.02788},
 publisher = {{arXiv}},
 shorttitle = {Hybrid-{SQuAD}},
 title = {Hybrid-{SQuAD}: Hybrid Scholarly Question Answering Dataset},
 url = {http://arxiv.org/abs/2412.02788},
 urldate = {2025-01-18}
}

@article{taffa_leveraging_2023,
 abstract = {This paper presents a scholarly Knowledge Graph Question Answering ({KGQA}) that answers bibliographic natural language questions by leveraging a large language model ({LLM}) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a {BERT}-based sentence encoder and retrieves their corresponding {SPARQL}. Using the top-n similar question-{SPARQL} pairs as an example and the test question creates a prompt. Then pass the prompt to the {LLM} and generate a {SPARQL}. Finally, runs the {SPARQL} against the underlying {KG} - {ORKG} (Open Research {KG}) endpoint and returns an answer. Our system achieves an F1 score of 99.0\%, on {SciQA} - one of the Scholarly-{QALD}-23 challenge benchmarks.},
 author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
 date = {2023},
 doi = {10.48550/ARXIV.2311.09841},
 keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Databases (cs.{DB}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), no type information},
 note = {Publisher: {arXiv}
Version Number: 1},
 rights = {Creative Commons Attribution 4.0 International},
 title = {Leveraging {LLMs} in Scholarly Knowledge Graph Question Answering},
 url = {https://arxiv.org/abs/2311.09841},
 urldate = {2025-01-12}
}

@inproceedings{theisen_writing_2017,
 abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented "Writing Good Software Engineering Research Papers" in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering ({ICSE}) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to {ICSE} 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository ({MSR}) papers, a category of papers not seen in 2002. The advent of {MSR} papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
 author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
 booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
 date = {2017-05},
 doi = {10.1109/ICSE-C.2017.51},
 eventtitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
 keywords = {abstracts, Computer science, Conferences, Data mining, guidelines, Market research, no type information, research, Software, Software engineering, writing, Writing},
 pages = {402--402},
 shorttitle = {Writing Good Software Engineering Research Papers},
 title = {Writing Good Software Engineering Research Papers: Revisited},
 url = {https://ieeexplore.ieee.org/abstract/document/7965369},
 urldate = {2025-01-25}
}

@article{thuan_construction_2019,
 author = {Thuan, Nguyen and Drechsler, Andreas and Antunes, Pedro},
 date = {2019-03-01},
 doi = {10.17705/1CAIS.04420},
 issn = {1529-3181},
 journaltitle = {Communications of the Association for Information Systems},
 keywords = {finished, notion},
 number = {1},
 title = {Construction of Design Science Research Questions},
 url = {https://aisel.aisnet.org/cais/vol44/iss1/20},
 volume = {44}
}

@misc{tran_comparative_2022,
 abstract = {Question answering over knowledge bases ({KBQA}) has become a popular approach to help users extract information from knowledge bases. Although several systems exist, choosing one suitable for a particular application scenario is difficult. In this article, we provide a comparative study of six representative {KBQA} systems on eight benchmark datasets. In that, we study various question types, properties, languages, and domains to provide insights on where existing systems struggle. On top of that, we propose an advanced mapping algorithm to aid existing models in achieving superior results. Moreover, we also develop a multilingual corpus {COVID}-{KGQA}, which encourages {COVID}-19 research and multilingualism for the diversity of future {AI}. Finally, we discuss the key findings and their implications as well as performance guidelines and some future improvements. Our source code is available at {\textbackslash}url\{https://github.com/tamlhp/kbqa\}.},
 author = {Tran, Khiem Vinh and Phan, Hao Phu and Quach, Khang Nguyen Duc and Nguyen, Ngan Luu-Thuy and Jo, Jun and Nguyen, Thanh Tam},
 date = {2022-11-15},
 doi = {10.48550/arXiv.2211.08170},
 eprint = {2211.08170},
 eprinttype = {arxiv},
 keywords = {finished, notion},
 number = {{arXiv}:2211.08170},
 publisher = {{arXiv}},
 title = {A Comparative Study of Question Answering over Knowledge Bases},
 url = {http://arxiv.org/abs/2211.08170},
 urldate = {2024-10-26}
}

@inproceedings{trivedi_lc-quad_2017,
 abstract = {Being able to access knowledge bases in an intuitive way has been an active area of research over the past years. In particular, several question answering ({QA}) approaches which allow to query {RDF} datasets in natural language have been developed as they allow end users to access knowledge without needing to learn the schema of a knowledge base and learn a formal query language. To foster this research area, several training datasets have been created, e.g. in the {QALD} (Question Answering over Linked Data) initiative. However, existing datasets are insufficient in terms of size, variety or complexity to apply and evaluate a range of machine learning based {QA} approaches for learning complex {SPARQL} queries. With the provision of the Large-Scale Complex Question Answering Dataset ({LC}-{QuAD}), we close this gap by providing a dataset with 5000 questions and their corresponding {SPARQL} queries over the {DBpedia} dataset. In this article, we describe the dataset creation process and how we ensure a high variety of questions, which should enable to assess the robustness and accuracy of the next generation of {QA} systems for knowledge graphs.},
 author = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
 booktitle = {The Semantic Web – {ISWC} 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part {II}},
 date = {2017-10-21},
 doi = {10.1007/978-3-319-68204-4_22},
 isbn = {978-3-319-68203-7},
 keywords = {no type information, notion},
 location = {Berlin, Heidelberg},
 pages = {210--218},
 publisher = {Springer-Verlag},
 shorttitle = {{LC}-{QuAD}},
 title = {{LC}-{QuAD}: A Corpus for Complex Question Answering over Knowledge Graphs},
 url = {https://doi.org/10.1007/978-3-319-68204-4_22},
 urldate = {2024-09-23}
}

@inproceedings{unger_template-based_2012,
 abstract = {As an increasing amount of {RDF} data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the {RDF} data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a {SPARQL} template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.},
 author = {Unger, Christina and Bühmann, Lorenz and Lehmann, Jens and Ngonga Ngomo, Axel-Cyrille and Gerber, Daniel and Cimiano, Philipp},
 booktitle = {Proceedings of the 21st international conference on World Wide Web},
 date = {2012-04-16},
 doi = {10.1145/2187836.2187923},
 isbn = {978-1-4503-1229-5},
 location = {New York, {NY}, {USA}},
 pages = {639--648},
 publisher = {Association for Computing Machinery},
 series = {{WWW} '12},
 title = {Template-based question answering over {RDF} data},
 url = {https://dl.acm.org/doi/10.1145/2187836.2187923},
 urldate = {2025-01-25}
}

@inproceedings{usbeck_9th_2018,
 abstract = {Recent years have seen a growing amount of research on question answering ({QA}) over Semantic Web data, shaping an interaction paradigm that allows end users to profit from the expressive power of Semantic Web standards. At the same time, {QA} systems hide their complexity behind an intuitive and easy-touse interface. However, the growing amount of data available on the Semantic Web has led to a heterogeneous data landscape where {QA} systems struggle to keep up with the volume, variety and veracity of the underlying knowledge. The Question Answering over Linked Data ({QALD}) challenges aim to provide up-to-date benchmarks for assessing and comparing state-of-the-art systems that mediate between a user, expressing his or her information need in natural language, and {RDF} data. In the past few years, more than 40 research groups and their systems have taken part in the last nine {QALD} challenges. The {QALD} challenge targets all researchers and practitioners working on querying Linked Data, natural language processing for question answering, multilingual information retrieval and related topics. The main goal is to gain insights into the strengths and shortcomings of different approaches and into possible solutions for coping with the large, heterogeneous and distributed nature of Semantic Web data. {QALD} has a 8-year history. The challenge began in 2011 and is developing benchmarks that are increasingly being used as a standard evaluation venue for question answering over Linked Data. Overviews of past instantiations of the challenge are available from the {CLEF} Working Notes, {CEUR} workshop notes as well as {ESWC} proceedings, see Table 1. This article will give a technical overview of the task and results of the 9th Question Answering over Linked Data challenge.},
 author = {Usbeck, Ricardo and Gusmita, Ria Hari and Ngomo, A. and Saleem, Muhammad},
 date = {2018},
 eventtitle = {Semdeep/{NLIWoD}@{ISWC}},
 title = {9th Challenge on Question Answering over Linked Data ({QALD}-9) (invited paper)},
 url = {https://www.semanticscholar.org/paper/9th-Challenge-on-Question-Answering-over-Linked-Usbeck-Gusmita/4f83e1b64f57ae0d546076279426e85c0e60298b},
 urldate = {2025-01-25}
}

@article{usbeck_qald-10_2023,
 abstract = {Knowledge Graph Question Answering ({KGQA}) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field f},
 author = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
 date = {2023-01-01},
 doi = {10.3233/SW-233471},
 issn = {1570-0844},
 issue = {Preprint},
 journaltitle = {Semantic Web},
 keywords = {finished},
 langid = {english},
 note = {Publisher: {IOS} Press},
 pages = {1--15},
 title = {{QALD}-10 – The 10th challenge on question answering over linked data},
 url = {https://content.iospress.com/articles/semantic-web/sw233471},
 urldate = {2025-01-24},
 volume = {Preprint}
}

@misc{wang_modern_2022,
 abstract = {Question Answering ({QA}) is one of the most important natural language processing ({NLP}) tasks. It aims using {NLP} technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging {QA} datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential {QA} datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common {QA} tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of {QA} research.},
 author = {Wang, Zhen},
 date = {2022-06-30},
 doi = {10.48550/arXiv.2206.15030},
 eprint = {2206.15030 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Computation and Language, not related, notion},
 number = {{arXiv}:2206.15030},
 publisher = {{arXiv}},
 shorttitle = {Modern Question Answering Datasets and Benchmarks},
 title = {Modern Question Answering Datasets and Benchmarks: A Survey},
 url = {http://arxiv.org/abs/2206.15030},
 urldate = {2024-12-09}
}

@inproceedings{yih_value_2016,
 abstract = {We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering.},
 author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
 booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 date = {2016},
 doi = {10.18653/v1/P16-2033},
 eventtitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
 langid = {english},
 location = {Berlin, Germany},
 pages = {201--206},
 publisher = {Association for Computational Linguistics},
 title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
 url = {http://aclweb.org/anthology/P16-2033},
 urldate = {2025-01-26}
}

@inproceedings{zafar_formal_2018,
 abstract = {Question answering ({QA}) systems often consist of several components such as Named Entity Disambiguation ({NED}), Relation Extraction ({RE}), and Query Generation ({QG}). In this paper, we focus on the {QG} process of a {QA} pipeline on a large-scale Knowledge Base ({KB}), with noisy annotations and complex sentence structures. We therefore propose {SQG}, a {SPARQL} Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional {QA} pipeline. {SQG} can be used on large open-domain {KBs} and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the {NED} and {RE} components. This ability allows {SQG} to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the {QG} component. The captured subgraph covers multiple candidate walks, which correspond to {SPARQL} queries. To enhance the accuracy, we present a ranking model based on Tree-{LSTM} that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. {SQG} outperforms the baseline systems and achieves a macro F1-measure of 75\% on the {LC}-{QuAD} dataset.},
 author = {Zafar, Hamid and Napolitano, Giulio and Lehmann, Jens},
 booktitle = {The Semantic Web},
 date = {2018},
 doi = {10.1007/978-3-319-93417-4_46},
 editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Raphaël and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
 isbn = {978-3-319-93417-4},
 langid = {english},
 location = {Cham},
 pages = {714--728},
 publisher = {Springer International Publishing},
 title = {Formal Query Generation for Question Answering over Knowledge Bases}
}

@inproceedings{zhang_question_2003,
 abstract = {Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors ({NN}), Naive Bayes ({NB}), Decision Tree ({DT}), Sparse Network of Winnows ({SNoW}), and Support Vector Machines ({SVM}) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the {SVM} outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the {SVM} to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the {TREC} {QA} track.},
 author = {Zhang, Dell and Lee, Wee Sun},
 booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
 date = {2003-07-28},
 doi = {10.1145/860435.860443},
 isbn = {978-1-58113-646-3},
 location = {New York, {NY}, {USA}},
 pages = {26--32},
 publisher = {Association for Computing Machinery},
 series = {{SIGIR} '03},
 title = {Question classification using support vector machines},
 url = {https://dl.acm.org/doi/10.1145/860435.860443},
 urldate = {2025-01-25}
}

@article{zhao_ontology_2009,
 abstract = {The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.},
 author = {Zhao, Yajing and Dong, Jing and Peng, Tu},
 date = {2009-10},
 doi = {10.1109/TSC.2009.20},
 issn = {1939-1374},
 journaltitle = {{IEEE} Transactions on Services Computing},
 number = {4},
 pages = {303--317},
 rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
 shortjournal = {{IEEE} Trans. Serv. Comput.},
 title = {Ontology Classification for Semantic-Web-Based Software Engineering},
 url = {http://ieeexplore.ieee.org/document/5161251/},
 urldate = {2025-01-25},
 volume = {2}
}

@misc{zhu_retrieving_2021,
 abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
 author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
 date = {2021-05-08},
 doi = {10.48550/arXiv.2101.00774},
 eprint = {2101.00774 [cs]},
 eprinttype = {arxiv},
 keywords = {Computer Science - Artificial Intelligence},
 number = {{arXiv}:2101.00774},
 publisher = {{arXiv}},
 shorttitle = {Retrieving and Reading},
 title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
 url = {http://arxiv.org/abs/2101.00774},
 urldate = {2025-01-25}
}
