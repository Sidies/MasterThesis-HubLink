
@misc{banerjee_dblp-quad_2023,
	title = {{DBLP}-{QuAD}: A Question Answering Dataset over the {DBLP} Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2303.13351},
	shorttitle = {{DBLP}-{QuAD}},
	abstract = {In this work we create a question answering dataset over the {DBLP} scholarly knowledge graph ({KG}). {DBLP} is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding {SPARQL} queries which can be executed over the {DBLP} {KG} to fetch the correct answer. {DBLP}-{QuAD} is the largest scholarly question answering dataset.},
	number = {{arXiv}:2303.13351},
	publisher = {{arXiv}},
	author = {Banerjee, Debayan and Awale, Sushil and Usbeck, Ricardo and Biemann, Chris},
	urldate = {2024-07-09},
	date = {2023-03-29},
	langid = {english},
	keywords = {finished, notion, related\_dataset},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\5CNDQ4X4\\Banerjee et al. - 2023 - DBLP-QuAD A Question Answering Dataset over the DBLP Scholarly Knowledge Graph.pdf:application/pdf},
}

@inproceedings{trivedi_lc-quad_2017,
	location = {Berlin, Heidelberg},
	title = {{LC}-{QuAD}: A Corpus for Complex Question Answering over Knowledge Graphs},
	isbn = {978-3-319-68203-7},
	url = {https://doi.org/10.1007/978-3-319-68204-4_22},
	doi = {10.1007/978-3-319-68204-4_22},
	shorttitle = {{LC}-{QuAD}},
	abstract = {Being able to access knowledge bases in an intuitive way has been an active area of research over the past years. In particular, several question answering ({QA}) approaches which allow to query {RDF} datasets in natural language have been developed as they allow end users to access knowledge without needing to learn the schema of a knowledge base and learn a formal query language. To foster this research area, several training datasets have been created, e.g. in the {QALD} (Question Answering over Linked Data) initiative. However, existing datasets are insufficient in terms of size, variety or complexity to apply and evaluate a range of machine learning based {QA} approaches for learning complex {SPARQL} queries. With the provision of the Large-Scale Complex Question Answering Dataset ({LC}-{QuAD}), we close this gap by providing a dataset with 5000 questions and their corresponding {SPARQL} queries over the {DBpedia} dataset. In this article, we describe the dataset creation process and how we ensure a high variety of questions, which should enable to assess the robustness and accuracy of the next generation of {QA} systems for knowledge graphs.},
	pages = {210--218},
	booktitle = {The Semantic Web – {ISWC} 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part {II}},
	publisher = {Springer-Verlag},
	author = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
	urldate = {2024-09-23},
	date = {2017-10-21},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\D9YEIAEG\\Trivedi et al. - 2017 - LC-QuAD A Corpus for Complex Question Answering over Knowledge Graphs.pdf:application/pdf},
}

@inproceedings{dubey_lc-quad_2019,
	location = {Cham},
	title = {{LC}-{QuAD} 2.0: A Large Dataset for Complex Question Answering over Wikidata and {DBpedia}},
	isbn = {978-3-030-30796-7},
	doi = {10.1007/978-3-030-30796-7_5},
	shorttitle = {{LC}-{QuAD} 2.0},
	abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like {WebQuestions}, {QALD} and {LCQuAD} have been published in the past. The biggest data set available for complex questions ({LCQuAD}) over knowledge graphs contains five thousand questions. We now provide {LC}-{QuAD} 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding {SPARQL} queries. {LC}-{QuAD} 2.0 is compatible with both Wikidata and {DBpedia} 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
	pages = {69--78},
	booktitle = {The Semantic Web – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	date = {2019},
	langid = {english},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ARFHIF8U\\Dubey et al. - 2019 - LC-QuAD 2.0 A Large Dataset for Complex Question Answering over Wikidata and DBpedia.pdf:application/pdf},
}

@misc{bordes_large-scale_2015,
	title = {Large-scale Simple Question Answering with Memory Networks},
	url = {http://arxiv.org/abs/1506.02075},
	abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
	number = {{arXiv}:1506.02075},
	publisher = {{arXiv}},
	author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
	urldate = {2024-09-23},
	date = {2015-06-05},
	eprinttype = {arxiv},
	eprint = {1506.02075 [cs]},
	keywords = {finished, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\KXD2CVA2\\Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NCZKK6ME\\1506.html:text/html},
}

@misc{tran_comparative_2022,
	title = {A Comparative Study of Question Answering over Knowledge Bases},
	url = {http://arxiv.org/abs/2211.08170},
	doi = {10.48550/arXiv.2211.08170},
	abstract = {Question answering over knowledge bases ({KBQA}) has become a popular approach to help users extract information from knowledge bases. Although several systems exist, choosing one suitable for a particular application scenario is difficult. In this article, we provide a comparative study of six representative {KBQA} systems on eight benchmark datasets. In that, we study various question types, properties, languages, and domains to provide insights on where existing systems struggle. On top of that, we propose an advanced mapping algorithm to aid existing models in achieving superior results. Moreover, we also develop a multilingual corpus {COVID}-{KGQA}, which encourages {COVID}-19 research and multilingualism for the diversity of future {AI}. Finally, we discuss the key findings and their implications as well as performance guidelines and some future improvements. Our source code is available at {\textbackslash}url\{https://github.com/tamlhp/kbqa\}.},
	number = {{arXiv}:2211.08170},
	publisher = {{arXiv}},
	author = {Tran, Khiem Vinh and Phan, Hao Phu and Quach, Khang Nguyen Duc and Nguyen, Ngan Luu-Thuy and Jo, Jun and Nguyen, Thanh Tam},
	urldate = {2024-10-26},
	date = {2022-11-15},
	eprinttype = {arxiv},
	eprint = {2211.08170},
	keywords = {finished, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GYCPXHQW\\Tran et al. - 2022 - A Comparative Study of Question Answering over Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VG9KIM26\\2211.html:text/html},
}

@inproceedings{riloff_rule-based_2000,
	title = {A Rule-based Question Answering System for Reading Comprehension Tests},
	url = {https://aclanthology.org/W00-0603},
	doi = {10.3115/1117595.1117598},
	booktitle = {{ANLP}-{NAACL} 2000 Workshop: Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems},
	author = {Riloff, Ellen and Thelen, Michael},
	urldate = {2024-10-27},
	date = {2000},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5HB3ATU\\Riloff and Thelen - 2000 - A Rule-based Question Answering System for Reading Comprehension Tests.pdf:application/pdf},
}

@inproceedings{moldovan_structure_2000,
	location = {Hong Kong},
	title = {The Structure and Performance of an Open-Domain Question Answering System},
	url = {https://aclanthology.org/P00-1071},
	doi = {10.3115/1075218.1075289},
	eventtitle = {{ACL} 2000},
	pages = {563--570},
	booktitle = {Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
	urldate = {2024-10-27},
	date = {2000-10},
	keywords = {finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\I9HD6KI6\\Moldovan et al. - 2000 - The Structure and Performance of an Open-Domain Question Answering System.pdf:application/pdf},
}

@inproceedings{mikhailian_learning_2009,
	location = {Suntec, Singapore},
	title = {Learning foci for Question Answering over Topic Maps},
	url = {https://aclanthology.org/P09-2082},
	eventtitle = {{ACL}-{IJCNLP} 2009},
	pages = {325--328},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Mikhailian, Alexander and Dalmas, Tiphaine and Pinchuk, Rani},
	editor = {Su, Keh-Yih and Su, Jian and Wiebe, Janyce and Li, Haizhou},
	urldate = {2024-10-27},
	date = {2009-08},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XDL4SXLQ\\Mikhailian et al. - 2009 - Learning foci for Question Answering over Topic Maps.pdf:application/pdf},
}

@article{thuan_construction_2019,
	title = {Construction of Design Science Research Questions},
	volume = {44},
	issn = {1529-3181},
	url = {https://aisel.aisnet.org/cais/vol44/iss1/20},
	doi = {10.17705/1CAIS.04420},
	number = {1},
	journaltitle = {Communications of the Association for Information Systems},
	author = {Thuan, Nguyen and Drechsler, Andreas and Antunes, Pedro},
	date = {2019-03-01},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\W3667CHP\\Thuan et al. - 2019 - Construction of Design Science Research Questions.pdf:application/pdf},
}

@article{dillon_classification_1984,
	title = {The Classification of Research Questions},
	volume = {54},
	issn = {0034-6543},
	url = {https://doi.org/10.3102/00346543054003327},
	doi = {10.3102/00346543054003327},
	abstract = {What are the kinds of questions that may be posed for research? A dozen schemes proposing to classify research questions are surveyed, analyzed, and applied to the understanding and practice of inquiry. The extent to which the various schemes account for questions found in educational journals is estimated. Some principles and issues are identified to stimulate work on the classification of research questions in education and other enterprises of inquiry. On the whole, little is known about the kinds of questions that may be posed for research.},
	pages = {327--361},
	number = {3},
	journaltitle = {Review of Educational Research},
	author = {Dillon, J. T.},
	urldate = {2024-11-10},
	date = {1984-09-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {finished, notion},
	file = {SAGE PDF Full Text:C\:\\Users\\Marco\\Zotero\\storage\\P8PL8YLD\\Dillon - 1984 - The Classification of Research Questions.pdf:application/pdf},
}

@inproceedings{sjoberg_future_2007,
	title = {The Future of Empirical Methods in Software Engineering Research},
	url = {https://ieeexplore.ieee.org/document/4221632},
	doi = {10.1109/FOSE.2007.30},
	abstract = {We present the vision that for all fields of software engineering ({SE}), empirical research methods should enable the development of scientific knowledge about how useful different {SE} technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new {SE} technology and is a major input to important {SE} decisions in industry. Major challenges to the pursuit of this vision are: more {SE} research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
	eventtitle = {Future of Software Engineering ({FOSE} '07)},
	pages = {358--378},
	booktitle = {Future of Software Engineering ({FOSE} '07)},
	author = {Sjoberg, Dag I. K. and Dyba, Tore and Jorgensen, Magne},
	urldate = {2024-11-10},
	date = {2007-05},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\9SLHWRLK\\Sjoberg et al. - 2007 - The Future of Empirical Methods in Software Engineering Research.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\ILRQHH33\\4221632.html:text/html},
}

@misc{jaradeh_question_2020,
	title = {Question Answering on Scholarly Knowledge Graphs},
	url = {http://arxiv.org/abs/2006.01527},
	doi = {10.48550/arXiv.2006.01527},
	abstract = {Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present {JarvisQA}, a {BERT} based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, {JarvisQA} is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.},
	number = {{arXiv}:2006.01527},
	publisher = {{arXiv}},
	author = {Jaradeh, Mohamad Yaser and Stocker, Markus and Auer, Sören},
	urldate = {2024-11-27},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2006.01527},
	keywords = {finished, notion, related\_dataset},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6H5I4YVV\\Jaradeh et al. - 2020 - Question Answering on Scholarly Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\P8HTJ8QY\\2006.html:text/html},
}

@incollection{gruninger_role_1995,
	location = {Boston, {MA}},
	title = {The Role of Competency Questions in Enterprise Engineering},
	isbn = {978-0-387-34847-6},
	url = {https://doi.org/10.1007/978-0-387-34847-6_3},
	abstract = {We present a logical framework for representing activities, states, time, and cost in an enterprise integration architecture. We define ontologies for these concepts in first-order logic and consider the problems of temporal projection and reasoning about the occurrence of actions. We characterize the ontology with the use of competency questions. The ontology must contain a necessary and sufficient set of axioms to represent and solve these questions. These questions not only characterize existing ontologies for enterprise engineering, but also drive the development of new ontologies that are required to solve the competency questions.},
	pages = {22--31},
	booktitle = {Benchmarking — Theory and Practice},
	publisher = {Springer {US}},
	author = {Grüninger, Michael and Fox, Mark S.},
	editor = {Rolstadås, Asbjørn},
	urldate = {2024-11-27},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-0-387-34847-6_3},
	keywords = {Enterprise Engineering, Enterprise Model, no type information, notion, Quality Function Deployment, Situation Calculus, Temporal Projection},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6VRYG3KW\\Grüninger und Fox - 1995 - The Role of Competency Questions in Enterprise Engineering.pdf:application/pdf},
}

@article{nguyen_ripple_2017,
	title = {Ripple Down Rules for question answering},
	volume = {8},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw204},
	doi = {10.3233/SW-150204},
	abstract = {Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users’ queries. However, these systems are mostly designed for English. In th},
	pages = {511--532},
	number = {4},
	journaltitle = {Semantic Web},
	author = {Nguyen, Dat Quoc and Nguyen, Dai Quoc and Pham, Son Bao},
	urldate = {2024-11-27},
	date = {2017-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\DHZ8ALS4\\Nguyen et al. - 2017 - Ripple Down Rules for question answering.pdf:application/pdf},
}

@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
	pages = {345--361},
	number = {3},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	urldate = {2024-11-27},
	date = {2016-07-01},
	keywords = {Information retrieval, Natural language processing, Natural language understanding, no type information, notion, Question answering system, Search engine},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EWYKRSZT\\Mishra und Jain - 2016 - A survey on question answering systems with classification.pdf:application/pdf},
}

@article{agee_developing_2009,
	title = {Developing qualitative research questions: a reflective process},
	volume = {22},
	issn = {0951-8398},
	url = {https://doi.org/10.1080/09518390902736512},
	doi = {10.1080/09518390902736512},
	shorttitle = {Developing qualitative research questions},
	abstract = {The reflective and interrogative processes required for developing effective qualitative research questions can give shape and direction to a study in ways that are often underestimated. Good research questions do not necessarily produce good research, but poorly conceived or constructed questions will likely create problems that affect all subsequent stages of a study. In qualitative studies, the ongoing process of questioning is an integral part of understanding the unfolding lives and perspectives of others. This article addresses both the development of initial research questions and how the processes of generating and refining questions are critical to the shaping of a qualitative study.},
	pages = {431--447},
	number = {4},
	journaltitle = {International Journal of Qualitative Studies in Education},
	author = {Agee, Jane},
	urldate = {2024-11-26},
	date = {2009-07-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09518390902736512},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SDZ5PV3Q\\Agee - 2009 - Developing qualitative research questions a reflective process.pdf:application/pdf},
}

@article{fandino_formulating_2019,
	title = {Formulating a good research question: Pearls and pitfalls},
	volume = {63},
	issn = {0019-5049},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6691636/},
	doi = {10.4103/ija.IJA_198_19},
	shorttitle = {Formulating a good research question},
	abstract = {The process of formulating a good research question can be challenging and frustrating. While a comprehensive literature review is compulsory, the researcher usually encounters methodological difficulties in the conduct of the study, particularly if the primary study question has not been adequately selected in accordance with the clinical dilemma that needs to be addressed. Therefore, optimising time and resources before embarking in the design of a clinical protocol can make an impact on the final results of the research project. Researchers have developed effective ways to convey the message of how to build a good research question that can be easily recalled under the acronyms of {PICOT} (population, intervention, comparator, outcome, and time frame) and {FINER} (feasible, interesting, novel, ethical, and relevant). In line with these concepts, this article highlights the main issues faced by clinicians, when developing a research question.},
	pages = {611--616},
	number = {8},
	journaltitle = {Indian Journal of Anaesthesia},
	shortjournal = {Indian J Anaesth},
	author = {Fandino, Wilson},
	urldate = {2024-11-26},
	date = {2019-08},
	pmid = {31462805},
	pmcid = {PMC6691636},
	keywords = {no type information, notion},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BUG64PVV\\Fandino - 2019 - Formulating a good research question Pearls and pitfalls.pdf:application/pdf},
}

@article{ratan_formulation_2019,
	title = {Formulation of Research Question – Stepwise Approach},
	volume = {24},
	doi = {10.4103/jiaps.JIAPS_76_18},
	abstract = {Formulation of research question ({RQ}) is an essentiality before starting any research. It aims to explore an existing uncertainty in an area of concern and points to a need for deliberate investigation. It is, therefore, pertinent to formulate a good {RQ}. The present paper aims to discuss the process of formulation of {RQ} with stepwise approach. The characteristics of good {RQ} are expressed by acronym “{FINERMAPS}” expanded as feasible, interesting, novel, ethical, relevant, manageable, appropriate, potential value, publishability, and systematic. A {RQ} can address different formats depending on the aspect to be evaluated. Based on this, there can be different types of {RQ} such as based on the existence of the phenomenon, description and classification, composition, relationship, comparative, and causality. To develop a {RQ}, one needs to begin by identifying the subject of interest and then do preliminary research on that subject. The researcher then defines what still needs to be known in that particular subject and assesses the implied questions. After narrowing the focus and scope of the research subject, researcher frames a {RQ} and then evaluates it. Thus, conception to formulation of {RQ} is very systematic process and has to be performed meticulously as research guided by such question can have wider impact in the field of social and health research by leading to formulation of policies for the benefit of larger population.},
	pages = {15},
	journaltitle = {Journal of Indian Association of Pediatric Surgeons},
	shortjournal = {Journal of Indian Association of Pediatric Surgeons},
	author = {Ratan, {SimmiK} and Anand, Tanu and Ratan, John},
	date = {2019-01-01},
	keywords = {finished, notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7SRKUYPL\\Ratan et al. - 2019 - Formulation of Research Question – Stepwise Approach.pdf:application/pdf},
}

@misc{giglou_scholarly_2024,
	title = {Scholarly Question Answering using Large Language Models in the {NFDI}4DataScience Gateway},
	url = {http://arxiv.org/abs/2406.07257},
	doi = {10.48550/arXiv.2406.07257},
	abstract = {This paper introduces a scholarly Question Answering ({QA}) system on top of the {NFDI}4DataScience Gateway, employing a Retrieval Augmented Generation-based ({RAG}) approach. The {NFDI}4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search. The {RAG}-based scholarly {QA}, powered by a Large Language Model ({LLM}), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search. The effectiveness of both the Gateway and the scholarly {QA} system is demonstrated through experimental analysis.},
	number = {{arXiv}:2406.07257},
	publisher = {{arXiv}},
	author = {Giglou, Hamed Babaei and Taffa, Tilahun Abedissa and Abdullah, Rana and Usmanova, Aida and Usbeck, Ricardo and D'Souza, Jennifer and Auer, Sören},
	urldate = {2025-01-18},
	date = {2024-06-11},
	eprinttype = {arxiv},
	eprint = {2406.07257 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HKAKIZPQ\\Giglou et al. - 2024 - Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ZFSB6VMH\\2406.html:text/html},
}

@misc{taffa_hybrid-squad_2024,
	title = {Hybrid-{SQuAD}: Hybrid Scholarly Question Answering Dataset},
	url = {http://arxiv.org/abs/2412.02788},
	doi = {10.48550/arXiv.2412.02788},
	shorttitle = {Hybrid-{SQuAD}},
	abstract = {Existing Scholarly Question Answering ({QA}) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs ({KGs}). However, scholarly information often spans heterogeneous sources, necessitating the development of {QA} systems that integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-{SQuAD} (Hybrid Scholarly Question Answering Dataset), a novel large-scale {QA} dataset designed to facilitate answering questions incorporating both text and {KG} facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the {KGs} {DBLP} and {SemOpenAlex} alongside corresponding text from Wikipedia. In addition, we propose a {RAG}-based baseline hybrid {QA} model, achieving an exact match score of 69.65 on the Hybrid-{SQuAD} test set.},
	number = {{arXiv}:2412.02788},
	publisher = {{arXiv}},
	author = {Taffa, Tilahun Abedissa and Banerjee, Debayan and Assabie, Yaregal and Usbeck, Ricardo},
	urldate = {2025-01-18},
	date = {2024-12-05},
	eprinttype = {arxiv},
	eprint = {2412.02788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF6CAJHK\\Taffa et al. - 2024 - Hybrid-SQuAD Hybrid Scholarly Question Answering Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9MZ7S7VM\\2412.html:text/html},
}

@article{taffa_leveraging_2023,
	title = {Leveraging {LLMs} in Scholarly Knowledge Graph Question Answering},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2311.09841},
	doi = {10.48550/ARXIV.2311.09841},
	abstract = {This paper presents a scholarly Knowledge Graph Question Answering ({KGQA}) that answers bibliographic natural language questions by leveraging a large language model ({LLM}) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a {BERT}-based sentence encoder and retrieves their corresponding {SPARQL}. Using the top-n similar question-{SPARQL} pairs as an example and the test question creates a prompt. Then pass the prompt to the {LLM} and generate a {SPARQL}. Finally, runs the {SPARQL} against the underlying {KG} - {ORKG} (Open Research {KG}) endpoint and returns an answer. Our system achieves an F1 score of 99.0\%, on {SciQA} - one of the Scholarly-{QALD}-23 challenge benchmarks.},
	author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
	urldate = {2025-01-12},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Databases (cs.{DB}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GLHLBCLS\\Taffa und Usbeck - 2023 - Leveraging LLMs in Scholarly Knowledge Graph Question Answering.pdf:application/pdf},
}

@inproceedings{nguyen_semantic_2024,
	location = {Singapore},
	title = {Semantic Parsing for Question and Answering over Scholarly Knowledge Graph with Large Language Models},
	isbn = {978-981-97-3076-6},
	doi = {10.1007/978-981-97-3076-6_20},
	abstract = {This paper presents a study to answer the question of how to map a natural language ({NL}) sentence to a semantic representation and its application to question answering over the {DBLP} database. We investigate the deep learning approach using pre-trained models and their fine-tuning on training data for semantic parsing tasks. Experimental results on standard datasets show the effectiveness of pre-trained models in mapping an {NL} sentence to {SPARQL}, a query language for semantic databases. The results also show that the T5 and Flan-T5 models outperform other models in terms of translation accuracy. In addition to the empirical results on pre-trained models, we also consider the problem of examining large language models ({LLMs}) such as Llama and Mistras, or Qwen models for answering questions on the {DBLP} database. Experimental results showed the potentiality of using {LLMs} with chain-of-thought prompting methods. The results indicated that without using training data, we were able to obtain promising results for some types of questions when translating them to {SPARQL}.},
	pages = {284--298},
	booktitle = {New Frontiers in Artificial Intelligence},
	publisher = {Springer Nature},
	author = {Nguyen, Le-Minh and Khang, Le-Nguyen and Anh, Kieu Que and Hien, Nguyen Dieu and Nagai, Yukari},
	editor = {Suzumura, Toyotaro and Bono, Mayumi},
	date = {2024},
	langid = {english},
	keywords = {Knowledge Graph, Mapping {NL} to {SPARQL}, no type information, Question Answering, Relation search, Semantic parsing, Semantic Representation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\AF633YZA\\Nguyen et al. - 2024 - Semantic Parsing for Question and Answering over Scholarly Knowledge Graph with Large Language Model.pdf:application/pdf},
}

@article{molla_question_2007,
	title = {Question Answering in Restricted Domains: An Overview},
	volume = {33},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli.2007.33.1.41},
	doi = {10.1162/coli.2007.33.1.41},
	shorttitle = {Question Answering in Restricted Domains},
	abstract = {Automated question answering has been a topic of research and development since the earliest {AI} applications. Computing power has increased since the first such systems were developed, and the general methodology has changed from the use of hand-encoded knowledge bases about simple domains to the use of text collections as the main knowledge source over more complex domains. Still, many research issues remain. The focus of this article is on the use of restricted domains for automated question answering. The article contains a historical perspective on question answering over restricted domains and an overview of the current methods and applications used in restricted domains. A main characteristic of question answering in restricted domains is the integration of domain-specific information that is either developed for question answering or that has been developed for other purposes. We explore the main methods developed to leverage this domain-specific information.},
	pages = {41--61},
	number = {1},
	journaltitle = {Computational Linguistics},
	shortjournal = {Computational Linguistics},
	author = {Mollá, Diego and Vicedo, José Luis},
	urldate = {2025-01-25},
	date = {2007-03-01},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M2AHTZT4\\Mollá und Vicedo - 2007 - Question Answering in Restricted Domains An Overview.pdf:application/pdf},
}

@inproceedings{theisen_writing_2017,
	title = {Writing Good Software Engineering Research Papers: Revisited},
	url = {https://ieeexplore.ieee.org/abstract/document/7965369},
	doi = {10.1109/ICSE-C.2017.51},
	shorttitle = {Writing Good Software Engineering Research Papers},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented "Writing Good Software Engineering Research Papers" in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering ({ICSE}) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to {ICSE} 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository ({MSR}) papers, a category of papers not seen in 2002. The advent of {MSR} papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	eventtitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	pages = {402--402},
	booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	urldate = {2025-01-25},
	date = {2017-05},
	keywords = {abstracts, Computer science, Conferences, Data mining, guidelines, Market research, no type information, research, Software, Software engineering, writing, Writing},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JKD4WJ2B\\Theisen et al. - 2017 - Writing Good Software Engineering Research Papers Revisited.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\4DY9J7T2\\7965369.html:text/html},
}

@misc{pan_rag_2024,
	title = {A {RAG} Approach for Generating Competency Questions in Ontology Engineering},
	url = {http://arxiv.org/abs/2409.08820},
	doi = {10.48550/arXiv.2409.08820},
	abstract = {Competency question ({CQ}) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models ({LLMs}), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to {LLMs}, we present a retrieval-augmented generation ({RAG}) approach that uses {LLMs} for the automatic generation of {CQs} given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the {RAG} and different temperature setting of the {LLM}. We conduct experiments using {GPT}-4 on two domain ontology engineering tasks and compare results against ground-truth {CQs} constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the {RAG} improves the performance of {LLMs} on generating {CQs} for concrete ontology engineering tasks.},
	number = {{arXiv}:2409.08820},
	publisher = {{arXiv}},
	author = {Pan, Xueli and Ossenbruggen, Jacco van and Boer, Victor de and Huang, Zhisheng},
	urldate = {2025-01-24},
	date = {2024-09-13},
	eprinttype = {arxiv},
	eprint = {2409.08820 [cs]},
	keywords = {Computer Science - Artificial Intelligence, no type information},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\95JBRIN5\\Pan et al. - 2024 - A RAG Approach for Generating Competency Questions in Ontology Engineering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3YCF48HA\\2409.html:text/html},
}

@article{frank_question_2007,
	title = {Question answering from structured knowledge sources},
	volume = {5},
	issn = {1570-8683},
	url = {https://www.sciencedirect.com/science/article/pii/S157086830500090X},
	doi = {10.1016/j.jal.2005.12.006},
	series = {Questions and Answers: Theoretical and Applied Perspectives},
	abstract = {We present an implemented approach for domain-restricted question answering from structured knowledge sources, based on robust semantic analysis in a hybrid {NLP} system architecture. We perform question interpretation and answer extraction in an architecture that builds on a lexical-conceptual structure for question interpretation, which is interfaced with domain-specific concepts and properties in a structured knowledge base. Question interpretation involves a limited amount of domain-specific inferences, and accounts for higher-level quantificational questions. Question interpretation and answer extraction are modular components that interact in clearly defined ways. We derive so-called proto queries from the linguistic representations, which provide partial constraints for answer extraction from the underlying knowledge sources. The search queries we construct from proto queries effectively compute minimal spanning trees from the underlying knowledge sources. Our approach naturally extends to multilingual question answering, and has been developed as a prototype system for two application domains: the domain of Nobel prize winners, and the domain of Language Technology, on the basis of the large ontology underlying the information portal {LT} World.},
	pages = {20--48},
	number = {1},
	journaltitle = {Journal of Applied Logic},
	shortjournal = {Journal of Applied Logic},
	author = {Frank, Anette and Krieger, Hans-Ulrich and Xu, Feiyu and Uszkoreit, Hans and Crysmann, Berthold and Jörg, Brigitte and Schäfer, Ulrich},
	urldate = {2025-01-26},
	date = {2007-03-01},
	keywords = {Data base queries, Hybrid {NLP}, Multilinguality, no type information, Ontology modeling, {QA}, Question semantics, {RMRS}},
	file = {ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NTWNJ2GR\\S157086830500090X.html:text/html},
}

@article{stol_abc_2018,
	title = {The {ABC} of Software Engineering Research},
	volume = {27},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3241743},
	doi = {10.1145/3241743},
	abstract = {A variety of research methods and techniques are available to {SE} researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the {ABC} framework for {SE} research, which offers a holistic view of eight archetypal research strategies. {ABC} refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The {ABC} framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key {SE} domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance {SE} research.},
	pages = {11:1--11:51},
	number = {3},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Stol, Klaas-Jan and Fitzgerald, Brian},
	urldate = {2025-01-25},
	date = {2018-09-17},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GBFDJIMR\\Stol und Fitzgerald - 2018 - The ABC of Software Engineering Research.pdf:application/pdf},
}

@incollection{beck_chapter_2023,
	title = {Chapter 18 - The question: types of research questions and how to develop them},
	isbn = {978-0-323-90300-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323903004001075},
	series = {Handbook for Designing and Conducting Clinical and Translational Research},
	shorttitle = {Chapter 18 - The question},
	abstract = {Regardless of their level of training, clinicians encounter questions relating to patient care and outcomes on a daily basis. Clarifying an answer to these questions not only benefits the clinicians' immediate patients, but also those of his or her colleagues by adding to the existing scientific literature. Fine tuning one’s initial question into a hypothesis and eventual study and analysis sets the stage for a successful project. Utilizing the acronyms {PICOT}, {FINER}, and {SPIDER} helps researchers set reasonable and measurable objectives and aims, thereby avoiding the common pitfalls in question development.},
	pages = {111--120},
	booktitle = {Translational Surgery},
	publisher = {Academic Press},
	author = {Beck, Lauren L.},
	editor = {Eltorai, Adam E. M. and Bakal, Jeffrey A. and Newell, Paige C. and Osband, Adena J.},
	urldate = {2025-01-25},
	date = {2023-01-01},
	doi = {10.1016/B978-0-323-90300-4.00107-5},
	keywords = {{FINER}, no type information, {PICOT}, Research aim, Research objective, Research questions, Study development},
	file = {ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9TTAIFPL\\B9780323903004001075.html:text/html},
}

@misc{jia_leveraging_2024,
	title = {Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2405.15374},
	doi = {10.48550/arXiv.2405.15374},
	abstract = {The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science ({CS}) researchers at the Australian National University ({ANU}). The system integrates Large Language Models ({LLMs}) with the {ANU} Scholarly Knowledge Graph ({ASKG}), a structured repository of all research-related artifacts produced at {ANU} in the {CS} field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph ({KG}). To address the limitations of traditional scholarly {KG} construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model ({DDM}) for comprehensive document representation and the {KG}-enhanced Query Processing ({KGQP}) for optimized complex query handling. {DDM} enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while {KGQP} leverages the {KG} structure to improve query accuracy and efficiency with {LLMs}. By combining the {ASKG} with {LLMs}, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic {LLM}-{SPARQL} fusion to retrieve relevant facts and textual nodes from the {ASKG}. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency. We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with {LLMs}.},
	number = {{arXiv}:2405.15374},
	publisher = {{arXiv}},
	author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio J. Rodríguez and Omran, Pouya G.},
	urldate = {2025-01-26},
	date = {2024-05-24},
	eprinttype = {arxiv},
	eprint = {2405.15374 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, no type information},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\TDBT34CI\\Jia et al. - 2024 - Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\L5WIMD2R\\2405.html:text/html},
}

@inproceedings{hermjakob_parsing_2001,
	title = {Parsing and Question Classification for Question Answering},
	url = {https://aclanthology.org/W01-1203/},
	booktitle = {Proceedings of the {ACL} 2001 Workshop on Open-Domain Question Answering},
	author = {Hermjakob, Ulf},
	urldate = {2025-01-24},
	date = {2001},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KS5QL4VW\\Hermjakob - 2001 - Parsing and Question Classification for Question Answering.pdf:application/pdf},
}

@article{li_learning_2006,
	title = {Learning question classifiers: the role of semantic information},
	volume = {12},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/learning-question-classifiers-the-role-of-semantic-information/F3E3EBFC2061BBF74A6EB926A3A3B291},
	doi = {10.1017/S1351324905003955},
	shorttitle = {Learning question classifiers},
	abstract = {To respond correctly to a free form factual question given a large collection of text data, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This work presents a machine learning approach to question classification. Guided by a layered semantic hierarchy of answer types, we develop a hierarchical classifier that classifies questions into fine-grained classes. This work also performs a systematic study of the use of semantic information sources in natural language classification tasks. It is shown that, in the context of question classification, augmenting the input of the classifier with appropriate semantic category information results in significant improvements to classification accuracy. We show accurate results on a large collection of free-form questions used in {TREC} 10 and 11.},
	pages = {229--249},
	number = {3},
	journaltitle = {Natural Language Engineering},
	author = {Li, Xin and Roth, Dan},
	urldate = {2025-01-26},
	date = {2006-09},
	langid = {english},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\LUYXMDNY\\Li und Roth - 2006 - Learning question classifiers the role of semantic information.pdf:application/pdf},
}

@article{kamper_types_2020,
	title = {Types of Research Questions: Descriptive, Predictive, or Causal},
	volume = {50},
	issn = {0190-6011},
	url = {https://www.jospt.org/doi/full/10.2519/jospt.2020.0703},
	doi = {10.2519/jospt.2020.0703},
	shorttitle = {Types of Research Questions},
	abstract = {A previous Evidence in Practice article explained why a specific and answerable research question is important for clinicians and researchers. Determining whether a study aims to answer a descriptive, predictive, or causal question should be one of the first things a reader does when reading an article. Any type of question can be relevant and useful to support evidence-based practice, but only if the question is well defined, matched to the right study design, and reported correctly. J Orthop Sports Phys Ther 2020;50(8):468–469. doi:10.2519/jospt.2020.0703},
	pages = {468--469},
	number = {8},
	journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
	author = {Kamper, Steven J.},
	urldate = {2025-01-25},
	date = {2020-08},
	note = {Publisher: Journal of Orthopaedic \& Sports Physical Therapy},
	keywords = {clinical practice, evidence-based practice, finished, research, study quality},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BI2W9X8Q\\Kamper - 2020 - Types of Research Questions Descriptive, Predictive, or Causal.pdf:application/pdf},
}

@misc{li_scigraphqa_2023,
	title = {{SciGraphQA}: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs},
	url = {http://arxiv.org/abs/2308.03349},
	doi = {10.48550/arXiv.2308.03349},
	shorttitle = {{SciGraphQA}},
	abstract = {In this work, we present {SciGraphQA}, a synthetic multi-turn question-answer dataset related to academic graphs. {SciGraphQA} is 13 times larger than {ChartVQA}, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart {VQA} dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning {ArXiv} papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. We asked {GPT}-4 to assess the matching quality of our question-answer turns given the paper's context, obtaining an average rating of 8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most popular {MLLM} models such as {LLaVa}, {mPLUGowl}, {BLIP}-2, and {openFlamingo}'s on our dataset, finding {LLaVA}-13B being the most performant with a {CIDEr} score of 0.08. We further enriched the question prompts for {LLAVA} by including the serialized data tables extracted from the graphs using the {DePlot} model, boosting {LLaVA}'s 0-shot {CIDEr} to 0.15. To verify the validity of our dataset, we also fine-tuned {LLaVa} using our dataset, reaching a substantially higher {CIDEr} score of 0.26. We anticipate further accuracy improvement by including segmentation mask tokens and leveraging larger {LLM} backbones coupled with emergent prompting techniques. Our code and data are open-sourced.},
	number = {{arXiv}:2308.03349},
	publisher = {{arXiv}},
	author = {Li, Shengzhi and Tajbakhsh, Nima},
	urldate = {2025-01-25},
	date = {2023-08-07},
	eprinttype = {arxiv},
	eprint = {2308.03349 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, no type information},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\J6NLGJT2\\Li und Tajbakhsh - 2023 - SciGraphQA A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\YTL4Y9BU\\2308.html:text/html},
}

@article{chakraborty_introduction_2021,
	title = {Introduction to neural network-based question answering over knowledge graphs},
	volume = {11},
	rights = {© 2021 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
	issn = {1942-4795},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/widm.1389},
	doi = {10.1002/widm.1389},
	abstract = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs ({KGQA}) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for {KGQA}. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for {KGQA}, and ease their process of making informed decisions while creating their own {QA} systems. This article is categorized under: Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Technologies {\textgreater} Artificial Intelligence},
	pages = {e1389},
	number = {3},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
	urldate = {2025-01-25},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1389},
	keywords = {deep learning, knowledge graphs, no type information, question answering},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GD8N2ZSF\\Chakraborty et al. - 2021 - Introduction to neural network-based question answering over knowledge graphs.pdf:application/pdf},
}

@inproceedings{liu_taxonomy_2015,
	location = {New York, {NY}, {USA}},
	title = {A Taxonomy for Classifying Questions Asked in Social Question and Answering},
	isbn = {978-1-4503-3146-3},
	url = {https://dl.acm.org/doi/10.1145/2702613.2732928},
	doi = {10.1145/2702613.2732928},
	series = {{CHI} {EA} '15},
	abstract = {The rapid advancement of Web2.0 technologies has made social networking sites, such as Facebook and twitter, important venues for individuals to seek and share information. As understanding the information needs of users is crucial for designing and developing tools to support their social Q\&amp;A behaviors, in this paper, we present a new way of classifying questions from a design perspective, with the aim of facilitating the development of question routing systems according to individual's information need. As an attempt to understand the questioner's intent in social question and answering environments, we propose a taxonomy of questions posted on Twitter, called {ASK}. Our taxonomy uncovers three different kinds of questions: accuracy, social, and knowledge. In addition, to enable automatic detection on these three types of information needs, we measured and reported on the differences in {ASK} types of questions reflected at both lexical and syntactic levels.},
	pages = {1947--1952},
	booktitle = {Proceedings of the 33rd Annual {ACM} Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhe and Jansen, Bernard J.},
	urldate = {2025-01-25},
	date = {2015-04-18},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DPQGCG9R\\Liu und Jansen - 2015 - A Taxonomy for Classifying Questions Asked in Social Question and Answering.pdf:application/pdf},
}

@inproceedings{unger_template-based_2012,
	location = {New York, {NY}, {USA}},
	title = {Template-based question answering over {RDF} data},
	isbn = {978-1-4503-1229-5},
	url = {https://dl.acm.org/doi/10.1145/2187836.2187923},
	doi = {10.1145/2187836.2187923},
	series = {{WWW} '12},
	abstract = {As an increasing amount of {RDF} data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the {RDF} data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a {SPARQL} template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.},
	pages = {639--648},
	booktitle = {Proceedings of the 21st international conference on World Wide Web},
	publisher = {Association for Computing Machinery},
	author = {Unger, Christina and Bühmann, Lorenz and Lehmann, Jens and Ngonga Ngomo, Axel-Cyrille and Gerber, Daniel and Cimiano, Philipp},
	urldate = {2025-01-25},
	date = {2012-04-16},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ULBUFL56\\Unger et al. - 2012 - Template-based question answering over RDF data.pdf:application/pdf},
}

@inproceedings{navarro-almanza_towards_2017,
	title = {Towards Supporting Software Engineering Using Deep Learning: A Case of Software Requirements Classification},
	url = {https://ieeexplore.ieee.org/abstract/document/8337942},
	doi = {10.1109/CONISOFT.2017.00021},
	shorttitle = {Towards Supporting Software Engineering Using Deep Learning},
	abstract = {Software Requirements are the basis of high-quality software development process, each step is related to {SR}, these represent the needs and expectations of the software in a very detailed form. The software requirement classification ({SRC}) task requires a lot of human effort, specially when there are huge of requirements, therefore, the automation of {SRC} have been addressed using Natural Language Processing ({NLP}) and Information Retrieval ({IR}) techniques, however, generally requires human effort to analyze and create features from corpus (set of requirements). In this work, we propose to use Deep Learning ({DL}) to classify software requirements without labor intensive feature engineering. The model that we propose is based on Convolutional Neural Network ({CNN}) that has been state of art in other natural language related tasks. To evaluate our proposed model, {PROMISE} corpus was used, contains a set of labeled requirements in functional and 11 different categories of non-functional requirements. We achieve promising results on {SRC} using {CNN} even without handcrafted features.},
	eventtitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
	pages = {116--120},
	booktitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
	author = {Navarro-Almanza, Raul and Juarez-Ramirez, Reyes and Licea, Guillermo},
	urldate = {2025-01-25},
	date = {2017-10},
	keywords = {Computer architecture, Convolutional Neural Network, Convolutional neural networks, Data models, Machine learning, no type information, Software, Software engineering, Software Engineering, Software Requirement Classification, Task analysis, Word Embedding},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\4CHUMTQC\\Navarro-Almanza et al. - 2017 - Towards Supporting Software Engineering Using Deep Learning A Case of Software Requirements Classif.pdf:application/pdf},
}

@inproceedings{kotonya_towards_2003,
	title = {Towards a classification model for component-based software engineering research},
	url = {https://ieeexplore.ieee.org/abstract/document/1231566},
	doi = {10.1109/EURMIC.2003.1231566},
	abstract = {Accurate and timely information is a key motivator in the widespread adoption of {CBSE} technology in Europe. Although there are overlaps and informal communications between researchers and adopters of {CBSE} technology in Europe, there is no systematic mechanism for information interchange between the two. {CBSEnet} is a European Union initiative to create an Internet-based forum for the exchange of information between researchers and adopters of {CBSE}. We describe a proposed classification model for {CBSE} research that will form the basis for structuring the {CBSEnet} knowledge base.},
	eventtitle = {2003 Proceedings 29th Euromicro Conference},
	pages = {43--52},
	booktitle = {2003 Proceedings 29th Euromicro Conference},
	author = {{Kotonya} and {Sommerville} and {Hall}},
	urldate = {2025-01-25},
	date = {2003-09},
	note = {{ISSN}: 1089-6503},
	keywords = {Electronic data interchange, Internet, no type information, Object oriented programming, Software development management},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\NANNFD7I\\Kotonya et al. - 2003 - Towards a classification model for component-based software engineering research.pdf:application/pdf},
}

@article{zhao_ontology_2009,
	title = {Ontology Classification for Semantic-Web-Based Software Engineering},
	volume = {2},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1939-1374},
	url = {http://ieeexplore.ieee.org/document/5161251/},
	doi = {10.1109/TSC.2009.20},
	abstract = {The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.},
	pages = {303--317},
	number = {4},
	journaltitle = {{IEEE} Transactions on Services Computing},
	shortjournal = {{IEEE} Trans. Serv. Comput.},
	author = {Zhao, Yajing and Dong, Jing and Peng, Tu},
	urldate = {2025-01-25},
	date = {2009-10},
}

@inproceedings{britto_blooms_2015,
	title = {Bloom's taxonomy in software engineering education: A systematic mapping study},
	url = {https://ieeexplore.ieee.org/abstract/document/7344084},
	doi = {10.1109/FIE.2015.7344084},
	shorttitle = {Bloom's taxonomy in software engineering education},
	abstract = {Designing and assessing learning outcomes could be a challenging activity for any Software Engineering ({SE}) educator. To support the process of designing and assessing {SE} courses, educators have been applied the cognitive domain of Bloom's taxonomy. However, to the best of our knowledge, the evidence on the usage of Bloom's taxonomy in {SE} higher education has not yet been systematically aggregated or reviewed. Therefore, in this paper we report the state of the art on the usage of Bloom's taxonomy in {SE} education, identified by conducted a systematic mapping study. As a result of the performed systematic mapping study, 26 studies were deemed as relevant. The main findings from these studies are: i) Bloom's taxonomy has mostly been applied at undergraduate level for both design and assessment of software engineering courses; ii) software construction is the leading {SE} subarea in which Bloom's taxonomy has been applied. The results clearly point out the usefulness of Bloom's taxonomy in the {SE} education context. We intend to use the results from this systematic mapping study to develop a set of guidelines to support the usage of Bloom's taxonomy cognitive levels to design and assess {SE} courses.},
	eventtitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
	pages = {1--8},
	booktitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
	author = {Britto, Ricardo and Usman, Muhammad},
	urldate = {2025-01-25},
	date = {2015-10},
	keywords = {Context, Data mining, Education, no type information, Software, Software engineering, Systematics, Taxonomy},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\L6J3LNFA\\Britto und Usman - 2015 - Bloom's taxonomy in software engineering education A systematic mapping study.pdf:application/pdf},
}

@article{britto_extended_2016,
	title = {An extended global software engineering taxonomy},
	volume = {4},
	issn = {2195-1721},
	url = {https://doi.org/10.1186/s40411-016-0029-2},
	doi = {10.1186/s40411-016-0029-2},
	abstract = {In Global Software Engineering ({GSE}), the need for a common terminology and knowledge classification has been identified to facilitate the sharing and combination of knowledge by {GSE} researchers and practitioners. A {GSE} taxonomy was recently proposed to address such a need, focusing on a core set of dimensions; however its dimensions do not represent an exhaustive list of relevant {GSE} factors. Therefore, this study extends the existing taxonomy, incorporating new {GSE} dimensions that were identified by means of two empirical studies conducted recently.},
	pages = {3},
	number = {1},
	journaltitle = {Journal of Software Engineering Research and Development},
	shortjournal = {J Softw Eng Res Dev},
	author = {Britto, Ricardo and Wohlin, Claes and Mendes, Emilia},
	urldate = {2025-01-25},
	date = {2016-06-07},
	langid = {english},
	keywords = {Global software engineering, Knowledge classification, no type information, Software engineering management, Taxonomy},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GHXB96P5\\Britto et al. - 2016 - An extended global software engineering taxonomy.pdf:application/pdf},
}

@article{steinmetz_what_2021,
	title = {What is in the {KGQA} Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs},
	volume = {10},
	issn = {1861-2040},
	url = {https://doi.org/10.1007/s13740-021-00128-9},
	doi = {10.1007/s13740-021-00128-9},
	shorttitle = {What is in the {KGQA} Benchmark Datasets?},
	abstract = {Question Answering based on Knowledge Graphs ({KGQA}) still faces difficult challenges when transforming natural language ({NL}) to {SPARQL} queries. Simple questions only referring to one triple are answerable by most {QA} systems, but more complex questions requiring complex queries containing subqueries or several functions are still a tough challenge within this field of research. Evaluation results of {QA} systems therefore also might depend on the benchmark dataset the system has been tested on. For the purpose to give an overview and reveal specific characteristics, we examined currently available {KGQA} datasets regarding several challenging aspects. This paper presents a detailed look into the datasets and compares them in terms of challenges a {KGQA} system is facing.},
	pages = {241--265},
	number = {3},
	journaltitle = {Journal on Data Semantics},
	shortjournal = {J Data Semant},
	author = {Steinmetz, Nadine and Sattler, Kai-Uwe},
	urldate = {2025-01-25},
	date = {2021-12-01},
	langid = {english},
	keywords = {Artificial Intelligence, Dataset analysis, finished, Natural language transformation, Pattern recognition, Question answering on knowledge graphs},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\T9Q8XQAM\\Steinmetz und Sattler - 2021 - What is in the KGQA Benchmark Datasets Survey on Challenges in Datasets for Question Answering on K.pdf:application/pdf},
}

@inproceedings{bolotova_non-factoid_2022,
	location = {New York, {NY}, {USA}},
	title = {A Non-Factoid Question-Answering Taxonomy},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531926},
	doi = {10.1145/3477495.3531926},
	series = {{SIGIR} '22},
	abstract = {Non-factoid question answering ({NFQA}) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - {NFQs}. There is still little understanding of the categories of {NFQs} that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are.  This work presents the first comprehensive taxonomy of {NFQ} categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised {NFQs} and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major {NFQA} datasets, showing that the {NFQ} categories that are the most challenging for current {NFQA} systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy, along with the category classifier, will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories.},
	pages = {1196--1207},
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Bolotova, Valeriia and Blinov, Vladislav and Scholer, Falk and Croft, W. Bruce and Sanderson, Mark},
	urldate = {2025-01-25},
	date = {2022-07-07},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\A7ABRSYP\\Bolotova et al. - 2022 - A Non-Factoid Question-Answering Taxonomy.pdf:application/pdf},
}

@misc{leidner_question_2002,
	title = {Question Answering over Unstructured Data without Domain Restrictions},
	url = {http://arxiv.org/abs/cs/0207058},
	doi = {10.48550/arXiv.cs/0207058},
	abstract = {Information needs are naturally represented as questions. Automatic Natural-Language Question Answering ({NLQA}) has only recently become a practical task on a larger scale and without domain constraints. This paper gives a brief introduction to the field, its history and the impact of systematic evaluation competitions. It is then demonstrated that an {NLQA} system for English can be built and evaluated in a very short time using off-the-shelf parsers and thesauri. The system is based on Robust Minimal Recursion Semantics ({RMRS}) and is portable with respect to the parser used as a frontend. It applies atomic term unification supported by question classification and {WordNet} lookup for semantic similarity matching of parsed question representation and free text.},
	number = {{arXiv}:cs/0207058},
	publisher = {{arXiv}},
	author = {Leidner, Jochen L.},
	urldate = {2025-01-24},
	date = {2002-07-18},
	eprinttype = {arxiv},
	eprint = {cs/0207058},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2TU65NMU\\0207058.html:text/html},
}

@article{usbeck_qald-10_2023,
	title = {{QALD}-10 – The 10th challenge on question answering over linked data},
	volume = {Preprint},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw233471},
	doi = {10.3233/SW-233471},
	abstract = {Knowledge Graph Question Answering ({KGQA}) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field f},
	pages = {1--15},
	issue = {Preprint},
	journaltitle = {Semantic Web},
	author = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
	urldate = {2025-01-24},
	date = {2023-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6SCX689Q\\Usbeck et al. - 2023 - QALD-10 – The 10th challenge on question answering over linked data.pdf:application/pdf},
}

@inproceedings{allam_question_2016,
	title = {The Question Answering Systems : A Survey .},
	url = {https://www.semanticscholar.org/paper/The-Question-Answering-Systems-%3A-A-Survey-.-Allam-Haggag/b2944a85b9cb428a28e30bdd236471e712667e91},
	shorttitle = {The Question Answering Systems},
	abstract = {Question Answering ({QA}) is a specialized area in the field of Information Retrieval ({IR}). The {QA} systems are concerned with providing relevant answers in response to questions proposed in natural language. {QA} is therefore composed of three distinct modules, each of which has a core component beside other supplementary components. These three core components are: question classification, information retrieval, and answer extraction. Question classification plays an essential role in {QA} systems by classifying the submitted question according to its type. Information retrieval is very important for question answering, because if no correct answers are present in a document, no further processing could be carried out to find an answer. Finally, answer extraction aims to retrieve the answer for a question asked by the user. This survey paper provides an overview of Question-Answering and its system architecture, as well as the previous related work comparing each research against the others with respect to the components that were covered and the approaches that were followed. At the end, the survey provides an analytical discussion of the proposed {QA} models, along with their main contributions, experimental results, and limitations.},
	author = {Allam, Ali Mohamed Nabil and Haggag, Mohamed H.},
	urldate = {2025-02-05},
	date = {2016},
}
