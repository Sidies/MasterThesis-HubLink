
@article{auer_sciqa_2023,
	title = {The {SciQA} Scientific Question Answering Benchmark for Scholarly Knowledge},
	volume = {13},
	rights = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-33607-z},
	doi = {10.1038/s41598-023-33607-z},
	abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering ({QA}) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as {DBpedia} and Wikidata. We present {SciQA} a scientific {QA} benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph ({ORKG}) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the {ORKG}. The questions cover a range of research fields and question types and are translated into corresponding {SPARQL} queries over the {ORKG}. Based on two preliminary evaluations, we show that the resulting {SciQA} benchmark represents a challenging task for next-generation {QA} systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data ({QALD}) Challenge.},
	pages = {7240},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
	urldate = {2024-08-26},
	date = {2023-05-04},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EELWCWD3\\Auer et al. - 2023 - The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge.pdf:application/pdf},
}

@article{saikh_scienceqa_2022,
	title = {{ScienceQA}: a novel resource for question answering on scholarly articles},
	volume = {23},
	issn = {1432-1300},
	url = {https://doi.org/10.1007/s00799-022-00329-y},
	doi = {10.1007/s00799-022-00329-y},
	shorttitle = {{ScienceQA}},
	abstract = {Machine Reading Comprehension ({MRC}) of a document is a challenging problem that requires discourse-level understanding. Information extraction from scholarly articles nowadays is a critical use case for researchers to understand the underlying research quickly and move forward, especially in this age of infodemic. {MRC} on research articles can also provide helpful information to the reviewers and editors. However, the main bottleneck in building such models is the availability of human-annotated data. In this paper, firstly, we introduce a dataset to facilitate question answering ({QA}) on scientific articles. We prepare the dataset in a semi-automated fashion having more than 100k human-annotated context–question–answer triples. Secondly, we implement one baseline {QA} model based on Bidirectional Encoder Representations from Transformers ({BERT}). Additionally, we implement two models: the first one is based on Science {BERT} ({SciBERT}), and the second is the combination of {SciBERT} and Bi-Directional Attention Flow (Bi-{DAF}). The best model (i.e., {SciBERT}) obtains an F1 score of 75.46\%. Our dataset is novel, and our work opens up a new avenue for scholarly document processing research by providing a benchmark {QA} dataset and standard baseline. We make our dataset and codes available here at https://github.com/{TanikSaikh}/Scientific-Question-Answering.},
	pages = {289--301},
	number = {3},
	journaltitle = {International Journal on Digital Libraries},
	shortjournal = {Int J Digit Libr},
	author = {Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal, Asif and Bhattacharyya, Pushpak},
	urldate = {2024-09-24},
	date = {2022-09-01},
	langid = {english},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PQDSQULV\\Saikh et al. - 2022 - ScienceQA a novel resource for question answering on scholarly articles.pdf:application/pdf},
}

@inproceedings{karras_divide_2023,
	title = {Divide and Conquer the {EmpiRE}: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering},
	url = {https://ieeexplore.ieee.org/document/10304795},
	doi = {10.1109/ESEM56168.2023.10304795},
	shorttitle = {Divide and Conquer the {EmpiRE}},
	abstract = {[Background.] Empirical research in requirements engineering ({RE}) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its “current” state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph ({ORKG}) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in {RE} ({KG}-{EmpiRE}) whose data is openly available. Our long-term goal is to continuously maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. [Method.] We conduct a literature review using the {ORKG} to build and publish {KG}-{EmpiRE} which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020–2025. [Results.] From 570 papers of the {IEEE} International Requirements Engineering Conference (2000–2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The {ORKG} is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
	eventtitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	pages = {1--12},
	booktitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
	urldate = {2024-09-28},
	date = {2023-10},
	keywords = {domain\_specific, finished, notion},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2SUXDABT\\Karras et al. - 2023 - Divide and Conquer the EmpiRE A Community-Maintainable Knowledge Graph of Empirical Research in Req.pdf:application/pdf},
}

@inproceedings{shaw_writing_2003,
	title = {Writing good software engineering research papers},
	url = {https://ieeexplore.ieee.org/document/1201262},
	doi = {10.1109/ICSE.2003.1201262},
	abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to {XSE} 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
	eventtitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	pages = {726--736},
	booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	author = {Shaw, M.},
	urldate = {2024-10-02},
	date = {2003-05},
	note = {{ISSN}: 0270-5257},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\N3KSK3ZY\\Shaw - 2003 - Writing good software engineering research papers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\GVGKBD9L\\1201262.html:text/html},
}

@incollection{easterbrook_selecting_2008,
	location = {London},
	title = {Selecting Empirical Methods for Software Engineering Research},
	isbn = {978-1-84800-044-5},
	url = {https://doi.org/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	pages = {285--311},
	booktitle = {Guide to Advanced Empirical Software Engineering},
	publisher = {Springer},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	urldate = {2024-10-02},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-1-84800-044-5_11},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KC9B8WYM\\Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineering Research.pdf:application/pdf},
}

@inproceedings{li_learning_2002,
	title = {Learning Question Classifiers},
	url = {https://aclanthology.org/C02-1150},
	doi = {10.3115/1072228.107237},
	eventtitle = {{COLING} 2002},
	booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
	author = {Li, Xin and Roth, Dan},
	urldate = {2024-10-26},
	date = {2002},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RU9HI9XC\\Li and Roth - 2002 - Learning Question Classifiers.pdf:application/pdf},
}

@inproceedings{singhal_att_1999,
	title = {{AT}\&T at {TREC}-8},
	abstract = {In 1999, {AT}\&T participated in the ad-hoc task and the Question Answering {QA}, Spoken Document Retrieval {SDR}, and Web tracks. Most of our eeort for {TREC}-8 focused on the {QA} and {SDR} tracks. Results from {SDR} track show that our document expansion techniques, presented in 99, are very eeective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can the correct answer for about 45 of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.},
	author = {Singhal, Amit and Abney, Steve and Bacchiani, Michiel and Collins, Michael and Hindle, Donald and Pereira, Fernando},
	date = {1999-11-01},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BH2UE3PD\\Singhal et al. - 1999 - AT&T at TREC-8.pdf:application/pdf},
}

@misc{wang_modern_2022,
	title = {Modern Question Answering Datasets and Benchmarks: A Survey},
	url = {http://arxiv.org/abs/2206.15030},
	doi = {10.48550/arXiv.2206.15030},
	shorttitle = {Modern Question Answering Datasets and Benchmarks},
	abstract = {Question Answering ({QA}) is one of the most important natural language processing ({NLP}) tasks. It aims using {NLP} technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging {QA} datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential {QA} datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common {QA} tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of {QA} research.},
	number = {{arXiv}:2206.15030},
	publisher = {{arXiv}},
	author = {Wang, Zhen},
	urldate = {2024-12-09},
	date = {2022-06-30},
	eprinttype = {arxiv},
	eprint = {2206.15030 [cs]},
	keywords = {Computer Science - Computation and Language, not related, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YN4FWW34\\Wang - 2022 - Modern Question Answering Datasets and Benchmarks A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3Q5LRYHP\\2206.html:text/html},
}

@misc{liang_scemqa_2024,
	title = {{SceMQA}: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
	url = {http://arxiv.org/abs/2402.05138},
	doi = {10.48550/arXiv.2402.05138},
	shorttitle = {{SceMQA}},
	abstract = {The paper introduces {SceMQA}, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. {SceMQA} focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of {AI} models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. {SceMQA} also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models ({MLLMs}), across various experimental settings. The results show that further research and development are needed in developing more capable {MLLM}, as highlighted by only 50\% to 60\% accuracy achieved by the strongest models. Our benchmark and analysis will be available at https://scemqa.github.io/},
	number = {{arXiv}:2402.05138},
	publisher = {{arXiv}},
	author = {Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang},
	urldate = {2024-12-09},
	date = {2024-02-06},
	eprinttype = {arxiv},
	eprint = {2402.05138 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, not related, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GK23ULAL\\Liang et al. - 2024 - SceMQA A Scientific College Entrance Level Multimodal Question Answering Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\M67H6CV5\\2402.html:text/html},
}

@inproceedings{lehmann_large_2024,
	location = {Cham},
	title = {Large Language Models for Scientific Question Answering: An Extensive Analysis of the {SciQA} Benchmark},
	isbn = {978-3-031-60626-7},
	doi = {10.1007/978-3-031-60626-7_11},
	shorttitle = {Large Language Models for Scientific Question Answering},
	abstract = {The {SciQA} benchmark for scientific question answering aims to represent a challenging task for next-generation question-answering systems on which vanilla large language models fail. In this article, we provide an analysis of the performance of language models on this benchmark including prompting and fine-tuning techniques to adapt them to the {SciQA} task. We show that both fine-tuning and prompting techniques with intelligent few-shot selection allow us to obtain excellent results on the {SciQA} benchmark. We discuss the valuable lessons and common error categories, and outline their implications on how to optimise large language models for question answering over knowledge graphs.},
	pages = {199--217},
	booktitle = {The Semantic Web},
	publisher = {Springer Nature Switzerland},
	author = {Lehmann, Jens and Meloni, Antonello and Motta, Enrico and Osborne, Francesco and Recupero, Diego Reforgiato and Salatino, Angelo Antonio and Vahdati, Sahar},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	date = {2024},
	langid = {english},
	keywords = {Few-shot learning, Fine-tuning, Knowledge graphs, Language models., Question answering},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XGV6PN7E\\Lehmann et al. - 2024 - Large Language Models for Scientific Question Answering An Extensive Analysis of the SciQA Benchmar.pdf:application/pdf},
}

@inproceedings{lehnert_conceptual_1977,
	location = {San Francisco, {CA}, {USA}},
	title = {A conceptual theory of question answering},
	series = {{IJCAI}'77},
	abstract = {A theory of Q/A has been proposed from the perspective of natural language processing that relieson ideas in conceptual information processing and theories of human memory organization. This theory of Q/A has been implemented in a computer program, {QUALM}. {QUALM} is currently used by two story understanding systems ({SAM} and {PAM}) to complete a natural language processing system that reads stories and answers questions about what was read.},
	pages = {158--164},
	booktitle = {Proceedings of the 5th international joint conference on Artificial intelligence - Volume 1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Lehnert, Wendy G.},
	urldate = {2025-03-02},
	date = {1977-08-22},
}
