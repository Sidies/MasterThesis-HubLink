
@misc{banerjee_dblp-quad_2023,
	title = {{DBLP}-{QuAD}: A Question Answering Dataset over the {DBLP} Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2303.13351},
	shorttitle = {{DBLP}-{QuAD}},
	abstract = {In this work we create a question answering dataset over the {DBLP} scholarly knowledge graph ({KG}). {DBLP} is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding {SPARQL} queries which can be executed over the {DBLP} {KG} to fetch the correct answer. {DBLP}-{QuAD} is the largest scholarly question answering dataset.},
	number = {{arXiv}:2303.13351},
	publisher = {{arXiv}},
	author = {Banerjee, Debayan and Awale, Sushil and Usbeck, Ricardo and Biemann, Chris},
	urldate = {2024-07-09},
	date = {2023-03-29},
	langid = {english},
	keywords = {finished, notion, related\_dataset},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\5CNDQ4X4\\Banerjee et al. - 2023 - DBLP-QuAD A Question Answering Dataset over the DBLP Scholarly Knowledge Graph.pdf:application/pdf},
}

@article{auer_sciqa_2023,
	title = {The {SciQA} Scientific Question Answering Benchmark for Scholarly Knowledge},
	volume = {13},
	rights = {2023 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-023-33607-z},
	doi = {10.1038/s41598-023-33607-z},
	abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering ({QA}) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as {DBpedia} and Wikidata. We present {SciQA} a scientific {QA} benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph ({ORKG}) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the {ORKG}. The questions cover a range of research fields and question types and are translated into corresponding {SPARQL} queries over the {ORKG}. Based on two preliminary evaluations, we show that the resulting {SciQA} benchmark represents a challenging task for next-generation {QA} systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data ({QALD}) Challenge.},
	pages = {7240},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
	urldate = {2024-08-26},
	date = {2023-05-04},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EELWCWD3\\Auer et al. - 2023 - The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge.pdf:application/pdf},
}

@inproceedings{dubey_lc-quad_2019,
	location = {Cham},
	title = {{LC}-{QuAD} 2.0: A Large Dataset for Complex Question Answering over Wikidata and {DBpedia}},
	isbn = {978-3-030-30796-7},
	doi = {10.1007/978-3-030-30796-7_5},
	shorttitle = {{LC}-{QuAD} 2.0},
	abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like {WebQuestions}, {QALD} and {LCQuAD} have been published in the past. The biggest data set available for complex questions ({LCQuAD}) over knowledge graphs contains five thousand questions. We now provide {LC}-{QuAD} 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding {SPARQL} queries. {LC}-{QuAD} 2.0 is compatible with both Wikidata and {DBpedia} 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
	pages = {69--78},
	booktitle = {The Semantic Web – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	date = {2019},
	langid = {english},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ARFHIF8U\\Dubey et al. - 2019 - LC-QuAD 2.0 A Large Dataset for Complex Question Answering over Wikidata and DBpedia.pdf:application/pdf},
}

@misc{bordes_large-scale_2015,
	title = {Large-scale Simple Question Answering with Memory Networks},
	url = {http://arxiv.org/abs/1506.02075},
	abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
	number = {{arXiv}:1506.02075},
	publisher = {{arXiv}},
	author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
	urldate = {2024-09-23},
	date = {2015-06-05},
	eprinttype = {arxiv},
	eprint = {1506.02075 [cs]},
	keywords = {finished, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\KXD2CVA2\\Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NCZKK6ME\\1506.html:text/html},
}

@inproceedings{karras_divide_2023,
	title = {Divide and Conquer the {EmpiRE}: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering},
	url = {https://ieeexplore.ieee.org/document/10304795},
	doi = {10.1109/ESEM56168.2023.10304795},
	shorttitle = {Divide and Conquer the {EmpiRE}},
	abstract = {[Background.] Empirical research in requirements engineering ({RE}) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its “current” state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph ({ORKG}) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in {RE} ({KG}-{EmpiRE}) whose data is openly available. Our long-term goal is to continuously maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. [Method.] We conduct a literature review using the {ORKG} to build and publish {KG}-{EmpiRE} which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020–2025. [Results.] From 570 papers of the {IEEE} International Requirements Engineering Conference (2000–2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The {ORKG} is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
	eventtitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	pages = {1--12},
	booktitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
	urldate = {2024-09-28},
	date = {2023-10},
	keywords = {domain\_specific, finished, notion},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2SUXDABT\\Karras et al. - 2023 - Divide and Conquer the EmpiRE A Community-Maintainable Knowledge Graph of Empirical Research in Req.pdf:application/pdf},
}

@inproceedings{shaw_writing_2003,
	title = {Writing good software engineering research papers},
	url = {https://ieeexplore.ieee.org/document/1201262},
	doi = {10.1109/ICSE.2003.1201262},
	abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to {XSE} 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
	eventtitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	pages = {726--736},
	booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	author = {Shaw, M.},
	urldate = {2024-10-02},
	date = {2003-05},
	note = {{ISSN}: 0270-5257},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\N3KSK3ZY\\Shaw - 2003 - Writing good software engineering research papers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\GVGKBD9L\\1201262.html:text/html},
}

@incollection{easterbrook_selecting_2008,
	location = {London},
	title = {Selecting Empirical Methods for Software Engineering Research},
	isbn = {978-1-84800-044-5},
	url = {https://doi.org/10.1007/978-1-84800-044-5_11},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	pages = {285--311},
	booktitle = {Guide to Advanced Empirical Software Engineering},
	publisher = {Springer},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	urldate = {2024-10-02},
	date = {2008},
	langid = {english},
	doi = {10.1007/978-1-84800-044-5_11},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KC9B8WYM\\Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineering Research.pdf:application/pdf},
}

@misc{tran_comparative_2022,
	title = {A Comparative Study of Question Answering over Knowledge Bases},
	url = {http://arxiv.org/abs/2211.08170},
	doi = {10.48550/arXiv.2211.08170},
	abstract = {Question answering over knowledge bases ({KBQA}) has become a popular approach to help users extract information from knowledge bases. Although several systems exist, choosing one suitable for a particular application scenario is difficult. In this article, we provide a comparative study of six representative {KBQA} systems on eight benchmark datasets. In that, we study various question types, properties, languages, and domains to provide insights on where existing systems struggle. On top of that, we propose an advanced mapping algorithm to aid existing models in achieving superior results. Moreover, we also develop a multilingual corpus {COVID}-{KGQA}, which encourages {COVID}-19 research and multilingualism for the diversity of future {AI}. Finally, we discuss the key findings and their implications as well as performance guidelines and some future improvements. Our source code is available at {\textbackslash}url\{https://github.com/tamlhp/kbqa\}.},
	number = {{arXiv}:2211.08170},
	publisher = {{arXiv}},
	author = {Tran, Khiem Vinh and Phan, Hao Phu and Quach, Khang Nguyen Duc and Nguyen, Ngan Luu-Thuy and Jo, Jun and Nguyen, Thanh Tam},
	urldate = {2024-10-26},
	date = {2022-11-15},
	eprinttype = {arxiv},
	eprint = {2211.08170},
	keywords = {finished, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GYCPXHQW\\Tran et al. - 2022 - A Comparative Study of Question Answering over Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VG9KIM26\\2211.html:text/html},
}

@inproceedings{li_learning_2002,
	title = {Learning Question Classifiers},
	url = {https://aclanthology.org/C02-1150},
	doi = {10.3115/1072228.107237},
	eventtitle = {{COLING} 2002},
	booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
	author = {Li, Xin and Roth, Dan},
	urldate = {2024-10-26},
	date = {2002},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RU9HI9XC\\Li and Roth - 2002 - Learning Question Classifiers.pdf:application/pdf},
}

@inproceedings{singhal_att_1999,
	title = {{AT}\&T at {TREC}-8},
	abstract = {In 1999, {AT}\&T participated in the ad-hoc task and the Question Answering {QA}, Spoken Document Retrieval {SDR}, and Web tracks. Most of our eeort for {TREC}-8 focused on the {QA} and {SDR} tracks. Results from {SDR} track show that our document expansion techniques, presented in 99, are very eeective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can the correct answer for about 45 of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.},
	author = {Singhal, Amit and Abney, Steve and Bacchiani, Michiel and Collins, Michael and Hindle, Donald and Pereira, Fernando},
	date = {1999-11-01},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BH2UE3PD\\Singhal et al. - 1999 - AT&T at TREC-8.pdf:application/pdf},
}

@inproceedings{riloff_rule-based_2000,
	title = {A Rule-based Question Answering System for Reading Comprehension Tests},
	url = {https://aclanthology.org/W00-0603},
	doi = {10.3115/1117595.1117598},
	booktitle = {{ANLP}-{NAACL} 2000 Workshop: Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems},
	author = {Riloff, Ellen and Thelen, Michael},
	urldate = {2024-10-27},
	date = {2000},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5HB3ATU\\Riloff and Thelen - 2000 - A Rule-based Question Answering System for Reading Comprehension Tests.pdf:application/pdf},
}

@inproceedings{moldovan_structure_2000,
	location = {Hong Kong},
	title = {The Structure and Performance of an Open-Domain Question Answering System},
	url = {https://aclanthology.org/P00-1071},
	doi = {10.3115/1075218.1075289},
	eventtitle = {{ACL} 2000},
	pages = {563--570},
	booktitle = {Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
	urldate = {2024-10-27},
	date = {2000-10},
	keywords = {finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\I9HD6KI6\\Moldovan et al. - 2000 - The Structure and Performance of an Open-Domain Question Answering System.pdf:application/pdf},
}

@inproceedings{mikhailian_learning_2009,
	location = {Suntec, Singapore},
	title = {Learning foci for Question Answering over Topic Maps},
	url = {https://aclanthology.org/P09-2082},
	eventtitle = {{ACL}-{IJCNLP} 2009},
	pages = {325--328},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Mikhailian, Alexander and Dalmas, Tiphaine and Pinchuk, Rani},
	editor = {Su, Keh-Yih and Su, Jian and Wiebe, Janyce and Li, Haizhou},
	urldate = {2024-10-27},
	date = {2009-08},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XDL4SXLQ\\Mikhailian et al. - 2009 - Learning foci for Question Answering over Topic Maps.pdf:application/pdf},
}

@article{thuan_construction_2019,
	title = {Construction of Design Science Research Questions},
	volume = {44},
	issn = {1529-3181},
	url = {https://aisel.aisnet.org/cais/vol44/iss1/20},
	doi = {10.17705/1CAIS.04420},
	number = {1},
	journaltitle = {Communications of the Association for Information Systems},
	author = {Thuan, Nguyen and Drechsler, Andreas and Antunes, Pedro},
	date = {2019-03-01},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\W3667CHP\\Thuan et al. - 2019 - Construction of Design Science Research Questions.pdf:application/pdf},
}

@article{dillon_classification_1984,
	title = {The Classification of Research Questions},
	volume = {54},
	issn = {0034-6543},
	url = {https://doi.org/10.3102/00346543054003327},
	doi = {10.3102/00346543054003327},
	abstract = {What are the kinds of questions that may be posed for research? A dozen schemes proposing to classify research questions are surveyed, analyzed, and applied to the understanding and practice of inquiry. The extent to which the various schemes account for questions found in educational journals is estimated. Some principles and issues are identified to stimulate work on the classification of research questions in education and other enterprises of inquiry. On the whole, little is known about the kinds of questions that may be posed for research.},
	pages = {327--361},
	number = {3},
	journaltitle = {Review of Educational Research},
	author = {Dillon, J. T.},
	urldate = {2024-11-10},
	date = {1984-09-01},
	langid = {english},
	note = {Publisher: American Educational Research Association},
	keywords = {finished, notion},
	file = {SAGE PDF Full Text:C\:\\Users\\Marco\\Zotero\\storage\\P8PL8YLD\\Dillon - 1984 - The Classification of Research Questions.pdf:application/pdf},
}

@inproceedings{sjoberg_future_2007,
	title = {The Future of Empirical Methods in Software Engineering Research},
	url = {https://ieeexplore.ieee.org/document/4221632},
	doi = {10.1109/FOSE.2007.30},
	abstract = {We present the vision that for all fields of software engineering ({SE}), empirical research methods should enable the development of scientific knowledge about how useful different {SE} technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new {SE} technology and is a major input to important {SE} decisions in industry. Major challenges to the pursuit of this vision are: more {SE} research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
	eventtitle = {Future of Software Engineering ({FOSE} '07)},
	pages = {358--378},
	booktitle = {Future of Software Engineering ({FOSE} '07)},
	author = {Sjoberg, Dag I. K. and Dyba, Tore and Jorgensen, Magne},
	urldate = {2024-11-10},
	date = {2007-05},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\9SLHWRLK\\Sjoberg et al. - 2007 - The Future of Empirical Methods in Software Engineering Research.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\ILRQHH33\\4221632.html:text/html},
}

@misc{jaradeh_question_2020,
	title = {Question Answering on Scholarly Knowledge Graphs},
	url = {http://arxiv.org/abs/2006.01527},
	doi = {10.48550/arXiv.2006.01527},
	abstract = {Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present {JarvisQA}, a {BERT} based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, {JarvisQA} is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.},
	number = {{arXiv}:2006.01527},
	publisher = {{arXiv}},
	author = {Jaradeh, Mohamad Yaser and Stocker, Markus and Auer, Sören},
	urldate = {2024-11-27},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2006.01527},
	keywords = {finished, notion, related\_dataset},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6H5I4YVV\\Jaradeh et al. - 2020 - Question Answering on Scholarly Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\P8HTJ8QY\\2006.html:text/html},
}

@article{nguyen_ripple_2017,
	title = {Ripple Down Rules for question answering},
	volume = {8},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw204},
	doi = {10.3233/SW-150204},
	abstract = {Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users’ queries. However, these systems are mostly designed for English. In th},
	pages = {511--532},
	number = {4},
	journaltitle = {Semantic Web},
	author = {Nguyen, Dat Quoc and Nguyen, Dai Quoc and Pham, Son Bao},
	urldate = {2024-11-27},
	date = {2017-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\DHZ8ALS4\\Nguyen et al. - 2017 - Ripple Down Rules for question answering.pdf:application/pdf},
}

@inproceedings{chernov_linguistically_2015,
	location = {Vilnius, Lithuania},
	title = {Linguistically Motivated Question Classification},
	url = {https://aclanthology.org/W15-1809},
	eventtitle = {{NoDaLiDa} 2015},
	pages = {51--59},
	booktitle = {Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015)},
	publisher = {Linköping University Electronic Press, Sweden},
	author = {Chernov, Alexandr and Petukhova, Volha and Klakow, Dietrich},
	editor = {Megyesi, Beáta},
	urldate = {2024-11-27},
	date = {2015-05},
	keywords = {finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\29FG3BPG\\Chernov et al. - 2015 - Linguistically Motivated Question Classification.pdf:application/pdf},
}

@article{ratan_formulation_2019,
	title = {Formulation of Research Question – Stepwise Approach},
	volume = {24},
	doi = {10.4103/jiaps.JIAPS_76_18},
	abstract = {Formulation of research question ({RQ}) is an essentiality before starting any research. It aims to explore an existing uncertainty in an area of concern and points to a need for deliberate investigation. It is, therefore, pertinent to formulate a good {RQ}. The present paper aims to discuss the process of formulation of {RQ} with stepwise approach. The characteristics of good {RQ} are expressed by acronym “{FINERMAPS}” expanded as feasible, interesting, novel, ethical, relevant, manageable, appropriate, potential value, publishability, and systematic. A {RQ} can address different formats depending on the aspect to be evaluated. Based on this, there can be different types of {RQ} such as based on the existence of the phenomenon, description and classification, composition, relationship, comparative, and causality. To develop a {RQ}, one needs to begin by identifying the subject of interest and then do preliminary research on that subject. The researcher then defines what still needs to be known in that particular subject and assesses the implied questions. After narrowing the focus and scope of the research subject, researcher frames a {RQ} and then evaluates it. Thus, conception to formulation of {RQ} is very systematic process and has to be performed meticulously as research guided by such question can have wider impact in the field of social and health research by leading to formulation of policies for the benefit of larger population.},
	pages = {15},
	journaltitle = {Journal of Indian Association of Pediatric Surgeons},
	shortjournal = {Journal of Indian Association of Pediatric Surgeons},
	author = {Ratan, {SimmiK} and Anand, Tanu and Ratan, John},
	date = {2019-01-01},
	keywords = {finished, notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7SRKUYPL\\Ratan et al. - 2019 - Formulation of Research Question – Stepwise Approach.pdf:application/pdf},
}

@misc{taffa_hybrid-squad_2024,
	title = {Hybrid-{SQuAD}: Hybrid Scholarly Question Answering Dataset},
	url = {http://arxiv.org/abs/2412.02788},
	doi = {10.48550/arXiv.2412.02788},
	shorttitle = {Hybrid-{SQuAD}},
	abstract = {Existing Scholarly Question Answering ({QA}) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs ({KGs}). However, scholarly information often spans heterogeneous sources, necessitating the development of {QA} systems that integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-{SQuAD} (Hybrid Scholarly Question Answering Dataset), a novel large-scale {QA} dataset designed to facilitate answering questions incorporating both text and {KG} facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the {KGs} {DBLP} and {SemOpenAlex} alongside corresponding text from Wikipedia. In addition, we propose a {RAG}-based baseline hybrid {QA} model, achieving an exact match score of 69.65 on the Hybrid-{SQuAD} test set.},
	number = {{arXiv}:2412.02788},
	publisher = {{arXiv}},
	author = {Taffa, Tilahun Abedissa and Banerjee, Debayan and Assabie, Yaregal and Usbeck, Ricardo},
	urldate = {2025-01-18},
	date = {2024-12-05},
	eprinttype = {arxiv},
	eprint = {2412.02788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF6CAJHK\\Taffa et al. - 2024 - Hybrid-SQuAD Hybrid Scholarly Question Answering Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9MZ7S7VM\\2412.html:text/html},
}

@article{kamper_types_2020,
	title = {Types of Research Questions: Descriptive, Predictive, or Causal},
	volume = {50},
	issn = {0190-6011},
	url = {https://www.jospt.org/doi/full/10.2519/jospt.2020.0703},
	doi = {10.2519/jospt.2020.0703},
	shorttitle = {Types of Research Questions},
	abstract = {A previous Evidence in Practice article explained why a specific and answerable research question is important for clinicians and researchers. Determining whether a study aims to answer a descriptive, predictive, or causal question should be one of the first things a reader does when reading an article. Any type of question can be relevant and useful to support evidence-based practice, but only if the question is well defined, matched to the right study design, and reported correctly. J Orthop Sports Phys Ther 2020;50(8):468–469. doi:10.2519/jospt.2020.0703},
	pages = {468--469},
	number = {8},
	journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
	author = {Kamper, Steven J.},
	urldate = {2025-01-25},
	date = {2020-08},
	note = {Publisher: Journal of Orthopaedic \& Sports Physical Therapy},
	keywords = {clinical practice, evidence-based practice, finished, research, study quality},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BI2W9X8Q\\Kamper - 2020 - Types of Research Questions Descriptive, Predictive, or Causal.pdf:application/pdf},
}

@inproceedings{liu_taxonomy_2015,
	location = {New York, {NY}, {USA}},
	title = {A Taxonomy for Classifying Questions Asked in Social Question and Answering},
	isbn = {978-1-4503-3146-3},
	url = {https://dl.acm.org/doi/10.1145/2702613.2732928},
	doi = {10.1145/2702613.2732928},
	series = {{CHI} {EA} '15},
	abstract = {The rapid advancement of Web2.0 technologies has made social networking sites, such as Facebook and twitter, important venues for individuals to seek and share information. As understanding the information needs of users is crucial for designing and developing tools to support their social Q\&amp;A behaviors, in this paper, we present a new way of classifying questions from a design perspective, with the aim of facilitating the development of question routing systems according to individual's information need. As an attempt to understand the questioner's intent in social question and answering environments, we propose a taxonomy of questions posted on Twitter, called {ASK}. Our taxonomy uncovers three different kinds of questions: accuracy, social, and knowledge. In addition, to enable automatic detection on these three types of information needs, we measured and reported on the differences in {ASK} types of questions reflected at both lexical and syntactic levels.},
	pages = {1947--1952},
	booktitle = {Proceedings of the 33rd Annual {ACM} Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhe and Jansen, Bernard J.},
	urldate = {2025-01-25},
	date = {2015-04-18},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DPQGCG9R\\Liu und Jansen - 2015 - A Taxonomy for Classifying Questions Asked in Social Question and Answering.pdf:application/pdf},
}

@article{steinmetz_what_2021,
	title = {What is in the {KGQA} Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs},
	volume = {10},
	issn = {1861-2040},
	url = {https://doi.org/10.1007/s13740-021-00128-9},
	doi = {10.1007/s13740-021-00128-9},
	shorttitle = {What is in the {KGQA} Benchmark Datasets?},
	abstract = {Question Answering based on Knowledge Graphs ({KGQA}) still faces difficult challenges when transforming natural language ({NL}) to {SPARQL} queries. Simple questions only referring to one triple are answerable by most {QA} systems, but more complex questions requiring complex queries containing subqueries or several functions are still a tough challenge within this field of research. Evaluation results of {QA} systems therefore also might depend on the benchmark dataset the system has been tested on. For the purpose to give an overview and reveal specific characteristics, we examined currently available {KGQA} datasets regarding several challenging aspects. This paper presents a detailed look into the datasets and compares them in terms of challenges a {KGQA} system is facing.},
	pages = {241--265},
	number = {3},
	journaltitle = {Journal on Data Semantics},
	shortjournal = {J Data Semant},
	author = {Steinmetz, Nadine and Sattler, Kai-Uwe},
	urldate = {2025-01-25},
	date = {2021-12-01},
	langid = {english},
	keywords = {Artificial Intelligence, Dataset analysis, finished, Natural language transformation, Pattern recognition, Question answering on knowledge graphs},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\T9Q8XQAM\\Steinmetz und Sattler - 2021 - What is in the KGQA Benchmark Datasets Survey on Challenges in Datasets for Question Answering on K.pdf:application/pdf},
}

@inproceedings{bolotova_non-factoid_2022,
	location = {New York, {NY}, {USA}},
	title = {A Non-Factoid Question-Answering Taxonomy},
	isbn = {978-1-4503-8732-3},
	url = {https://dl.acm.org/doi/10.1145/3477495.3531926},
	doi = {10.1145/3477495.3531926},
	series = {{SIGIR} '22},
	abstract = {Non-factoid question answering ({NFQA}) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - {NFQs}. There is still little understanding of the categories of {NFQs} that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are.  This work presents the first comprehensive taxonomy of {NFQ} categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised {NFQs} and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major {NFQA} datasets, showing that the {NFQ} categories that are the most challenging for current {NFQA} systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy, along with the category classifier, will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories.},
	pages = {1196--1207},
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Bolotova, Valeriia and Blinov, Vladislav and Scholer, Falk and Croft, W. Bruce and Sanderson, Mark},
	urldate = {2025-01-25},
	date = {2022-07-07},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\A7ABRSYP\\Bolotova et al. - 2022 - A Non-Factoid Question-Answering Taxonomy.pdf:application/pdf},
}

@article{usbeck_qald-10_2023,
	title = {{QALD}-10 – The 10th challenge on question answering over linked data},
	volume = {Preprint},
	issn = {1570-0844},
	url = {https://content.iospress.com/articles/semantic-web/sw233471},
	doi = {10.3233/SW-233471},
	abstract = {Knowledge Graph Question Answering ({KGQA}) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field f},
	pages = {1--15},
	issue = {Preprint},
	journaltitle = {Semantic Web},
	author = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
	urldate = {2025-01-24},
	date = {2023-01-01},
	langid = {english},
	note = {Publisher: {IOS} Press},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6SCX689Q\\Usbeck et al. - 2023 - QALD-10 – The 10th challenge on question answering over linked data.pdf:application/pdf},
}

@inproceedings{allam_question_2016,
	title = {The Question Answering Systems : A Survey .},
	url = {https://www.semanticscholar.org/paper/The-Question-Answering-Systems-%3A-A-Survey-.-Allam-Haggag/b2944a85b9cb428a28e30bdd236471e712667e91},
	shorttitle = {The Question Answering Systems},
	abstract = {Question Answering ({QA}) is a specialized area in the field of Information Retrieval ({IR}). The {QA} systems are concerned with providing relevant answers in response to questions proposed in natural language. {QA} is therefore composed of three distinct modules, each of which has a core component beside other supplementary components. These three core components are: question classification, information retrieval, and answer extraction. Question classification plays an essential role in {QA} systems by classifying the submitted question according to its type. Information retrieval is very important for question answering, because if no correct answers are present in a document, no further processing could be carried out to find an answer. Finally, answer extraction aims to retrieve the answer for a question asked by the user. This survey paper provides an overview of Question-Answering and its system architecture, as well as the previous related work comparing each research against the others with respect to the components that were covered and the approaches that were followed. At the end, the survey provides an analytical discussion of the proposed {QA} models, along with their main contributions, experimental results, and limitations.},
	author = {Allam, Ali Mohamed Nabil and Haggag, Mohamed H.},
	urldate = {2025-02-05},
	date = {2016},
}
