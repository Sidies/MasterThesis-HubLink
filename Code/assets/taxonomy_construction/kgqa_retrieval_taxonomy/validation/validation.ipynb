{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxonomy Evaluation Preparation\n",
    "\n",
    "In this notebook, we are going to prepare the data for the evaluation of each iteration of the taxonomy. \n",
    "\n",
    "The evaluation will focus on the metrics of laconicity, lucidity, completeness, adaption, innovation, orthogonality and soundness as outlined by Kaplan et al. [1]. There is already a implementation available to calculate these metrics [here](https://github.com/Eden-06/abstraction-quality/). We will be using this implementation. For this we need to prepare 3 files which will be generated when this notebook is run.\n",
    "1. A `extractions.txt` file which contains all the extractions from the literature analysis\n",
    "2. A `taxonomy.txt` file which contains the taxonomy of the iteration we want to evaluate\n",
    "3. A `mapping.txt` file which maps each of the extractions to the taxonomy\n",
    "4. A `orthogonality.xlsx` file for each iteration which contains a matrix for each taxonomy category and class.\n",
    "\n",
    "After the files have been generated, there will be a folder for each increment of the taxonomy. In each folder there will be individual files for each iteration. \n",
    "The next step is to use the `aquality.rb` script from the implementation mentioned above to calculate the metrics. For this, the script has to be put inside of the folder of the iteration and run.\n",
    "This will generate a `results.txt` file which contains the results of the evaluation. \n",
    "\n",
    "Next, there will be orthogonality excel files for each iteration. They have to be manually filled out following the explanations by Kaplan et a. [1].\n",
    "\n",
    "Moreover, this notebook is calculating the adaption and innovation scores for each iteration:\n",
    "1. A `innovation_and_adaption.txt` file which contains the innovation and adaption scores. \n",
    "\n",
    "#### Note\n",
    "The generation of these files can be easily and automatically done because we decided to document the taxonomy creation process with structured `JSON` artifacts that are easy for machine processing. The following code will generate the files. The evaluation is then done with the provided script from the implementation mentioned above in `ruby`.\n",
    "\n",
    "\n",
    "\n",
    "[1] Kaplan, A., et al.: Introducing an evaluation method for taxonomies. In: EASE. ACM (2022), accepted, to appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to load the JSON files\n",
    "import os\n",
    "import json\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "tax_inc_dir = os.path.join(\"../construction_and_refinement/taxonomy_increments.json\")\n",
    "cluster_inc_dir = os.path.join(\"../clustering/cluster_increments.json\")\n",
    "extraction_dir = os.path.join(\"../extraction/extraction.json\")\n",
    "\n",
    "with open(os.path.join(current_dir, cluster_inc_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    cluster_increments = json.load(f)\n",
    "\n",
    "with open(os.path.join(current_dir, tax_inc_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    taxonomy_increments = json.load(f)\n",
    "\n",
    "with open(os.path.join(current_dir, extraction_dir), \"r\", encoding=\"utf-8\") as f:\n",
    "    extraction = json.load(f)\n",
    "\n",
    "# Storage directory\n",
    "storage_dir = os.path.join(current_dir, \"metric_calculations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we prepare the extractions file\n",
    "extractions = []\n",
    "for paper in extraction:\n",
    "    title = paper.get(\"title\")\n",
    "    for p_type_dict in paper.get(\"types\"):\n",
    "        p_type = p_type_dict.get(\"type\")\n",
    "        extractions.append(f\"{p_type} ({title})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of cluster types: 227\n"
     ]
    }
   ],
   "source": [
    "# Next we need to get each type of the extraction process mapped to each cluster\n",
    "# This is necessary, because in the taxonomy, we can trace the construction back to each cluster\n",
    "cluster_id_types = {}\n",
    "for cluster in cluster_increments:\n",
    "    iteration = cluster.get(\"iteration\")\n",
    "    if not iteration == 2:\n",
    "        continue\n",
    "    amount_of_cluster_types = 0\n",
    "    for cluster_dict in cluster.get(\"clusters\"):\n",
    "        cluster_id = cluster_dict.get(\"id\")\n",
    "        for subcluster_dict in cluster_dict.get(\"subclusters\"):\n",
    "            subcluster_id = subcluster_dict.get(\"id\")\n",
    "            for type_dict in subcluster_dict.get(\"types\"):\n",
    "                if subcluster_id not in cluster_id_types:\n",
    "                    cluster_id_types[subcluster_id] = []\n",
    "                cluster_id_types[subcluster_id].append(type_dict)\n",
    "                amount_of_cluster_types += 1\n",
    "    print(f\"Amount of cluster types: {amount_of_cluster_types}\")\n",
    "\n",
    "# Then we prepare a helper function that created the taxonomy and mapping dictionaries\n",
    "def get_taxonomy_eval_data(iteration: int = 1):\n",
    "    \"\"\"\n",
    "    Returns the catalog taxonomy and mapping dictionaries for a given iteration.\n",
    "    \"\"\"\n",
    "    catalog_taxonomy = []\n",
    "    mapping = []\n",
    "    for iteration_dict in taxonomy_increments:\n",
    "        tax_iteration = iteration_dict.get(\"iteration\")\n",
    "        if not iteration == tax_iteration:\n",
    "            continue\n",
    "\n",
    "        for dimension_dict in iteration_dict.get(\"taxonomy\"):\n",
    "            dimension_name = dimension_dict.get(\"name\", \"Unknown\")\n",
    "            for category_dict in dimension_dict.get(\"categories\"):\n",
    "                category_name = category_dict.get(\"name\", \"Unknown\")\n",
    "                for subcategory_dict in category_dict.get(\"categories\", []):\n",
    "                    subcategory_name = subcategory_dict.get(\"name\", \"Unknown\")\n",
    "                    catalog_taxonomy.append(\n",
    "                        dimension_name + \" - \" + category_name + \" - \" + subcategory_name)\n",
    "                    based_on_clusters = subcategory_dict.get(\"based_on_clusters\", [])\n",
    "                    for cluster_id in based_on_clusters:\n",
    "                        if cluster_id in cluster_id_types:\n",
    "                            for type_dict in cluster_id_types[cluster_id]:\n",
    "                                type_name = type_dict.get(\"name\", \"Unknown\")\n",
    "                                type_source = type_dict.get(\"source\", \"Unknown\")\n",
    "                                mapping.append(\n",
    "                                    dimension_name + \" - \" + category_name + \" - \" + subcategory_name + \" : \" + type_name + \" (\" + type_source + \")\")\n",
    "\n",
    "                catalog_taxonomy.append(\n",
    "                    dimension_name + \" - \" + category_name)\n",
    "                based_on_clusters = category_dict.get(\"based_on_clusters\", [])\n",
    "                for cluster_id in based_on_clusters:\n",
    "                    if cluster_id in cluster_id_types:\n",
    "                        for type_dict in cluster_id_types[cluster_id]:\n",
    "                            type_name = type_dict.get(\"name\", \"Unknown\")\n",
    "                            type_source = type_dict.get(\"source\", \"Unknown\")\n",
    "                            mapping.append(\n",
    "                                dimension_name + \" - \" + category_name + \" : \" + type_name + \" (\" + type_source + \")\")\n",
    "                            \n",
    "    mapping.sort()\n",
    "    return catalog_taxonomy, mapping\n",
    "\n",
    "def compute_innovation_and_adaption(catalog_taxonomy: list, mapping: list):\n",
    "    \"\"\"\n",
    "    Computes the innovation and adaptation metrics.\n",
    "    \"\"\"\n",
    "    adapted_count = 0\n",
    "    new_count = 0\n",
    "    \n",
    "    for taxonomy_item in catalog_taxonomy:\n",
    "        if any(m.startswith(taxonomy_item + \" :\") for m in mapping):\n",
    "            adapted_count += 1\n",
    "        else:\n",
    "            new_count += 1\n",
    "    \n",
    "    total = len(catalog_taxonomy)\n",
    "    innovation = new_count / total if total > 0 else 0.0\n",
    "    adaptation = adapted_count / total if total > 0 else 0.0\n",
    "\n",
    "    innovation_str = f\"innovation: {innovation:.2f} ({new_count}/{total})\"\n",
    "    adaptation_str = f\"adaptation: {adaptation:.2f} ({adapted_count}/{total})\"\n",
    "    return innovation_str, adaptation_str\n",
    "\n",
    "\n",
    "# Now we prepare a helper function to store the results in the storage directory\n",
    "def save_taxonomy_eval_data(iteration: int = 1, catalog_taxonomy: list = [], mapping: list = [], extractions: list = []):\n",
    "    iteration_dir = os.path.join(storage_dir, f\"taxonomy_iteration_{iteration}\")\n",
    "    os.makedirs(iteration_dir, exist_ok=True)\n",
    "    with open(os.path.join(iteration_dir, \"catalog_taxonomy.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(catalog_taxonomy))\n",
    "\n",
    "    with open(os.path.join(iteration_dir, \"mapping.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(mapping))\n",
    "\n",
    "    with open(os.path.join(iteration_dir, \"extractions.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(extractions))\n",
    "\n",
    "    innovation_score, adaptation_score = compute_innovation_and_adaption(catalog_taxonomy, mapping)\n",
    "\n",
    "    with open(os.path.join(iteration_dir, \"innovation_and_adaption.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(innovation_score)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(adaptation_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of catalog taxonomy: 58\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graph Representation - Single Fact',\n",
       " 'Graph Representation - Multi Fact',\n",
       " 'Answer Type - Undefined',\n",
       " 'Answer Type - Other',\n",
       " 'Answer Type - Date',\n",
       " 'Answer Type - Distance Measurement',\n",
       " 'Answer Type - Actor',\n",
       " 'Answer Type - Technology',\n",
       " 'Answer Type - Definition',\n",
       " 'Answer Type - Time',\n",
       " 'Answer Type - Name',\n",
       " 'Answer Type - Title',\n",
       " 'Answer Type - Bibliometric Numbers',\n",
       " 'Answer Type - Software System',\n",
       " 'Answer Type - Monetary',\n",
       " 'Answer Type - Abbreviation',\n",
       " 'Answer Type - Instructional',\n",
       " 'Answer Type - Procedure/Technique',\n",
       " 'Answer Type - Organization',\n",
       " 'Answer Type - Duration',\n",
       " 'Answer Type - Boolean',\n",
       " 'Answer Type - Entity',\n",
       " 'Answer Type - Description',\n",
       " 'Answer Type - Properties',\n",
       " 'Answer Type - Human/Person',\n",
       " 'Answer Type - Location',\n",
       " 'Answer Type - Quantitative',\n",
       " 'Answer Type - Tool/Notation',\n",
       " 'Answer Type - Solution',\n",
       " 'Answer Type - Theoretical Framework',\n",
       " 'Question Type - Negation',\n",
       " 'Question Type - Dependency',\n",
       " 'Question Type - Contingency',\n",
       " 'Question Type - Counting',\n",
       " 'Question Type - Superlative',\n",
       " 'Question Type - Comparison',\n",
       " 'Question Type - Listing',\n",
       " 'Question Type - Multiple Intentions',\n",
       " 'Question Type - Ranking',\n",
       " 'Question Type - Temporal',\n",
       " 'Question Type - Aggregation',\n",
       " 'Question Type - Relationship',\n",
       " 'Research Focus - Development Methods',\n",
       " 'Research Focus - Analytical Methods',\n",
       " 'Research Focus - Generalization',\n",
       " 'Research Focus - Qualitative Modeling',\n",
       " 'Research Focus - Empirical Modeling',\n",
       " 'Research Focus - Analytic Modeling',\n",
       " 'Answer Credibility - Factual',\n",
       " 'Answer Credibility - Opinion',\n",
       " 'Answer Credibility - Debate',\n",
       " 'Answer Credibility - Predictive',\n",
       " 'Question Goal - Exploratory',\n",
       " 'Question Goal - Reasoning',\n",
       " 'Question Goal - Problem Solving',\n",
       " 'Question Goal - Gap Spotting',\n",
       " 'Question Goal - Problematization',\n",
       " 'Question Goal - Method Improvement']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mapping: 162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Answer Credibility - Debate : Debate (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Factual : Accuracy (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Credibility - Factual : Evidence-Based (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Factual : Factual Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Credibility - Opinion : Experience (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Opinion : Knowledge (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Credibility - Predictive : Predictive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Answer Type - Abbreviation : Abbreviation (Learning Question Classifiers)',\n",
       " 'Answer Type - Actor : Actor (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Bibliometric Numbers : Bibliometric Numbers (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Boolean : Affirm (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_3Term (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_MoreTuples (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Ask (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Boolean (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Answer Type - Boolean : Boolean (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Answer Type - Boolean : Boolean (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Answer Type - Boolean : Boolean (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Yes/No (A Comparative Study of Question Answering over Knowledge Bases)',\n",
       " 'Answer Type - Boolean : YesNo (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Date : Answer Type - Date (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Date : Date (AT&T at TREC-8)',\n",
       " 'Answer Type - Date : Date (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Date : Date (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Date : Month (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Definition : Answer Type - Definition (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Definition : Definition (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Definition : Definition (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Description : Base-Rate - Descriptive-Process (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Answer Type - Description : Description (Learning Question Classifiers)',\n",
       " 'Answer Type - Description : Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Description : Description and Classification (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Answer Type - Description : Descriptive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Answer Type - Distance Measurement : Answer Type - Distance (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Distance Measurement : Linear Measure (AT&T at TREC-8)',\n",
       " 'Answer Type - Duration : Duration (AT&T at TREC-8)',\n",
       " 'Answer Type - Entity : Entities (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Entity : Entity (Learning Question Classifiers)',\n",
       " 'Answer Type - Entity : Entity (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Human/Person : Answer Type - Person (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Human/Person : Human (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Human/Person : Human (Learning Question Classifiers)',\n",
       " 'Answer Type - Human/Person : Human (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Human/Person : Human Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Human/Person : Person (AT&T at TREC-8)',\n",
       " 'Answer Type - Human/Person : Social (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Type - Instructional : Instruction (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Type - Location : Answer Type - Location (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Location : Location (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Location : Location (AT&T at TREC-8)',\n",
       " 'Answer Type - Location : Location (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Location : Location (Learning Question Classifiers)',\n",
       " 'Answer Type - Location : Location (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Location : Location (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Monetary : Answer Type - Money (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Monetary : Answer Type - Price (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Monetary : Monetary Amount (AT&T at TREC-8)',\n",
       " 'Answer Type - Name : Name (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Organization : Answer Type - Organization (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Organization : Human Groups (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Organization : Organization (AT&T at TREC-8)',\n",
       " 'Answer Type - Organization : Organization (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Other : Other (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Other : Other (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Other : Other Named Entity (AT&T at TREC-8)',\n",
       " 'Answer Type - Procedure/Technique : Procedure or Technique (Writing good software engineering research papers)',\n",
       " 'Answer Type - Properties : First Order - Properties (The Classification of Research Questions)',\n",
       " 'Answer Type - Quantitative : Answer Type - Number (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Many (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Many-Class (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Numeric (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Quantitative : Numeric Value (Learning Question Classifiers)',\n",
       " 'Answer Type - Quantitative : Quantification Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Type - Quantitative : Quantity (AT&T at TREC-8)',\n",
       " 'Answer Type - Software System : Software System (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Solution : Specific Solution, Prototype, or Judgment (Writing good software engineering research papers)',\n",
       " 'Answer Type - Technology : Technology (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Theoretical Framework : Theory-in-Use (Construction of Design Science Research Questions)',\n",
       " 'Answer Type - Time : Answer Type - Time (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Time : Time (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Time : Time (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Time : Time (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Title : Answer Type - Title (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Tool/Notation : Tool or Notation (Writing good software engineering research papers)',\n",
       " 'Answer Type - Undefined : Answer Type - Undefined (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Undefined : Fourth Order -Extra (The Classification of Research Questions)',\n",
       " 'Answer Type - Undefined : Miscellaneous (AT&T at TREC-8)',\n",
       " 'Answer Type - Undefined : Unknown (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Graph Representation - Multi Fact : Combining (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Multi Fact : Multi Fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Multi Fact : Multi Facts (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Multi Fact : Question Content - Non-Factoid - sequence of facts (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Normal (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Single Fact : Normal (Ripple Down Rules for question answering)',\n",
       " 'Graph Representation - Single Fact : Question Content - Factoid (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Single Fact (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Single Fact : Single Fact (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Graph Representation - Single Fact : Single Fact with Type (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Single Fact : single fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Goal - Exploratory : Existence (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Goal - Exploratory : Exploratory - Description and Classification (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Exploratory : Exploratory - Descriptive-Comparative (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Exploratory : Exploratory - Existence (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Exploratory : Feasibility Study or Exploration (Writing good software engineering research papers)',\n",
       " 'Question Goal - Exploratory : Research Approach - Knowledge Goal (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Gap Spotting : Motivation - Gap Spotting (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Method Improvement : Design (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Problem Solving : Motivation - Problem Solving (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Problematization : Motivation - Problematization (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Reasoning : Answer Type - Reason (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Question Goal - Reasoning : Reason (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Question Type - Aggregation : Aggregation (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Question Type - Comparison : Comparative (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Comparison : Compare (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Comparison : Comparison (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Question Type - Comparison : Descriptive - Comparative (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Comparison : Second Order - Comparisons (The Classification of Research Questions)',\n",
       " 'Question Type - Contingency : Causal (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Question Type - Contingency : Causality (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Contingency : Causality - Causality (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Contingency : Causality - Comparative interactions (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Contingency : Causality-Comparative (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Contingency : Causality-Comparative Interaction (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Contingency : Third Order -Contingencies (The Classification of Research Questions)',\n",
       " 'Question Type - Counting : Base-Rate - Frequency and Distribution (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Counting : Count (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Counting : Count (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Counting : Count based (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Counting : Number Count (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Question Type - Counting : Question Content - Non-Factoid - counting (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Dependency : Clause (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Listing : List (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Listing : List Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Question Type - Listing : Listing (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Question Type - Listing : Resource List Typed (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Question Type - Listing : Resource List Untyped (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Question Type - Listing : Union (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Multiple Intentions : And/Or (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Multiple Intentions : Composition (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Multiple Intentions : Double Intent (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Multiple Intentions : Two Intention (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Negation : Negation (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Negation : Question Content - Non-Factoid - Negation (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Ranking : Question Content - Non-Factoid - Ranking (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Ranking : Ranking (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Relationship : Entity-relationship Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Question Type - Relationship : Relationship (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Relationship : UnknRel (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Superlative : Question Content - Non-Factoid - Minimum or Maximum (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Superlative : Question Content - Non-Factoid - Superlatives (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Superlative : Superlative/Comparative (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Superlative : Superlatives (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Temporal : Question Content - Non-Factoid - Temporal (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Temporal : Temporal Aggregators (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Temporal : Temporal Aspect (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Temporal : Trends over Time (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Research Focus - Analytic Modeling : Analytic Model (Writing good software engineering research papers)',\n",
       " 'Research Focus - Analytical Methods : Method for Analysis or Evaluation (Writing good software engineering research papers)',\n",
       " 'Research Focus - Development Methods : Method or Means of Development (Writing good software engineering research papers)',\n",
       " 'Research Focus - Empirical Modeling : Empirical Model (Writing good software engineering research papers)',\n",
       " 'Research Focus - Generalization : Generalization or Characterization (Writing good software engineering research papers)',\n",
       " 'Research Focus - Qualitative Modeling : Qualitative or Descriptive Model (Writing good software engineering research papers)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration = 1\n",
    "catalog_taxonomy, mapping = get_taxonomy_eval_data(iteration)\n",
    "print(f\"Length of catalog taxonomy: {len(catalog_taxonomy)}\")\n",
    "display(catalog_taxonomy)\n",
    "print(f\"Length of mapping: {len(mapping)}\")\n",
    "display(mapping)\n",
    "save_taxonomy_eval_data(iteration, catalog_taxonomy, mapping, extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of catalog taxonomy: 28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graph Representation - Single Fact',\n",
       " 'Graph Representation - Multi Fact',\n",
       " 'Answer Type - Named Entity',\n",
       " 'Answer Type - Description',\n",
       " 'Answer Type - Temporal',\n",
       " 'Answer Type - Quantitative',\n",
       " 'Answer Type - Boolean',\n",
       " 'Answer Type - Other Type',\n",
       " 'Answer Format - Simple',\n",
       " 'Answer Format - Explanatory',\n",
       " 'Answer Format - Enumerative',\n",
       " 'Answer Format - Other Format',\n",
       " 'Question Type - Negation',\n",
       " 'Question Type - Relationship',\n",
       " 'Question Type - Superlative',\n",
       " 'Question Type - Counting',\n",
       " 'Question Type - Ranking',\n",
       " 'Question Type - Comparison',\n",
       " 'Question Type - Multiple Intentions',\n",
       " 'Question Type - Temporal',\n",
       " 'Question Type - Aggregation',\n",
       " 'Answer Credibility - Objective',\n",
       " 'Answer Credibility - Subjective',\n",
       " 'Question Goal - Reasoning',\n",
       " 'Question Goal - Problem Solving',\n",
       " 'Question Goal - Problematization',\n",
       " 'Question Goal - Improvement',\n",
       " 'Question Goal - Prediction']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mapping: 157\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Answer Credibility - Objective : Accuracy (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Credibility - Objective : Evidence-Based (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Objective : Factual Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Credibility - Subjective : Debate (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Subjective : Experience (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Subjective : Knowledge (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Format - Enumerative : First Order - Properties (The Classification of Research Questions)',\n",
       " 'Answer Format - Enumerative : Instruction (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Format - Enumerative : List (Ripple Down Rules for question answering)',\n",
       " 'Answer Format - Enumerative : List Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Answer Format - Enumerative : Listing (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Resource List Typed (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Resource List Untyped (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Union (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Answer Type - Boolean : Affirm (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_3Term (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_MoreTuples (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Ask (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Boolean (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Answer Type - Boolean : Boolean (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Answer Type - Boolean : Boolean (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Answer Type - Boolean : Boolean (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Yes/No (A Comparative Study of Question Answering over Knowledge Bases)',\n",
       " 'Answer Type - Boolean : YesNo (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Description : Answer Type - Definition (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Description : Base-Rate - Descriptive-Process (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Answer Type - Description : Definition (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Description : Definition (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Description : Description (Learning Question Classifiers)',\n",
       " 'Answer Type - Description : Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Description : Description and Classification (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Answer Type - Description : Descriptive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Answer Type - Description : Theory-in-Use (Construction of Design Science Research Questions)',\n",
       " 'Answer Type - Description : Tool or Notation (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Abbreviation (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Actor (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Named Entity : Analytic Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Answer Type - Location (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Answer Type - Organization (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Answer Type - Person (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Answer Type - Title (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Design, Evaluation, or Analysis of a particular Instance (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Empirical Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Entities (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Entity (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Entity (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Named Entity : Generalization or Characterization (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Human (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Named Entity : Human (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Human (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Named Entity : Human Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Human Groups (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Location (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Named Entity : Location (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Location (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Named Entity : Location (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Location (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Named Entity : Location (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Method for Analysis or Evaluation (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Method or Means of Development (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Name (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Organization (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Organization (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Named Entity : Person (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Procedure or Technique (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Qualitative or Descriptive Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Social (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Type - Named Entity : Software System (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Named Entity : Specific Solution, Prototype, or Judgment (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Technology (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Other Type : Answer Type - Undefined (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Other Type : Fourth Order -Extra (The Classification of Research Questions)',\n",
       " 'Answer Type - Other Type : Miscellaneous (AT&T at TREC-8)',\n",
       " 'Answer Type - Other Type : Other (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Other Type : Other (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Other Type : Other Named Entity (AT&T at TREC-8)',\n",
       " 'Answer Type - Other Type : Unknown (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Quantitative : Answer Type - Distance (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Money (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Number (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Price (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Bibliometric Numbers (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Quantitative : Linear Measure (AT&T at TREC-8)',\n",
       " 'Answer Type - Quantitative : Many (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Many-Class (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Monetary Amount (AT&T at TREC-8)',\n",
       " 'Answer Type - Quantitative : Numeric (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Quantitative : Numeric Value (Learning Question Classifiers)',\n",
       " 'Answer Type - Quantitative : Quantification Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Type - Quantitative : Quantity (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Answer Type - Date (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Temporal : Answer Type - Time (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Temporal : Date (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Date (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Temporal : Date (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Temporal : Duration (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Month (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Temporal : Time (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Temporal : Time (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Temporal : Time (Linguistically Motivated Question Classification)',\n",
       " 'Graph Representation - Multi Fact : Combining (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Multi Fact : Multi Fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Multi Fact : Multi Facts (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Multi Fact : Question Content - Non-Factoid - sequence of facts (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Normal (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Single Fact : Normal (Ripple Down Rules for question answering)',\n",
       " 'Graph Representation - Single Fact : Question Content - Factoid (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Single Fact (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Single Fact : Single Fact (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Graph Representation - Single Fact : Single Fact with Type (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Single Fact : single fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Goal - Improvement : Design (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Prediction : Predictive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Question Goal - Problem Solving : Motivation - Problem Solving (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Problematization : Motivation - Gap Spotting (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Problematization : Motivation - Problematization (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Reasoning : Answer Type - Reason (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Question Goal - Reasoning : Reason (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Question Type - Aggregation : Aggregation (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Question Type - Comparison : Comparative (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Comparison : Compare (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Comparison : Comparison (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Question Type - Comparison : Descriptive - Comparative (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Comparison : Second Order - Comparisons (The Classification of Research Questions)',\n",
       " 'Question Type - Counting : Base-Rate - Frequency and Distribution (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Counting : Count (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Counting : Count (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Counting : Count based (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Counting : Number Count (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Question Type - Counting : Question Content - Non-Factoid - counting (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Multiple Intentions : And/Or (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Multiple Intentions : Composition (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Multiple Intentions : Double Intent (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Multiple Intentions : Two Intention (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Negation : Negation (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Negation : Question Content - Non-Factoid - Negation (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Ranking : Question Content - Non-Factoid - Ranking (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Ranking : Ranking (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Relationship : Causal (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Question Type - Relationship : Causality (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Relationship : Causality - Causality (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Relationship : Causality - Comparative interactions (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Relationship : Causality-Comparative (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Relationship : Causality-Comparative Interaction (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Type - Relationship : Clause (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Relationship : Entity-relationship Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Question Type - Relationship : Relationship (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Question Type - Relationship : Third Order -Contingencies (The Classification of Research Questions)',\n",
       " 'Question Type - Relationship : UnknRel (Ripple Down Rules for question answering)',\n",
       " 'Question Type - Superlative : Question Content - Non-Factoid - Minimum or Maximum (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Superlative : Question Content - Non-Factoid - Superlatives (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Superlative : Superlative/Comparative (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Question Type - Superlative : Superlatives (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Temporal : Question Content - Non-Factoid - Temporal (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Question Type - Temporal : Temporal Aggregators (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Question Type - Temporal : Temporal Aspect (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Type - Temporal : Trends over Time (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration = 2\n",
    "catalog_taxonomy, mapping = get_taxonomy_eval_data(iteration)\n",
    "print(f\"Length of catalog taxonomy: {len(catalog_taxonomy)}\")\n",
    "display(catalog_taxonomy)\n",
    "print(f\"Length of mapping: {len(mapping)}\")\n",
    "display(mapping)\n",
    "save_taxonomy_eval_data(iteration, catalog_taxonomy, mapping, extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of catalog taxonomy: 37\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Graph Representation - Single Fact',\n",
       " 'Graph Representation - Multi Fact',\n",
       " 'Answer Type - Named Entity',\n",
       " 'Answer Type - Description',\n",
       " 'Answer Type - Temporal',\n",
       " 'Answer Type - Quantitative',\n",
       " 'Answer Type - Boolean',\n",
       " 'Answer Type - Other Type',\n",
       " 'Condition Type - Named Entity',\n",
       " 'Condition Type - Description',\n",
       " 'Condition Type - Temporal',\n",
       " 'Condition Type - Quantitative',\n",
       " 'Condition Type - Other Type',\n",
       " 'Answer Format - Simple',\n",
       " 'Answer Format - Explanatory',\n",
       " 'Answer Format - Enumerative',\n",
       " 'Answer Format - Other Format',\n",
       " 'Retrieval Operation - Basic',\n",
       " 'Retrieval Operation - Negation',\n",
       " 'Retrieval Operation - Relationship',\n",
       " 'Retrieval Operation - Superlative',\n",
       " 'Retrieval Operation - Counting',\n",
       " 'Retrieval Operation - Ranking',\n",
       " 'Retrieval Operation - Comparison',\n",
       " 'Retrieval Operation - Aggregation',\n",
       " 'Intention Count - Single Intention',\n",
       " 'Intention Count - Multiple Intentions',\n",
       " 'Answer Credibility - Normative',\n",
       " 'Answer Credibility - Objective',\n",
       " 'Answer Credibility - Subjective',\n",
       " 'Question Goal - Information Lookup',\n",
       " 'Question Goal - Reasoning',\n",
       " 'Question Goal - Problem Solving',\n",
       " 'Question Goal - Problematization',\n",
       " 'Question Goal - Improvement',\n",
       " 'Question Goal - Other Goal',\n",
       " 'Question Goal - Prediction']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of mapping: 231\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Answer Credibility - Objective : Accuracy (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Credibility - Objective : Evidence-Based (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Objective : Factual Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Credibility - Subjective : Debate (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Subjective : Experience (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Credibility - Subjective : Knowledge (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Format - Enumerative : First Order - Properties (The Classification of Research Questions)',\n",
       " 'Answer Format - Enumerative : Instruction (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Answer Format - Enumerative : List (Ripple Down Rules for question answering)',\n",
       " 'Answer Format - Enumerative : List Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Answer Format - Enumerative : Listing (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Resource List Typed (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Resource List Untyped (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Format - Enumerative : Union (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Answer Type - Boolean : Affirm (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_3Term (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Affirm_MoreTuples (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Boolean : Ask (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Boolean (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Answer Type - Boolean : Boolean (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Answer Type - Boolean : Boolean (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Answer Type - Boolean : Boolean (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Boolean : Yes/No (A Comparative Study of Question Answering over Knowledge Bases)',\n",
       " 'Answer Type - Boolean : YesNo (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Description : Answer Type - Definition (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Description : Base-Rate - Descriptive-Process (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Answer Type - Description : Definition (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Description : Definition (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Description : Description (Learning Question Classifiers)',\n",
       " 'Answer Type - Description : Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Description : Description and Classification (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Answer Type - Description : Descriptive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Answer Type - Description : Tool or Notation (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Abbreviation (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Actor (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Named Entity : Analytic Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Answer Type - Location (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Answer Type - Organization (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Answer Type - Person (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Design, Evaluation, or Analysis of a particular Instance (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Empirical Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Entities (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Entity (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Entity (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Named Entity : Generalization or Characterization (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Human (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Named Entity : Human (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Human (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Named Entity : Human Description (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Human Groups (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Location (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Named Entity : Location (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Location (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Named Entity : Location (Learning Question Classifiers)',\n",
       " 'Answer Type - Named Entity : Location (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Named Entity : Location (Linguistically Motivated Question Classification)',\n",
       " 'Answer Type - Named Entity : Method for Analysis or Evaluation (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Method or Means of Development (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Name (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Named Entity : Organization (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Organization (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Named Entity : Person (AT&T at TREC-8)',\n",
       " 'Answer Type - Named Entity : Procedure or Technique (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Qualitative or Descriptive Model (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Social (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Answer Type - Named Entity : Software System (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Named Entity : Specific Solution, Prototype, or Judgment (Writing good software engineering research papers)',\n",
       " 'Answer Type - Named Entity : Technology (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Answer Type - Other Type : Answer Type - Undefined (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Other Type : Fourth Order -Extra (The Classification of Research Questions)',\n",
       " 'Answer Type - Other Type : Miscellaneous (AT&T at TREC-8)',\n",
       " 'Answer Type - Other Type : Other (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Other Type : Other (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Other Type : Other Named Entity (AT&T at TREC-8)',\n",
       " 'Answer Type - Other Type : Unknown (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Quantitative : Answer Type - Distance (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Money (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Number (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Answer Type - Price (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Quantitative : Bibliometric Numbers (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Quantitative : Linear Measure (AT&T at TREC-8)',\n",
       " 'Answer Type - Quantitative : Many (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Many-Class (Ripple Down Rules for question answering)',\n",
       " 'Answer Type - Quantitative : Monetary Amount (AT&T at TREC-8)',\n",
       " 'Answer Type - Quantitative : Numeric (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Quantitative : Numeric Value (Learning Question Classifiers)',\n",
       " 'Answer Type - Quantitative : Quantification Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Answer Type - Quantitative : Quantity (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Answer Type - Date (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Temporal : Answer Type - Time (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Answer Type - Temporal : Date (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Date (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Answer Type - Temporal : Date (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Answer Type - Temporal : Duration (AT&T at TREC-8)',\n",
       " 'Answer Type - Temporal : Month (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Temporal : Time (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Answer Type - Temporal : Time (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Answer Type - Temporal : Time (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Description : Answer Type - Definition (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Description : Base-Rate - Descriptive-Process (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Condition Type - Description : Definition (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Description : Definition (Ripple Down Rules for question answering)',\n",
       " 'Condition Type - Description : Description (Learning Question Classifiers)',\n",
       " 'Condition Type - Description : Description (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Description : Description and Classification (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Condition Type - Description : Descriptive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Condition Type - Description : Theory-in-Use (Construction of Design Science Research Questions)',\n",
       " 'Condition Type - Description : Tool or Notation (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Abbreviation (Learning Question Classifiers)',\n",
       " 'Condition Type - Named Entity : Actor (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Condition Type - Named Entity : Analytic Model (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Answer Type - Location (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Named Entity : Answer Type - Organization (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Named Entity : Answer Type - Person (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Named Entity : Answer Type - Title (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Named Entity : Design, Evaluation, or Analysis of a particular Instance (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Empirical Model (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Entities (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Named Entity : Entity (Learning Question Classifiers)',\n",
       " 'Condition Type - Named Entity : Entity (Ripple Down Rules for question answering)',\n",
       " 'Condition Type - Named Entity : Generalization or Characterization (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Human (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Condition Type - Named Entity : Human (Learning Question Classifiers)',\n",
       " 'Condition Type - Named Entity : Human (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Named Entity : Human Description (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Named Entity : Human Groups (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Named Entity : Location (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Condition Type - Named Entity : Location (AT&T at TREC-8)',\n",
       " 'Condition Type - Named Entity : Location (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Condition Type - Named Entity : Location (Learning Question Classifiers)',\n",
       " 'Condition Type - Named Entity : Location (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Named Entity : Location (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Named Entity : Method for Analysis or Evaluation (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Method or Means of Development (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Name (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Named Entity : Organization (AT&T at TREC-8)',\n",
       " 'Condition Type - Named Entity : Organization (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Condition Type - Named Entity : Person (AT&T at TREC-8)',\n",
       " 'Condition Type - Named Entity : Procedure or Technique (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Qualitative or Descriptive Model (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Social (A Taxonomy for Classifying Questions Asked in Social Question and Answering)',\n",
       " 'Condition Type - Named Entity : Software System (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Condition Type - Named Entity : Specific Solution, Prototype, or Judgment (Writing good software engineering research papers)',\n",
       " 'Condition Type - Named Entity : Technology (The Future of Empirical Methods in Software Engineering Research)',\n",
       " 'Condition Type - Other Type : Answer Type - Undefined (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Other Type : Fourth Order -Extra (The Classification of Research Questions)',\n",
       " 'Condition Type - Other Type : Miscellaneous (AT&T at TREC-8)',\n",
       " 'Condition Type - Other Type : Other (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Condition Type - Other Type : Other (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Other Type : Other Named Entity (AT&T at TREC-8)',\n",
       " 'Condition Type - Other Type : Unknown (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Condition Type - Quantitative : Answer Type - Distance (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Quantitative : Answer Type - Money (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Quantitative : Answer Type - Number (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Quantitative : Answer Type - Price (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Quantitative : Bibliometric Numbers (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Condition Type - Quantitative : Linear Measure (AT&T at TREC-8)',\n",
       " 'Condition Type - Quantitative : Many (Ripple Down Rules for question answering)',\n",
       " 'Condition Type - Quantitative : Many-Class (Ripple Down Rules for question answering)',\n",
       " 'Condition Type - Quantitative : Monetary Amount (AT&T at TREC-8)',\n",
       " 'Condition Type - Quantitative : Numeric (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Quantitative : Numeric Value (Learning Question Classifiers)',\n",
       " 'Condition Type - Quantitative : Quantification Questions (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Condition Type - Quantitative : Quantity (AT&T at TREC-8)',\n",
       " 'Condition Type - Temporal : Answer Type - Date (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Temporal : Answer Type - Time (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Condition Type - Temporal : Date (AT&T at TREC-8)',\n",
       " 'Condition Type - Temporal : Date (Hybrid-SQuAD: Hybrid Scholarly Question Answering Dataset)',\n",
       " 'Condition Type - Temporal : Date (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Condition Type - Temporal : Duration (AT&T at TREC-8)',\n",
       " 'Condition Type - Temporal : Month (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Condition Type - Temporal : Question Content - Non-Factoid - Temporal (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Condition Type - Temporal : Temporal Aggregators (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Condition Type - Temporal : Temporal Aspect (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Condition Type - Temporal : Time (A Rule-based Question Answering System for Reading Comprehension Tests)',\n",
       " 'Condition Type - Temporal : Time (Learning foci for Question Answering over Topic Maps)',\n",
       " 'Condition Type - Temporal : Time (Linguistically Motivated Question Classification)',\n",
       " 'Condition Type - Temporal : Trends over Time (Divide and Conquer the EmpiRE: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering)',\n",
       " 'Graph Representation - Multi Fact : Combining (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Multi Fact : Multi Fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Multi Fact : Multi Facts (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Multi Fact : Question Content - Non-Factoid - sequence of facts (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Normal (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Graph Representation - Single Fact : Normal (Ripple Down Rules for question answering)',\n",
       " 'Graph Representation - Single Fact : Question Content - Factoid (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Graph Representation - Single Fact : Single Fact (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Graph Representation - Single Fact : Single Fact (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Graph Representation - Single Fact : Single Fact with Type (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Graph Representation - Single Fact : single fact (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Intention Count - Multiple Intentions : And/Or (Ripple Down Rules for question answering)',\n",
       " 'Intention Count - Multiple Intentions : Composition (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Intention Count - Multiple Intentions : Double Intent (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Intention Count - Multiple Intentions : Two Intention (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Question Goal - Improvement : Design (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Question Goal - Prediction : Predictive (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Question Goal - Problem Solving : Motivation - Problem Solving (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Problematization : Motivation - Gap Spotting (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Problematization : Motivation - Problematization (Construction of Design Science Research Questions)',\n",
       " 'Question Goal - Reasoning : Answer Type - Reason (The Structure and Performance of an Open-Domain Question Answering System)',\n",
       " 'Question Goal - Reasoning : Reason (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Retrieval Operation - Aggregation : Aggregation (Question Answering on Scholarly Knowledge Graphs)',\n",
       " 'Retrieval Operation - Comparison : Comparative (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Retrieval Operation - Comparison : Compare (Ripple Down Rules for question answering)',\n",
       " 'Retrieval Operation - Comparison : Comparison (A Non-Factoid Question-Answering Taxonomy)',\n",
       " 'Retrieval Operation - Comparison : Descriptive - Comparative (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Retrieval Operation - Comparison : Second Order - Comparisons (The Classification of Research Questions)',\n",
       " 'Retrieval Operation - Counting : Base-Rate - Frequency and Distribution (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Retrieval Operation - Counting : Count (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Retrieval Operation - Counting : Count (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Retrieval Operation - Counting : Count based (10th Question Answering over Linked Data (QALD) Challenge)',\n",
       " 'Retrieval Operation - Counting : Number Count (What is in the KGQA Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs)',\n",
       " 'Retrieval Operation - Counting : Question Content - Non-Factoid - counting (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Retrieval Operation - Negation : Negation (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Retrieval Operation - Negation : Question Content - Non-Factoid - Negation (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Retrieval Operation - Ranking : Question Content - Non-Factoid - Ranking (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Retrieval Operation - Ranking : Ranking (LC-QuAD 2.0: A Large Dataset for Complex Question Answering over Wikidata and Dbpedia)',\n",
       " 'Retrieval Operation - Relationship : Causal (Types of research questions: descriptive, predictive, or causal)',\n",
       " 'Retrieval Operation - Relationship : Causality (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Retrieval Operation - Relationship : Causality - Causality (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Retrieval Operation - Relationship : Causality - Comparative interactions (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Retrieval Operation - Relationship : Causality-Comparative (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Retrieval Operation - Relationship : Causality-Comparative Interaction (Selecting Empirical Methods for Software Engineering Research)',\n",
       " 'Retrieval Operation - Relationship : Clause (Ripple Down Rules for question answering)',\n",
       " 'Retrieval Operation - Relationship : Entity-relationship Questions (Large-scale Simple Question Answering with Memory Networks)',\n",
       " 'Retrieval Operation - Relationship : Relationship (Formulation of Research Question - Stepwise Approach)',\n",
       " 'Retrieval Operation - Relationship : Third Order -Contingencies (The Classification of Research Questions)',\n",
       " 'Retrieval Operation - Relationship : UnknRel (Ripple Down Rules for question answering)',\n",
       " 'Retrieval Operation - Superlative : Question Content - Non-Factoid - Minimum or Maximum (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Retrieval Operation - Superlative : Question Content - Non-Factoid - Superlatives (The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge)',\n",
       " 'Retrieval Operation - Superlative : Superlative/Comparative (DBLP-QuAD: A Question Answering Dataset over the DBLP Scholarly Knowledge Graph)',\n",
       " 'Retrieval Operation - Superlative : Superlatives (10th Question Answering over Linked Data (QALD) Challenge)']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iteration = 3\n",
    "catalog_taxonomy, mapping = get_taxonomy_eval_data(iteration)\n",
    "print(f\"Length of catalog taxonomy: {len(catalog_taxonomy)}\")\n",
    "display(catalog_taxonomy)\n",
    "print(f\"Length of mapping: {len(mapping)}\")\n",
    "display(mapping)\n",
    "save_taxonomy_eval_data(iteration, catalog_taxonomy, mapping, extractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created formatted file with summary: orthogonality_iteration1_formatted.xlsx\n",
      "Created formatted file with summary: orthogonality_iteration2_formatted.xlsx\n",
      "Created formatted file with summary: orthogonality_iteration3_formatted.xlsx\n"
     ]
    }
   ],
   "source": [
    "# The following function has been generated with the help of ChatGPT\n",
    "# and is used to create the orthogonality matrix Excel file for each \n",
    "# taxonomy iteration. \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell, xl_range\n",
    "\n",
    "# --- Helper Functions to Extract Categories from Taxonomy ---\n",
    "\n",
    "def get_subcategories(category, parent_prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Recursively extract subcategory names. The full name is built by concatenating\n",
    "    the parent name with the subcategory name.\n",
    "    \"\"\"\n",
    "    subcats = []\n",
    "    for subcat in category.get(\"categories\", []):\n",
    "        # Build a full name that includes hierarchy: parent > child\n",
    "        subcat_full_name = f\"{parent_prefix} > {subcat.get('name', '')}\"\n",
    "        subcats.append(subcat_full_name)\n",
    "        subcats.extend(get_subcategories(subcat, parent_prefix=subcat_full_name))\n",
    "    return subcats\n",
    "\n",
    "def extract_categories_from_taxonomy(tax_it):\n",
    "    \"\"\"\n",
    "    For one taxonomy iteration (a dictionary from taxonomy_increments),\n",
    "    extract all category names. We prefix each category with its dimension name.\n",
    "    \"\"\"\n",
    "    categories = []\n",
    "    for dim in tax_it.get(\"taxonomy\", []):\n",
    "        dim_name = dim.get(\"name\", \"\")\n",
    "        for cat in dim.get(\"categories\", []):\n",
    "            cat_full_name = f\"{dim_name}: {cat.get('name', '')}\"\n",
    "            categories.append(cat_full_name)\n",
    "            categories.extend(get_subcategories(cat, parent_prefix=cat_full_name))\n",
    "    return categories\n",
    "\n",
    "def build_hierarchy_levels(cat_str):\n",
    "    \"\"\"\n",
    "    Convert a category string (e.g. 'Dim1: Cat1 > Sub1 > Sub2')\n",
    "    into a list of hierarchy levels.\n",
    "    \"\"\"\n",
    "    parts = cat_str.split(\": \")\n",
    "    if len(parts) == 1:\n",
    "        levels = [parts[0]]\n",
    "    else:\n",
    "        dimension = parts[0]\n",
    "        rest = parts[1]\n",
    "        subparts = rest.split(\" > \")\n",
    "        levels = [dimension] + subparts\n",
    "    return levels\n",
    "\n",
    "def count_leaf_categories_in_taxonomy(tax_it):\n",
    "    \"\"\"\n",
    "    Count the number of leaf categories (categories with no subcategories)\n",
    "    in a given taxonomy iteration.\n",
    "    \"\"\"\n",
    "    def count_leaf(cat):\n",
    "        # If a category has no subcategories, count it as a leaf.\n",
    "        if not cat.get(\"categories\"):\n",
    "            return 1\n",
    "        # Otherwise, count the leaves in its subcategories.\n",
    "        total = 0\n",
    "        for sub in cat.get(\"categories\", []):\n",
    "            total += count_leaf(sub)\n",
    "        return total\n",
    "\n",
    "    leaf_count = 0\n",
    "    for dim in tax_it.get(\"taxonomy\", []):\n",
    "        for cat in dim.get(\"categories\", []):\n",
    "            leaf_count += count_leaf(cat)\n",
    "    return leaf_count\n",
    "\n",
    "# --- Function to Generate Formatted Orthogonality Matrix Excel File with Summary ---\n",
    "\n",
    "def generate_orthogonality_excel(tax_it):\n",
    "    \"\"\"\n",
    "    For one taxonomy iteration (tax_it), create an Excel file with a hierarchical\n",
    "    header (for both rows and columns), an orthogonality matrix with:\n",
    "      - Off-diagonals filled with 0,\n",
    "      - Diagonals left empty,\n",
    "      - Conditional formatting that colors cells green if value==0,\n",
    "        red if value==1, and grey if blank.\n",
    "    Also adds metric calculations to the right of the matrix and a summary of\n",
    "    key metrics in the bottom left of the sheet.\n",
    "    \"\"\"\n",
    "    iteration = tax_it.get(\"iteration\")\n",
    "    if iteration is None:\n",
    "        print(\"No iteration number found, skipping.\")\n",
    "        return\n",
    "\n",
    "    # Extract category names (each as a string with hierarchy information)\n",
    "    cat_names = extract_categories_from_taxonomy(tax_it)\n",
    "    if not cat_names:\n",
    "        print(f\"No categories found for taxonomy iteration {iteration}. Skipping file creation.\")\n",
    "        return\n",
    "\n",
    "    # Build hierarchical levels for each category.\n",
    "    cat_levels = [build_hierarchy_levels(name) for name in cat_names]\n",
    "    # Determine the maximum depth (number of header rows/columns needed)\n",
    "    header_depth = max(len(levels) for levels in cat_levels)\n",
    "    # Pad all lists so that each has the same length.\n",
    "    cat_levels = [levels + [\"\"]*(header_depth - len(levels)) for levels in cat_levels]\n",
    "\n",
    "    num_cats = len(cat_names)\n",
    "\n",
    "    # Define starting row and column for the matrix area.\n",
    "    start_row = header_depth  # where matrix rows begin\n",
    "    start_col = header_depth  # where matrix columns begin\n",
    "\n",
    "    # Create a new Excel workbook and worksheet.\n",
    "    output_filename = f\"orthogonality_iteration{iteration}.xlsx\"\n",
    "    workbook = xlsxwriter.Workbook(output_filename)\n",
    "    worksheet = workbook.add_worksheet(\"Orthogonality Matrix\")\n",
    "\n",
    "    # Define formats.\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True, 'align': 'center', 'valign': 'vcenter', 'border': 1\n",
    "    })\n",
    "    cell_format = workbook.add_format({\n",
    "        'align': 'center', 'valign': 'vcenter', 'border': 1\n",
    "    })\n",
    "    metric_format = workbook.add_format({\n",
    "        'bold': True, 'bg_color': '#D3D3D3', 'align': 'center',\n",
    "        'valign': 'vcenter', 'border': 1\n",
    "    })\n",
    "    summary_label_format = workbook.add_format({\n",
    "        'bold': True, 'align': 'left'\n",
    "    })\n",
    "    summary_value_format = workbook.add_format({\n",
    "        'align': 'right'\n",
    "    })\n",
    "    summary_percent_format = workbook.add_format({\n",
    "        'num_format': '0.00%', 'align': 'right'\n",
    "    })\n",
    "    # Formats for conditional formatting (colors can be adjusted)\n",
    "    green_format = workbook.add_format({\n",
    "        'bg_color': '#C6EFCE', 'border': 1, 'align': 'center', 'valign': 'vcenter'\n",
    "    })\n",
    "    red_format = workbook.add_format({\n",
    "        'bg_color': '#FFC7CE', 'border': 1, 'align': 'center', 'valign': 'vcenter'\n",
    "    })\n",
    "    grey_format = workbook.add_format({\n",
    "        'bg_color': '#D9D9D9', 'border': 1, 'align': 'center', 'valign': 'vcenter'\n",
    "    })\n",
    "\n",
    "    # --- Write Column Headers (Hierarchical) ---\n",
    "    for level in range(header_depth):\n",
    "        j = 0\n",
    "        while j < num_cats:\n",
    "            this_value = cat_levels[j][level]\n",
    "            merge_start = j\n",
    "            while j < num_cats and cat_levels[j][level] == this_value:\n",
    "                j += 1\n",
    "            merge_end = j - 1\n",
    "            first_cell_row = level\n",
    "            first_cell_col = start_col + merge_start\n",
    "            last_cell_row = level\n",
    "            last_cell_col = start_col + merge_end\n",
    "            if merge_start == merge_end:\n",
    "                worksheet.write(first_cell_row, first_cell_col, this_value, header_format)\n",
    "            else:\n",
    "                worksheet.merge_range(first_cell_row, first_cell_col,\n",
    "                                      last_cell_row, last_cell_col,\n",
    "                                      this_value, header_format)\n",
    "    # Label for the metric column.\n",
    "    metric_label_col = start_col + num_cats\n",
    "    worksheet.merge_range(0, metric_label_col, header_depth-1, metric_label_col,\n",
    "                          \"Row Metric\", header_format)\n",
    "\n",
    "    # --- Write Row Headers (Hierarchical) ---\n",
    "    for level in range(header_depth):\n",
    "        i = 0\n",
    "        while i < num_cats:\n",
    "            this_value = cat_levels[i][level]\n",
    "            merge_start = i\n",
    "            while i < num_cats and cat_levels[i][level] == this_value:\n",
    "                i += 1\n",
    "            merge_end = i - 1\n",
    "            first_cell_row = start_row + merge_start\n",
    "            first_cell_col = level\n",
    "            last_cell_row = start_row + merge_end\n",
    "            last_cell_col = level\n",
    "            if merge_start == merge_end:\n",
    "                worksheet.write(first_cell_row, first_cell_col, this_value, header_format)\n",
    "            else:\n",
    "                worksheet.merge_range(first_cell_row, first_cell_col,\n",
    "                                      last_cell_row, last_cell_col,\n",
    "                                      this_value, header_format)\n",
    "    # Label for the metric row (bottom left)\n",
    "    metric_label_row = start_row + num_cats\n",
    "    worksheet.merge_range(metric_label_row, 0, metric_label_row, header_depth-1,\n",
    "                          \"Column Metric\", header_format)\n",
    "\n",
    "    # --- Fill the Matrix Cells ---\n",
    "    for i in range(num_cats):\n",
    "        for j in range(num_cats):\n",
    "            cell_row = start_row + i\n",
    "            cell_col = start_col + j\n",
    "            if i == j:\n",
    "                # Diagonal cell remains empty.\n",
    "                worksheet.write(cell_row, cell_col, \"\", cell_format)\n",
    "            else:\n",
    "                worksheet.write(cell_row, cell_col, 0, cell_format)\n",
    "\n",
    "    # --- Add Conditional Formatting to Matrix Cells ---\n",
    "    matrix_range = xl_range(start_row, start_col, start_row + num_cats - 1, start_col + num_cats - 1)\n",
    "    # Green for cells with 0.\n",
    "    worksheet.conditional_format(matrix_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 0,\n",
    "        'format': green_format\n",
    "    })\n",
    "    # Red for cells with 1.\n",
    "    worksheet.conditional_format(matrix_range, {\n",
    "        'type': 'cell',\n",
    "        'criteria': '==',\n",
    "        'value': 1,\n",
    "        'format': red_format\n",
    "    })\n",
    "    # Grey for blank cells.\n",
    "    worksheet.conditional_format(matrix_range, {\n",
    "        'type': 'blanks',\n",
    "        'format': grey_format\n",
    "    })\n",
    "\n",
    "    # --- Add Metrics Calculations Next to the Matrix ---\n",
    "    # Row Metric (sum each row).\n",
    "    for i in range(num_cats):\n",
    "        cell_row = start_row + i\n",
    "        start_cell = xl_rowcol_to_cell(cell_row, start_col)\n",
    "        end_cell = xl_rowcol_to_cell(cell_row, start_col + num_cats - 1)\n",
    "        formula = f\"=SUM({start_cell}:{end_cell})\"\n",
    "        worksheet.write_formula(cell_row, start_col + num_cats, formula, metric_format)\n",
    "\n",
    "    # Column Metric (sum each column).\n",
    "    for j in range(num_cats):\n",
    "        cell_col = start_col + j\n",
    "        start_cell = xl_rowcol_to_cell(start_row, cell_col)\n",
    "        end_cell = xl_rowcol_to_cell(start_row + num_cats - 1, cell_col)\n",
    "        formula = f\"=SUM({start_cell}:{end_cell})\"\n",
    "        worksheet.write_formula(start_row + num_cats, cell_col, formula, metric_format)\n",
    "    # Grand total in the bottom-right corner.\n",
    "    total_range_start = xl_rowcol_to_cell(start_row + num_cats, start_col)\n",
    "    total_range_end = xl_rowcol_to_cell(start_row + num_cats, start_col + num_cats - 1)\n",
    "    worksheet.write_formula(start_row + num_cats, start_col + num_cats,\n",
    "                              f\"=SUM({total_range_start}:{total_range_end})\", metric_format)\n",
    "\n",
    "    # --- Add Summary of Metrics at the Bottom Left ---\n",
    "    # Compute summary values based on the taxonomy and matrix location.\n",
    "    leaf_count = count_leaf_categories_in_taxonomy(tax_it)\n",
    "    total_fields = leaf_count * leaf_count          # Matrix is leaf_count x leaf_count.\n",
    "    relevant_fields = total_fields - leaf_count       # Excluding diagonal (disabled) cells.\n",
    "    disabled_fields = leaf_count\n",
    "\n",
    "    # We'll place the summary a few rows below the matrix.\n",
    "    summary_start_row = start_row + num_cats + 3\n",
    "    summary_col_label = 0  # Labels in column A.\n",
    "    summary_col_value = 1  # Values in column B.\n",
    "\n",
    "    # Row 0: Leaf count.\n",
    "    worksheet.write(summary_start_row + 0, summary_col_label, \"Leaf count\", summary_label_format)\n",
    "    worksheet.write(summary_start_row + 0, summary_col_value, leaf_count, summary_value_format)\n",
    "\n",
    "    # Row 1: Total fields.\n",
    "    worksheet.write(summary_start_row + 1, summary_col_label, \"Total fields\", summary_label_format)\n",
    "    worksheet.write(summary_start_row + 1, summary_col_value, total_fields, summary_value_format)\n",
    "\n",
    "    # Row 2: Relevant fields.\n",
    "    worksheet.write(summary_start_row + 2, summary_col_label, \"Relevant fields\", summary_label_format)\n",
    "    worksheet.write(summary_start_row + 2, summary_col_value, relevant_fields, summary_value_format)\n",
    "\n",
    "    # Row 3: Disable field.\n",
    "    worksheet.write(summary_start_row + 3, summary_col_label, \"Disable field\", summary_label_format)\n",
    "    worksheet.write(summary_start_row + 3, summary_col_value, disabled_fields, summary_value_format)\n",
    "\n",
    "    # Row 4: Overlaps (using a formula that sums the entire matrix range).\n",
    "    worksheet.write(summary_start_row + 4, summary_col_label, \"Overlaps\", summary_label_format)\n",
    "    overlaps_formula = f\"=SUM({matrix_range})\"\n",
    "    worksheet.write_formula(summary_start_row + 4, summary_col_value, overlaps_formula, summary_value_format)\n",
    "\n",
    "    # Row 5: Orthogonal = Total fields - Disabled fields - Overlaps.\n",
    "    worksheet.write(summary_start_row + 5, summary_col_label, \"Orthogonal\", summary_label_format)\n",
    "    # Get the cell reference for Overlaps (which is in column B of summary row 4).\n",
    "    overlaps_cell = xl_rowcol_to_cell(summary_start_row + 4, summary_col_value, row_abs=True, col_abs=True)\n",
    "    orthogonal_formula = f\"={total_fields}-{disabled_fields}-{overlaps_cell}\"\n",
    "    worksheet.write_formula(summary_start_row + 5, summary_col_value, orthogonal_formula, summary_value_format)\n",
    "\n",
    "    # Row 6: Score = Overlaps / (Total fields - Disabled fields).\n",
    "    worksheet.write(summary_start_row + 6, summary_col_label, \"Score\", summary_label_format)\n",
    "    score_formula = f\"={overlaps_cell}/({total_fields}-{disabled_fields})\"\n",
    "    worksheet.write_formula(summary_start_row + 6, summary_col_value, score_formula, summary_value_format)\n",
    "\n",
    "    # Row 7: Percentage = 1 - Score.\n",
    "    worksheet.write(summary_start_row + 7, summary_col_label, \"Percentage\", summary_label_format)\n",
    "    # Get the cell reference for Score (in column B of summary row 6).\n",
    "    score_cell = xl_rowcol_to_cell(summary_start_row + 6, summary_col_value, row_abs=True, col_abs=True)\n",
    "    percentage_formula = f\"=1-{score_cell}\"\n",
    "    worksheet.write_formula(summary_start_row + 7, summary_col_value, percentage_formula, summary_percent_format)\n",
    "\n",
    "    # --- Set Column Widths (optional) ---\n",
    "    # Adjust widths for header columns.\n",
    "    for col in range(header_depth):\n",
    "        worksheet.set_column(col, col, 20)\n",
    "    # Adjust widths for the matrix and metric columns.\n",
    "    for col in range(start_col, start_col + num_cats + 1):\n",
    "        worksheet.set_column(col, col, 12)\n",
    "    # Adjust widths for the summary columns.\n",
    "    worksheet.set_column(summary_col_label, summary_col_label, 15)\n",
    "    worksheet.set_column(summary_col_value, summary_col_value, 15)\n",
    "\n",
    "    workbook.close()\n",
    "    print(f\"Created formatted file with summary: {output_filename}\")\n",
    "\n",
    "\n",
    "# --- Loop Over Taxonomy Iterations and Create Files ---\n",
    "for tax_it in taxonomy_increments:\n",
    "    generate_orthogonality_excel(tax_it)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
