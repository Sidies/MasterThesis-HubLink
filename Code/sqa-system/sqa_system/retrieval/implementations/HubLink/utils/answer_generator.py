from typing import List, Optional, Tuple
import re
import ast
from pydantic import BaseModel, Field
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate

from sqa_system.core.data.models.context import ContextType
from sqa_system.core.data.models.context import Context
from sqa_system.core.data.models import Triple
from sqa_system.core.data.models.retrieval_answer import RetrievalAnswer
from sqa_system.core.language_model.base.llm_adapter import LLMAdapter
from sqa_system.core.language_model.prompt_provider import PromptProvider
from sqa_system.knowledge_base.knowledge_graph.storage.utils.graph_converter import GraphConverter
from sqa_system.knowledge_base.knowledge_graph.storage.base.knowledge_graph import KnowledgeGraph
from sqa_system.core.logging.logging import get_logger

from ..models import (
    HubLinkSettings,
    EntityWithDirection,
    HubPath
)
from ..utils.hub_source_handler import SourceDocumentSummary


logger = get_logger(__name__)


class HubAnswer(BaseModel):
    """
    Data object representing an answer generated for a hub during the retrieval process.

    Attributes:
        source_identifier (str): Identifies the source of information (e.g., DOI).
        source_name (str): The name of the source (e.g., paper title).
        hub_answer (str): The partial answer generated for the hub.
        relevant_paths (List[HubPath]): The relevant HubPaths used for the answer.
        relevant_source_data (Optional[SourceDocumentSummary]): Additional source data used as context.
    """
    source_identifier: str = Field(...,
                                   description="Identifies the source of information")
    source_name: str = Field(..., description="The name of the source.")
    hub_answer: str = Field(..., description="The partial answer from the hub.")
    relevant_paths: List[HubPath] = Field(
        ...,
        description="The relevant HubPaths used to generate the answer.")
    relevant_source_data: Optional[SourceDocumentSummary] = Field(
        None,
        description="The relevant source data used as additional context.")


class AnswerGenerator:
    """
    Generates answers for hubs and consolidates them into a final answer using an LLM.

    This class is responsible for:
    - Generating partial answers for each hub using relevant paths and source data.
    - Aggregating partial answers into a final answer using a language model.
    - Filtering and transforming contexts and triples for answer generation.

    Args:
        graph (KnowledgeGraph): The knowledge graph instance.
        llm (LLMAdapter): The LLM adapter for answer generation.
    """

    def __init__(self,
                 graph: KnowledgeGraph,
                 llm: LLMAdapter):
        self.graph = graph
        self.llm = llm
        self.prompt_provider = PromptProvider()
        self.graph_converter = GraphConverter(
            graph=self.graph, llm_adapter=self.llm)

    def get_final_answer(self,
                         question: str,
                         hub_answers: List[HubAnswer],
                         settings: HubLinkSettings) -> Optional[RetrievalAnswer]:
        """
        Generates the final answer from the partial answers of the hubs.

        Args:
            question (str): The input question.
            hub_answers (List[HubAnswer]): The partial answers from the hubs.
            settings (HubLinkSettings): The settings for the retrieval process.

        Returns:
            Optional[RetrievalAnswer]: The final answer generated by the LLM.
                Will only be returned if the LLM has enough information to generate
                a final answer. Otherwise, None is returned.
        """

        # Get the prompt for the answer generation
        prompt_text, _, _ = self.prompt_provider.get_prompt(
            "novel_retriever/final_answer_generation_prompt.yaml")

        # Prepare the partial answers for the LLM
        partial_answers = ""
        for index, answer in enumerate(hub_answers):
            partial_answers += f"[{index+1}] {answer.hub_answer}\n"

        # Prepare the Langchain elements
        parser = StrOutputParser()
        prompt = PromptTemplate(
            template=prompt_text,
            input_variables=["question", "partial_answers"],
        )

        # For debugging purposes
        final_prompt = prompt.invoke(
            {"question": question, "partial_answers": partial_answers})
        logger.debug(f"Prompt for LLM: {final_prompt}")

        # Run the Query
        chain = prompt | self.llm.llm | parser
        response = chain.invoke(
            {"question": question, "partial_answers": partial_answers})

        logger.debug(f"The partial answers for the hub are: {partial_answers}")
        logger.debug(f"The response from the LLM is: {response}")

        # Check if the LLM has enough information to generate a final answer
        if "insufficient" in response.lower():
            return None

        # Add the source information to the final answer
        response += "\n\n"
        for index, hub_answer in enumerate(hub_answers):
            if f"[{index+1}]" in response:
                response += f"[{index+1}] Source: {hub_answer.source_name}, "
                response += f"{hub_answer.source_identifier}\n"
            else:
                logger.debug(
                    "The LLM did not include the source %s in the final answer.",
                    hub_answer.source_identifier)

        if settings.return_source_data_as_context:
            contexts = self._get_relevant_source_contexts_from_hubs(
                hub_answers, question, settings.filter_output_context)
        else:
            contexts = self._get_relevant_triples_from_hubs(
                hub_answers, question, settings.filter_output_context)
        return RetrievalAnswer(contexts=contexts, retriever_answer=response)

    def get_partial_answer_for_hub(
            self,
            relevant_paths: List[HubPath],
            hub_root_entity: EntityWithDirection,
            question: str,
            source_document_data: SourceDocumentSummary = None) -> Optional[HubAnswer]:
        """
        Returns partial answers for a hub. 
        If the LLM does not have enough information to generate a partial answer, 
        None is returned.

        Args:
            relevant_paths (List[HubPath]): The relevant paths to the hub.
            hub_root_entity (EntitywithDirection): The root entity of the hub.
            question (str): The question to ask the LLM.
            source_document_data (SourceDocumentSummary): The source document data
                to be used as additional context for the LLM.

        Returns:
            Optional[HubAnswer]: The partial answer generated by the LLM.
                Will only be returned if the LLM has enough information to generate
                a partial answer. Otherwise, None is returned.
        """
        common_information = self._prepare_common_data_for_hub(
            hub_root_entity=hub_root_entity,
            source_document_data=source_document_data)

        # Get the prompt for the partial answer generation
        prompt_text, _, _ = self.prompt_provider.get_prompt(
            "novel_retriever/partial_answer_generation_prompt.yaml")

        # Prepare the texts for the LLM
        context_texts = ""
        scores = []
        for index, path in enumerate(relevant_paths):
            path_as_string = Triple.convert_list_to_string(
                path.path
            )
            context_texts += (f"({index+1}) **Description**: {path.path_text}; "
                      f"**Source**: {path_as_string}\n")
            scores.append(path.score)

        logger.debug(f"The relevant paths for the hub are: {context_texts}")
        logger.debug(f"The Embedded texts are: {str([path.embedded_text for path in relevant_paths])}")
        logger.debug(f"The scores for the relevant paths are: {scores}")

        # Run the partial answer query
        response = self._run_partial_answer_query(
            texts=context_texts,
            question=question,
            common_information=common_information,
            prompt_text=prompt_text
        )

        logger.debug(f"The response from the LLM is: {response}")

        # Check if the LLM has enough information to generate a partial answer
        if "insufficient" in response.lower():
            return None
        return HubAnswer(
            source_identifier=self._get_source_identifier_of_hub_entity(
                hub_root_entity),
            source_name=hub_root_entity.entity.text,
            hub_answer=response,
            relevant_paths=relevant_paths,
            relevant_source_data=source_document_data
        )

    def _prepare_common_data_for_hub(self,
                                     hub_root_entity: EntityWithDirection,
                                     source_document_data: SourceDocumentSummary) -> str:
        """
        This function prepares common data from the hub that is added as additional
        information in the prompt for the LLM.
        
        It first adds the title of the hub and then adds the path from the topic to the hub
        (if a topic was provided). Finally, it adds the linked data if it is provided.
        
        Args:
            hub_root_entity (EntityWithDirection): The root entity of the hub.
            source_document_data (SourceDocumentSummary): The source document data
                to be used as additional context for the LLM.
        
        Returns:
            str: The common information to be used as additional context for the LLM.
        """
        common_information = ("- **Paper**: All context is coming from a scholarly "
                              f"publication with the title: {hub_root_entity.entity.text}.\n")

        # We get the whole path from the topic to the hub (if a topic was provided)
        # The whole path is converted to text and used as additional information
        path_from_topic_to_hub = hub_root_entity.path_from_topic
        if path_from_topic_to_hub and len(path_from_topic_to_hub) > 0:
            common_information = "- **Additional Info**:" + self.graph_converter.path_to_text(
                path_from_topic_to_hub) + "\n"

        # Then, if linked data is provided, we add the linked data as well
        if source_document_data:
            common_information += "- **Vector Store Data**: \n"
            for index, data in enumerate(source_document_data):
                common_information += f"-- {index}: " + str(data) + "\n"
        return common_information

    def _run_partial_answer_query(self,
                                  texts: str,
                                  question: str,
                                  common_information: str,
                                  prompt_text: str) -> Optional[str]:
        """
        This method asks the LLM to generate a partial answer.

        Args:
            texts (str): The texts to be used as context for the LLM.
            question (str): The question to be asked to the LLM.
            common_information (str): Additional information to be 
                used as context for the LLM.
            prompt_text (str): The prompt text to be used for the LLM.

        Returns:
            Optional[str]: The partial answer generated by the LLM.
                Will only be returned if the LLM has enough information 
                to generate.
        """
        parser = StrOutputParser()
        prompt = PromptTemplate(
            template=prompt_text,
            input_variables=["texts", "question", "common_information"],
        )

        final_prompt = prompt.invoke(
            {"texts": texts, "question": question, "common_information": common_information})
        logger.debug(f"Prompt for LLM: {final_prompt}")

        llm_runnable = self.llm.llm
        if llm_runnable is None:
            raise ValueError("LLM has not been initialized correctly")

        chain = prompt | llm_runnable | parser
        response = chain.invoke(
            {"texts": texts, "question": question, "common_information": common_information})
        logger.debug(f"Response from LLM: {response}")
        return response

    def _get_relevant_source_contexts_from_hubs(self,
                                                hub_answers: List[HubAnswer],
                                                question: str,
                                                filter_output_contexts: bool) -> List[Context]:
        """
        Transforms the source data from the hubs into contexts.

        Args:
            hub_answers (List[HubAnswer]): The hub answers to be transformed.
            question (str): The question to be asked to the LLM.
            filter_output_contexts (bool): Whether to filter the output contexts
                based on the reponse of the LLM.

        Returns:
            List[Context]: The contexts generated from the source data.
        """
        # Get the paths of the hubs
        contexts: List[Context] = []
        for hub_answer in hub_answers:
            source_contexts = hub_answer.relevant_source_data.contexts
            # Sort by score
            source_contexts.sort(key=lambda x: x.score, reverse=True)
            if filter_output_contexts:
                filtered_contexts = self._filter_document_contexts_by_llm(
                    question, hub_answer.hub_answer, source_contexts)
                contexts.extend(filtered_contexts)
            else:
                contexts.extend(source_contexts)

        return contexts

    def _get_relevant_triples_from_hubs(self,
                                        hub_answers: List[HubAnswer],
                                        question: str,
                                        filter_output_triples: bool) -> List[Context]:
        """
        Extracts the relevant triples from each hub based on the relevant paths.
        The golden triples are sorted by the score of the relevant path.

        Args:
            hub_answers (List[HubAnswer]): The hub answers to be transformed.
            question (str): The question used for relevance assessment by the LLM.
            filter_output_triples (bool): Whether to filter the output triples
                using an LLM.

        Returns:
            List[Context]: The relevant triples for each hub as contexts.
        """
        all_triples_with_score: List[Tuple[Triple, float]] = []

        for hub_answer in hub_answers:
            hub_triples: List[Tuple[Triple, float]] = []
            for path in hub_answer.relevant_paths:
                for triple in path.path:
                    hub_triples.append((triple, path.score))

            # Sort the triples by the score
            hub_triples.sort(key=lambda x: x[1], reverse=True)

            if filter_output_triples:
                filtered_triples = self._filter_triple_contexts_by_llm(
                    hub_triples, question, hub_answer.hub_answer, [])
                if filtered_triples is None:
                    all_triples_with_score.extend(hub_triples)
                else:
                    all_triples_with_score.extend(filtered_triples)
            else:
                all_triples_with_score.extend(hub_triples)

        # Sort the triples by the score
        all_triples_with_score.sort(key=lambda x: x[1], reverse=True)

        relevant_contexts = []
        triples_visited = set()
        for triple, _ in all_triples_with_score:
            if triple in triples_visited:
                continue
            relevant_contexts.append(
                Context(
                    context_type=ContextType.KG,
                    text=str(triple)
                )
            )
            triples_visited.add(triple)

        return relevant_contexts

    def _filter_triple_contexts_by_llm(self,
                                       triples_with_score: List[Tuple[Triple, int]],
                                       question: str,
                                       answer: str,
                                       triples_to_exclude: List[Triple]) -> List[Tuple[Triple, int]] | None:
        """
        This function prompts an LLM to filter the triples based on the question.
        Only those triples are kept, where the LLM finds it relevant to the question.

        Args:
            triples_with_score (List[Tuple[Triple, int]]): The triples to be filtered.
            question (str): The question that is used for the relevance assessment of 
                the filtering process.
            answer (str): The answer that is used for the relevance assessment of
                the filtering process.
            triples_to_exclude (List[Triple]): The triples that should be excluded
                from the filtering process.

        Returns:
            List[Tuple[Triple, int]] | None: The filtered triples. If the process 
                failed, None is returned.
        """
        prompt_text, _, _ = self.prompt_provider.get_prompt(
            "novel_retriever/get_final_triples_prompt.yaml")
        parser = StrOutputParser()
        prompt = PromptTemplate(
            template=prompt_text,
            input_variables=["texts", "question", "answer"],
        )

        context_text = self._prepare_triples_for_llm(
            [triple for triple, _ in triples_with_score], triples_to_exclude)

        if context_text is None:
            return []

        final_prompt = prompt.invoke(
            {"question": question, "contexts": context_text, "answer": answer})
        logger.debug(f"Relevant Triples Prompt for LLM: {final_prompt}")

        chain = prompt | self.llm.llm | parser
        response = chain.invoke(
            {"question": question, "contexts": context_text, "answer": answer})
        logger.debug(f"Response from LLM for relevant triples: {response}")

        relevant_ids = self._extract_id_list(response)
        if len(relevant_ids) > len(triples_with_score):
            logger.debug(
                "LLM output contains more relevant indices than triples: %s", response)
            return None
        relevant_triples = []
        for index in relevant_ids:
            if 1 <= index <= len(triples_with_score):
                relevant_triples.append(triples_with_score[index - 1])
            else:
                logger.debug(
                    "Invalid index %s in LLM output: %s", index, response)
                return None

        return relevant_triples

    def _prepare_triples_for_llm(self,
                                 triples: List[Triple],
                                 triples_to_exclude: List[Triple]) -> str | None:
        """
        Converts the triples to a string format that is readable by the LLM.

        Args:
            triples (List[Triple]): The triples to be converted.
            triples_to_exclude (List[Triple]): The triples that should be excluded
                from the conversion process.

        Returns:
            str | None: The string representation of the triples. If no triples
                are left after the exclusion, None is returned.
        """
        context_texts = []
        for index, triple in enumerate(triples):
            if triple in triples_to_exclude:
                continue
            triple_as_string = f"({triple.entity_subject.text}, {triple.predicate}, {triple.entity_object.text})"

            context_texts.append(f"{index+1}. {triple_as_string}\n")

        if len(context_texts) == 0:
            return None

        return ''.join(context_texts)

    def _filter_document_contexts_by_llm(self,
                                         question: str,
                                         answer: str,
                                         contexts: List[Context]) -> List[Context]:
        """
        This function prompts an LLM to filter the source contexts based on the question.
        Only those contexts are kept, where the LLM finds it relevant to the question.

        Args:
            question (str): The question that is used for the relevance assessment of
                the filtering process.
            answer (str): The answer that is used for the relevance assessment of
                the filtering process.
            contexts (List[Context]): The contexts to be filtered.

        Returns:
            List[Context]: The filtered contexts. If the process failed, an empty list
                is returned.
        """

        prompt_text, _, _ = self.prompt_provider.get_prompt(
            "novel_retriever/get_final_documents_prompt.yaml")
        parser = StrOutputParser()
        prompt = PromptTemplate(
            template=prompt_text,
            input_variables=["question", "answer", "contexts"],
        )

        context_texts = []
        for index, context in enumerate(contexts):
            context_texts.append(f"{index+1}. {str(context)}\n")

        if len(context_texts) == 0:
            return []

        final_prompt = prompt.invoke(
            {"question": question, "contexts": context_texts}, {"answer": answer})
        logger.debug(f"Context Filter Prompt for LLM: {final_prompt}")

        chain = prompt | self.llm.llm | parser
        response = chain.invoke(
            {"question": question, "contexts": context_texts})
        logger.debug(f"Response from LLM for relevant contexts: {response}")

        relevant_ids = self._extract_id_list(response)
        relevant_contexts = []
        for index in relevant_ids:
            if 1 <= index <= len(contexts):
                relevant_contexts.append(contexts[index - 1])
            else:
                logger.warning(
                    "Invalid index %s in LLM output: %s", index, response)

        return relevant_contexts

    def _extract_id_list(self, llm_output: str) -> List[int]:
        """
        Parser function to extract a list of integers from the LLM output.

        Args:
            llm_output (str): The output from the LLM.

        Returns:
            List[int]: A list of integers extracted from the LLM output.
        """
        list_match = re.search(r'\[[^\]]*\]', llm_output)

        if list_match:
            list_str = list_match.group(0)
            try:
                # Attempt to safely evaluate the extracted string as a Python literal
                potential_list = ast.literal_eval(list_str)
                if isinstance(potential_list, list) and all(isinstance(item, int) for item in potential_list):
                    return potential_list
            except (ValueError, SyntaxError):
                pass

            # Fallback: manually extract all integers within the brackets
            return [int(num) for num in re.findall(r'\d+', list_str)]

        # If no brackets found, check if the entire output is just numbers separated by commas
        stripped_text = llm_output.strip()
        if re.fullmatch(r'(\d+\s*,\s*)*\d+', stripped_text):
            # The entire output is a comma-separated list of numbers
            return [int(num.strip()) for num in stripped_text.split(',')]

        # Fallback: find all integers anywhere in the text
        return [int(num) for num in re.findall(r'\d+', llm_output)]

    def _get_source_identifier_of_hub_entity(self, root_entity: EntityWithDirection) -> str:
        """
        Extracts the doi from the root entity of the hub.
        
        Args:
            root_entity (EntityWithDirection): The root entity of the hub.

        Returns:
            str: The doi of the root entity.
        """
        root_relations = self.graph.get_relations_of_head_entity(
            root_entity.entity)
        
        doi = ""
        for triple in root_relations:
            if "doi" in triple.predicate:
                doi = triple.entity_object.text
                break
        return doi
