{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook for Codereview 2\n",
    "\n",
    "_This notebook is a part of the code review 2._\n",
    "\n",
    "With this Notebook I want to introduce you to the system I have built.\n",
    "\n",
    "This notebook requires you to have a HuggingFace API key. You can get one by signing up at [HuggingFace](https://huggingface.co/join)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Content Overview\n",
    "\n",
    "The notebook is structured as follows:\n",
    "\n",
    "1. [Content Overview](#1.-Content-Overview)\n",
    "2. [Preparation](#2-preparation)\n",
    "3. [QA-Generation](#3-QA-Generation)\n",
    "4. [Retrieval](#4-retrieval)\n",
    "5. [Visualization](#5-visualize-the-results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preparation\n",
    "Before you can run the code, you need to do some preparation.\n",
    "\n",
    "1. Prepare a OpenAI API key. (The costs of running will be between 1-5 cents)\n",
    "2. Create a [Weights&Biases](https://wandb.ai/login) Account to additionally track the experiments on the Dashboard of Weights&Biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set the API Key for OpenAI. This key is stored on your local system using our SecretManager implementation which stores the key with an encryption key that is stored in your local system.\n",
    "\n",
    "Run the coded below and you should be prompted to enter the api key if it is not already added to the secretmanager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from sqa_system.core.data.secret_manager import SecretManager, EndpointType\n",
    "secret_manager = SecretManager()\n",
    "try:\n",
    "    api_key = secret_manager.get_api_key(EndpointType.HUGGINGFACE)\n",
    "    print(\"API Key found for HUGGINGFACE\")\n",
    "except ValueError as e:\n",
    "    print(\"No API key found for HUGGINGFACE\")\n",
    "    api_key = input(\"No API key found for HUGGINGFACE. Enter your API Key: \")\n",
    "    secret_manager.save_api_key(EndpointType.HUGGINGFACE, api_key)\n",
    "    print(\"API Key saved successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this code in a shared environment, it is also good practice to first check if the GPU is available and not currently in use. You can do this by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. QA-Generation\n",
    "\n",
    "In the OpenAI notebook, at this point the QA-Generation would be showcased. However, unfortunately this is not possible with OpenSource models. We tried almost all of the newer models available on Ollama and None was able to generate an answer that conforms to the JSON format required.\n",
    "\n",
    "We tried:\n",
    "- llama3.3:70b\n",
    "- phi4:14b\n",
    "- qwq:32b\n",
    "- llama3.1:8b\n",
    "- mistral:7b\n",
    "- qwen2.5:14b\n",
    "- qwen2.5:32b\n",
    "\n",
    "The highest model size we could try with the hardware that we have availalbe was 70b. I think the reason they are not returning is that the amount of parameters are not large enough to process the context window size that is required to process. As a result the following features are NOT working with open source models:\n",
    "1. QA-Generation\n",
    "2. Fulltext Extraction\n",
    "3. Evaluation with RAGAS Framework\n",
    "\n",
    "If you have ideas on how to solve this, your input is highly appreciated. For example you can look at our [Paper Content Extractor](../../sqa_system/core/data/extraction/paper_content_extractor.py) and [Ollama Adapter](../../sqa_system/core/language_model/implementations/ollama_llm_adapter.py).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless I want to explain how the configuration system works.\n",
    "\n",
    "A configuration is given as a json file. Every component in the framework has its own configuration files stored in the [data/configs](../../data/configs/) directory which are managed by the [ConfigurationManager](../../sqa_system/core/config/config_manager/implementations/) to distribute the configurations to the components.\n",
    "\n",
    "The json files are validated when load by the Configuration Manager, however some complex configurations are only validated at a later stage when the components are created. \n",
    "\n",
    "Furthermore, because it is hard to know which values are valid without experience with the system (and because it has gotten quite complex), I have created a CLI tool that can be used to generate configurations. This tool is located [here](../../sqa_system/app/cli_controller.py) and should be run from console. \n",
    "\n",
    "As you can see below, this is how a configuration for a local RDFLIB graph looks like. The RDFLIB graph is a graph that is stored in memory using the RDFLib library which we will use for demonstration purposes. The configuration tells the framework, which graph to use and how it should be created.\n",
    "\n",
    "For this codereview I have prepared a KG configuration for you. We will use this to create the graph and then generate questions and answers from it. This configuration is already prepared in the cache so using this **exact** configuration will not have any LLM calls during the generation of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    },
    "tags": [
     "preparation"
    ]
   },
   "outputs": [],
   "source": [
    "# For the reason mentioned above we are using the OpenAI configuration below for the KG creation. Because the extraction is fully cached, we are not actually calling the OpenAI API. However, for the initialization we still need a key which is why we will add a dummy key here.\n",
    "from sqa_system.core.data.secret_manager import SecretManager, EndpointType\n",
    "secret_manager = SecretManager()\n",
    "try:\n",
    "    api_key = secret_manager.get_api_key(EndpointType.OPENAI)\n",
    "    print(\"API Key found for OPENAI, but will not be used.\")\n",
    "except ValueError as e:\n",
    "    print(\"No API key found for OPENAI. Adding dummy...\")\n",
    "    secret_manager.save_api_key(EndpointType.OPENAI, \"DUMMY_KEY\")\n",
    "\n",
    "from sqa_system.core.config.models import KnowledgeGraphConfig\n",
    "graph_config = {\n",
    "    \"additional_params\": { # The building blocks that are used to generate the graph\n",
    "        \"building_blocks\": [\n",
    "            \"metadata\",\n",
    "            \"authors\",\n",
    "            \"publisher\",\n",
    "            \"venue\",\n",
    "            \"research_field\",\n",
    "            \"additional_fields\",\n",
    "            \"annotations\",\n",
    "            \"content\"\n",
    "        ]\n",
    "    },\n",
    "    \"graph_type\": \"local_rdflib\",   # We want to use a local graph using the RDFlib library\n",
    "    \"dataset_config\": {     # This is a inner configuration that describes the dataset that will be used for the graph creation\n",
    "        \"additional_params\": {},\n",
    "        \"file_name\": \"merged_ecsa_icsa.json\",   # This is the file that contains the dataset data\n",
    "        \"loader\": \"JsonPublicationLoader\",  # How the file will be loaded\n",
    "        \"loader_limit\": 30  # We limit the number of publications for demonstration.\n",
    "    },\n",
    "    \"extraction_llm\": {    # This is the configuration for the LLM that will be used for extracting data from the dataset\n",
    "            \"additional_params\": {},\n",
    "            \"endpoint\": \"OpenAI\",\n",
    "            \"name_model\": \"gpt-4o-mini\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": -1\n",
    "    },\n",
    "    \"extraction_context_size\": 2000\n",
    "}\n",
    "graph_config = KnowledgeGraphConfig.from_dict(graph_config)\n",
    "\n",
    "# Some default configs are already prepared which can be loaded from the respective configuration manager by name\n",
    "from sqa_system.core.config.models import LLMConfig\n",
    "llm_config = LLMConfig.from_dict(\n",
    "    {\n",
    "            \"additional_params\": {},\n",
    "            \"endpoint\": \"Ollama\",\n",
    "            \"name_model\": \"mistral\",\n",
    "            \"temperature\": 0.1,\n",
    "            \"max_tokens\": 4096\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the code above initialized the Knowledge Graph configuration object. We can now use this configuration to create the graph by passing it to the `KnowledgeGraphManager`. This manager is responsible for creating the graph and storing it for the session to allow other parts of the system to access it.\n",
    "\n",
    "The creation of the graph is a complex process including the steps:\n",
    "1. Loading the Dataset\n",
    "2. Extracting the Triples from the Dataset and tracing each triple to its origin\n",
    "3. Creating the Graph using the triples\n",
    "\n",
    "Because the creation is time consuming and if using a LLM like OpenAi can include costs, we cache each step using the [CacheManager](../../sqa_system/core/data/cache_manager.py) which stores the data in a SQLite database located in the [data/cache](../../data/cache) directory.\n",
    "\n",
    "As mentioned above we already cached the data necessary for this configuration which means that the creation of the graph <u>has no further costs</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we create the graph\n",
    "from sqa_system.knowledge_base.knowledge_graph.storage import KnowledgeGraphManager\n",
    "graph = KnowledgeGraphManager().get_item(graph_config) \n",
    "\n",
    "# We also load the LLM model using the LLMProvider which is responsible for creating and preparing\n",
    "# a LLM based on the configuration provided\n",
    "from sqa_system.core.language_model.llm_provider import LLMProvider\n",
    "llm_adapter = LLMProvider().get_llm_adapter(llm_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retrieval\n",
    "Now we start the experimentation. I have prepared several configurations for you to run and then visualize the results. We first have the HubLink Retrieval approach (our new approach) and then two baselines.\n",
    "\n",
    "#### HubLink Retriever\n",
    "Below I have prepared a configuration for you to run the HubLink Retrieval approach. This configuration is already indexed so the amount of LLM calls is minimized.\n",
    "\n",
    "The retriever has several parameters that can be configured as you can see in the configuration below. These configs allow to reproduce the experiments and to test different configurations of the retriever. Hyperparameter optimization can also be done using the `parameter_ranges` field in the configuration.\n",
    "\n",
    "Run the code below to prepare the configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.core.config.models import ExperimentConfig\n",
    "experiment_dict = {\n",
    "    \"additional_params\": {},\n",
    "    \"base_pipeline_config\": {\n",
    "        \"additional_params\": {},\n",
    "        \"pipes\": [\n",
    "            {\n",
    "                \"additional_params\": {\n",
    "                    \"embedding_config\": {\n",
    "                        \"additional_params\": {},\n",
    "                        \"endpoint\": \"HuggingFace\",\n",
    "                        \"name_model\": \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "                    },\n",
    "                    \"max_workers\": 8,\n",
    "                    \"number_of_hubs\": 5,\n",
    "                    \"top_paths_to_keep\": 10,\n",
    "                    \"run_indexing\": True,\n",
    "                    \"indexing_root_entity_types\": \"\",\n",
    "                    \"force_index_update\": False,\n",
    "                    \"max_hub_path_length\": 10,\n",
    "                    \"force_hub_update\": False,\n",
    "                    \"hub_types\": \"\",\n",
    "                    \"hub_edges\": -1,\n",
    "                    \"use_topic_if_given\": False,\n",
    "                    \"compare_hubs_with_same_hop_amount\": True,\n",
    "                    \"max_level\": 5,\n",
    "                    \"use_source_documents\": True,\n",
    "                    \"source_vector_store_config\": {\n",
    "                        \"additional_params\": {\n",
    "                            \"distance_metric\": \"l2\"\n",
    "                        },\n",
    "                        \"vector_store_type\": \"chroma\",\n",
    "                        \"chunking_strategy_config\": {\n",
    "                            \"additional_params\": {},\n",
    "                            \"chunking_strategy_type\": \"RecursiveCharacterChunkingStrategy\",\n",
    "                            \"chunk_size\": 500,\n",
    "                            \"chunk_overlap\": 0\n",
    "                        },\n",
    "                        \"embedding_config\": {\n",
    "                            \"additional_params\": {},\n",
    "                            \"endpoint\": \"HuggingFace\",\n",
    "                            \"name_model\": \"mixedbread-ai/mxbai-embed-large-v1\"\n",
    "                        },\n",
    "                        \"dataset_config\": {\n",
    "                            \"additional_params\": {},\n",
    "                            \"file_name\": \"merged_ecsa_icsa.json\",\n",
    "                            \"loader\": \"JsonPublicationLoader\",\n",
    "                            \"loader_limit\": 30\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"type\": \"kg_retrieval\",\n",
    "                \"retriever_type\": \"hublink\",\n",
    "                \"llm_config\": {\n",
    "                    \"additional_params\": {},\n",
    "                    \"endpoint\": \"Ollama\",\n",
    "                    \"name_model\": \"ollama run deepseek-r1:14b\",\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 4096\n",
    "                },\n",
    "                \"knowledge_graph_config\": {\n",
    "                    \"additional_params\": {\n",
    "                        \"building_blocks\": [\n",
    "                            \"metadata\",\n",
    "                            \"authors\",\n",
    "                            \"publisher\",\n",
    "                            \"venue\",\n",
    "                            \"research_field\",\n",
    "                            \"additional_fields\",\n",
    "                            \"annotations\",\n",
    "                            \"content\"\n",
    "                        ]\n",
    "                    },\n",
    "                    \"graph_type\": \"local_rdflib\",\n",
    "                    \"dataset_config\": {\n",
    "                        \"additional_params\": {},\n",
    "                        \"file_name\": \"merged_ecsa_icsa.json\",\n",
    "                        \"loader\": \"JsonPublicationLoader\",\n",
    "                        \"loader_limit\": 3\n",
    "                    },\n",
    "                    \"extraction_llm\": {\n",
    "                        \"additional_params\": {},\n",
    "                        \"endpoint\": \"OpenAI\",\n",
    "                        \"name_model\": \"gpt-4o-mini\",\n",
    "                        \"temperature\": 0.1,\n",
    "                        \"max_tokens\": -1\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"additional_params\": {},\n",
    "                \"type\": \"generation\",\n",
    "                \"llm_config\": {\n",
    "                    \"additional_params\": {},\n",
    "                    \"endpoint\": \"Ollama\",\n",
    "                    \"name_model\": \"ollama run deepseek-r1:14b\",\n",
    "                    \"temperature\": 0.1,\n",
    "                    \"max_tokens\": 4096\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"parameter_ranges\": [],\n",
    "    \"evaluators\": [\n",
    "                {\n",
    "                    \"name\": \"hit_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"hit_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"hit_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"hit_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"hit_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"hit_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"map_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"map_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"map_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"map_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"mrr_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"mrr_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"mrr_at_k\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"mrr_at_k\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"basic_score\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"basic_score\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"basic_score\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"basic_score\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"basic_score\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": -1,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"basic_score\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"basic_score\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": -1,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"basic_score\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"exact_match\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"exact_match\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"exact_match\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": 10,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"exact_match\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"exact_match\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": -1,\n",
    "                        \"context_type\": \"triple\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"exact_match\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"exact_match\",\n",
    "                    \"additional_params\": {\n",
    "                        \"k\": -1,\n",
    "                        \"context_type\": \"entity\"\n",
    "                    },\n",
    "                    \"evaluator_type\": \"exact_match\"\n",
    "                }\n",
    "    ],\n",
    "    \"qa_dataset\": {\n",
    "        \"additional_params\": {},\n",
    "        \"file_name\": \"question_answering_codereview.csv\",\n",
    "        \"loader\": \"CSVQALoader\",\n",
    "        \"loader_limit\": -1\n",
    "    }\n",
    "}\n",
    "experiment_config = ExperimentConfig.from_dict(experiment_dict)\n",
    "\n",
    "import os\n",
    "from sqa_system.core.data.file_path_manager import FilePathManager\n",
    "\n",
    "# Here we prepare the location where the results of the experiment will be stored\n",
    "file_path_manager = FilePathManager()\n",
    "current_directory = os.getcwd()\n",
    "folder_path = file_path_manager.combine_paths(current_directory, \"experiment_data_huggingface\")\n",
    "experiment_folder = file_path_manager.combine_paths(folder_path, experiment_config.base_pipeline_config.config_hash)\n",
    "print(f\"Results will be stored in {experiment_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the configuration to run the experiment. The experiment runner will use the configuration to create the retriever, load the QAPairs and run the experiment. \n",
    "\n",
    "The indexing process for this configuration has already been done and should be cached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.experimentation.experiment_runner import ExperimentRunner, ProgressHandler\n",
    "\n",
    "# We can now run the experiment using the ExperimentRunner\n",
    "runner = ExperimentRunner(\n",
    "    experiment_config=experiment_config,\n",
    "    results_folder_path=experiment_folder,\n",
    ")\n",
    "results = runner.run()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that the experiment finished, the results are stored in the experiment_data folder in the same folder of this notebook. This folder is generated by the experiment runner and contains 3 subfolders.\n",
    "1. configs: Contains the configuration(s) that have been used to run the experiment. This allows to check the configuration that was used for the experiment at any point at a later time. \n",
    "2. predictions: This includes the predictions stored as `.csv` files. Feel free to look at the file and review if anything is missing.\n",
    "3. visualizations: This folder contains visualizations done by the runner to get a feel for the performance of the retriever. These will not be the final visualizations but are helpful to get a quick overview of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: DocumentEmbed Retriever\n",
    "We need something to compare against so I also prepared two baselines for you. The first one is the DocumentEmbed Retriever. This retriever doesn't use a KG but instead directly embeds the fulltexts of the documents and stores them in a vector store. The retriever then uses the vector store to retrieve the most similar document to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.core.config.config_manager import ExperimentConfigManager\n",
    "experiment_config = ExperimentConfigManager().get_config_by_name(\"DocumentEmbed_Codereview_Huggingface\")\n",
    "experiment_folder = file_path_manager.combine_paths(folder_path, experiment_config.base_pipeline_config.config_hash)\n",
    "print(f\"Results will be stored in {experiment_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.experimentation.experiment_runner import ExperimentRunner\n",
    "\n",
    "# We can now run the experiment using the ExperimentRunner\n",
    "runner = ExperimentRunner(\n",
    "    experiment_config=experiment_config,\n",
    "    results_folder_path=experiment_folder,\n",
    ")\n",
    "results = runner.run()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline: TripleEmbed Retriever\n",
    "The second baseline is the TripleEmbed Retriever. This retriever uses the triples of the graph and embeds them. The retriever then uses the vector store to retrieve the most similar triple to the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.core.config.config_manager import ExperimentConfigManager\n",
    "experiment_config = ExperimentConfigManager().get_config_by_name(\"TripleEmbed_Codereview_Huggingface\")\n",
    "experiment_folder = file_path_manager.combine_paths(folder_path, experiment_config.base_pipeline_config.config_hash)\n",
    "print(f\"Results will be stored in {experiment_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.experimentation.experiment_runner import ExperimentRunner\n",
    "\n",
    "# We can now run the experiment using the ExperimentRunner\n",
    "runner = ExperimentRunner(\n",
    "    experiment_config=experiment_config,\n",
    "    results_folder_path=experiment_folder,\n",
    ")\n",
    "results = runner.run()\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Results\n",
    "We can also visualize the results using the `ExperimentVisualizer`. This visualizer provides several plots to compare the performances of the retrievers. It is provided with a folder path and searches the folder path for valid prediction files by crawling through the subfolders. It then matches the questions by their ID and the unique ID of the configuration to generate the plot.\n",
    "\n",
    "This is why you only see a long hash ID in the plot. For each configuration a unique hash is generated. This means that if we change even a single parameter in the config, its ID will change which makes it easy when comparing different configurations. It also makes sure that if we by chance run the same configuration twice, we can recognize this by the ID.\n",
    "\n",
    "To find out what ID constitutes to what configuration, we can look at the results folder `experiment_data` and find the configuration file that was used for the experiment.\n",
    "\n",
    "Looking at the results, it is expected that the baselines are close to the retrieval performance of our retriever. This is because of several reasons. \n",
    "1. The question types generated with the generator we used\n",
    "2. The low amount of data that we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqa_system.experimentation.utils.visualizer.experiment_visualizer import ExperimentVisualizer, PlotType\n",
    "visualizer = ExperimentVisualizer(\n",
    "    data_folder_path=folder_path,\n",
    "    should_print=True,\n",
    "    should_save_to_file=False,\n",
    ")\n",
    "visualizer.run(plots_to_generate=[PlotType.AVERAGE_METRICS_PER_CONFIG])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
