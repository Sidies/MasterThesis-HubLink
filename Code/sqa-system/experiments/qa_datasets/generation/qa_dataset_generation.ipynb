{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QA Generation\n",
    "\n",
    "In this notebook, the Question-Answering (QA) Dataset for the annotation data [merged_ecsa_icsa.json](../../../../data/external/merged_ecsa_icsa.json) is created.\n",
    "\n",
    "The creation was guided by a matrix consisting of two dimensions:\n",
    "1. **Use Case**\n",
    "    - **1**: The first use case reflects the current state-of-practice in scientific literature search. The researcher seeks additional details about the metadata of one or more papers. To find this information, the researcher provides the QA system with specific metadata information related to the papers he is interested in. In response, the QA system returns information on other metadata attributes of the papers rather than content information.\n",
    "    - **2**: In the second use case, the researcher seeks information about the content of one or more papers. In this use case, the researcher provides the QA system with metadata information about the papers and asks a question about their contents. The QA system is then expected to extract content information related to specific papers that conform to the metadata constraints provided.\n",
    "    - **3**: In the third use case, the researcher seeks information about metadata of one or more papers. In this use case, the researcher provides the QA system with content constraints about the papers and asks a question about the metadata of the paper. The QA system is then expected to extract metadata information related to the specific papers mentioned in the question.\n",
    "    - **4**: In the fourth use case, the researcher seeks information about the content of one or more papers. In this use case, the researcher provides the QA system with content information about the paper, and asks a question about the content of the paper. The QA system is then expected to extract content information related to the specific papers mentioned in the question.\n",
    "    - **5**: In the fifth use case, a researcher seeks information about the content of one or more papers. In this use case, the researcher provides the retriever with both metadata and content information about the papers and asks a question about the content of the paper. The retriever is then expected to extract content information related to the specific papers mentioned in the question.\n",
    "    - **6**: In the sixth use case, the researcher seeks information about metadata of one or more papers. In this specific use case, the researcher provides the retriever with both metadata and content information about the papers, such as the name of an evaluation method and the year of publication, and asks a question about the metadata of the papers. The retriever is then expected to extract metadata information related to the specific papers mentioned in the question.\n",
    "\n",
    "2. **Retrieval Operation Classification**\n",
    "    - **Basic**: Classifies those questions where the retriever is required to just find one ore more facts in the Knowledge Graph and use those to provide the answer without further processing.\n",
    "    - **Aggregation**: Classifies those questions where the retriever is required to quantitatively or qualitatively aggregate the information in the Knowledge Graph to answer the question.\n",
    "    - **Comparative**: Classifies those questions where the retriever is required to compare two or more pieces of information in the Knowledge Graph to answer the question.\n",
    "    - **Ranking**: Classifies those questions where the retriever is required to rank the information in the Knowledge Graph to answer the question.\n",
    "    - **Counting**: Classifies those questions where the retriever is required to count the number of occurrences of a certain information in the Knowledge Graph to answer the question.\n",
    "    - **Superlative**: Classifies those questions where the retriever is required to identify the most or least of a certain information in the Knowledge Graph to answer the question.\n",
    "    - **Relationship**: Classifies questions where the retriever must identify any type of interconnection or reliance between pieces of information in the Knowledge Graph. Essentially, it captures all scenarios where one piece of data is influenced by, contingent upon, or systematically linked to another.\n",
    "    - **Negation**: Classifies those questions where the retriever is required to negate the information in the Knowledge Graph to answer the question.\n",
    "\n",
    "### How to Read this File\n",
    "\n",
    "For each prepared question template (see [here](../templates.md)) we prepare the parameters for the clustering or subgraph construction strategies below. From each of the generated questions, we then selected those questions that we considered to already have a high quality and include no hallucination while conforming to the indended template, use case, and retrieval operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "from sqa_system.knowledge_base.knowledge_graph.storage import KnowledgeGraphManager\n",
    "from sqa_system.core.language_model.llm_provider import LLMProvider\n",
    "from sqa_system.core.data.models import QAPair\n",
    "from sqa_system.app.cli.cli_progress_handler import ProgressHandler\n",
    "from sqa_system.core.config.models import LLMConfig, KnowledgeGraphConfig, EmbeddingConfig\n",
    "\n",
    "# Initialize the Generators\n",
    "from sqa_system.qa_generator.strategies import (\n",
    "    FromTopicEntityGenerator, FromTopicEntityGeneratorOptions, GenerationOptions)\n",
    "from sqa_system.qa_generator.strategies import (\n",
    "    PaperComparisonGenerator, PaperComparisonGeneratorOptions)\n",
    "from sqa_system.qa_generator.strategies.clustering_strategy.cluster_based_question_generator import (\n",
    "    ClusterBasedQuestionGenerator, \n",
    "    ClusterGeneratorOptions, \n",
    "    AdditionalInformationRestriction,\n",
    "    ClusterStrategyOptions\n",
    ")\n",
    "\n",
    "\n",
    "# Prepare the Progress Handler, which we are going to disable because of compatibility issues\n",
    "# with Jupyter Notebooks\n",
    "progress_handler = ProgressHandler()\n",
    "progress_handler.disable()\n",
    "\n",
    "# Prepare Knowledge Graph\n",
    "kg_config = KnowledgeGraphConfig.from_dict({\n",
    "    \"additional_params\": {\n",
    "        \"contribution_building_blocks\": {\n",
    "            \"Classifications_2\": [\n",
    "                \"paper_class\",\n",
    "                \"research_level\",\n",
    "                \"all_research_objects\",\n",
    "                \"validity\",\n",
    "                \"evidence\"\n",
    "            ]\n",
    "        },\n",
    "        \"force_cache_update\": True,\n",
    "        \"force_publication_update\": False,\n",
    "        \"subgraph_root_entity_id\": \"R659055\",\n",
    "        \"orkg_base_url\": \"https://sandbox.orkg.org\"\n",
    "    },\n",
    "    \"graph_type\": \"orkg\",\n",
    "    \"dataset_config\": {\n",
    "        \"name\": \"merged_ecsa.json_jsonpublicationloader_limit-1\",\n",
    "        \"additional_params\": {},\n",
    "        \"file_name\": \"merged_ecsa_icsa.json\",\n",
    "        \"loader\": \"JsonPublicationLoader\",\n",
    "        \"loader_limit\": -1\n",
    "    },\n",
    "    \"extraction_llm\": {\n",
    "        \"name\": \"openai_gpt-4o-mini_tmp0.0_maxt-1\",\n",
    "        \"additional_params\": {},\n",
    "        \"endpoint\": \"OpenAI\",\n",
    "        \"name_model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"max_tokens\": -1\n",
    "    },\n",
    "    \"extraction_context_size\": 4000,\n",
    "    \"chunk_repetitions\": 2\n",
    "})\n",
    "graph = KnowledgeGraphManager().get_item(kg_config)\n",
    "\n",
    "# Prepare the Research Field Topic Entity\n",
    "research_field = graph.get_entity_by_id(\"R659055\")\n",
    "\n",
    "# Prepare Language Model\n",
    "gpt_4o_mini_config = LLMConfig.from_dict({\n",
    "    \"endpoint\": \"OpenAI\",\n",
    "    \"name_model\": \"gpt-4o-mini\",\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": -1\n",
    "})\n",
    "gpt_4o_mini = LLMProvider().get_llm_adapter(gpt_4o_mini_config)\n",
    "\n",
    "gpt_4o_config = LLMConfig.from_dict({\n",
    "    \"endpoint\": \"OpenAI\",\n",
    "    \"name_model\": \"gpt-4o\",\n",
    "    \"temperature\": 0.0,\n",
    "    \"max_tokens\": -1\n",
    "})\n",
    "gpt_4o = LLMProvider().get_llm_adapter(gpt_4o_config)\n",
    "\n",
    "gpt_o3_mini_config = LLMConfig.from_dict({\n",
    "    \"endpoint\": \"OpenAI\",\n",
    "    \"name_model\": \"o3-mini\",\n",
    "    \"temperature\": None,\n",
    "    \"max_tokens\": -1,\n",
    "    \"reasoning_effort\": \"low\"\n",
    "})\n",
    "gpt_o3_mini = LLMProvider().get_llm_adapter(gpt_o3_mini_config)\n",
    "\n",
    "embedding_config = EmbeddingConfig.from_dict({\n",
    "    \"name\": \"openai_text-embedding-3-small\",\n",
    "    \"additional_params\": {},\n",
    "    \"endpoint\": \"OpenAI\",\n",
    "    \"name_model\": \"text-embedding-3-small\"\n",
    "})\n",
    "\n",
    "def print_qa_pairs(qa_pairs: list[QAPair]):\n",
    "    if not qa_pairs:\n",
    "        print(\"No QA pairs generated\")\n",
    "    for qa_pair in qa_pairs:\n",
    "        print(f\"Question: {qa_pair.question}\")\n",
    "        print(f\"Answer: {qa_pair.golden_answer}\")\n",
    "        print(f\"Golden Triples: {qa_pair.golden_triples}\")\n",
    "        print(f\"Hops: {qa_pair.hops}\")\n",
    "        print(f\"Topic Entity: {qa_pair.topic_entity_value}\")\n",
    "        df = pd.DataFrame([qa_pair.model_dump()])\n",
    "        print(f\"CSV: \\n {df.to_csv(index=False)}\")\n",
    "        print(\"------------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "qa_strategy = FromTopicEntityGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "        topic_entity=research_field,\n",
    "    ),     \n",
    "    options=GenerationOptions(\n",
    "        template_text=\"In which venue has the paper '[paper_title]' been published?\",\n",
    "        additional_requirements=[\n",
    "            \"The generated question should include the title of the paper.\",\n",
    "            \"The context should only include the triple that contains the venue of the paper\",\n",
    "        ],\n",
    "        validate_contexts=False,\n",
    "        convert_path_to_text=False,\n",
    "        classify_questions=False,\n",
    "    )     \n",
    ")\n",
    "qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "qa_strategy = FromTopicEntityGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "        topic_entity=graph.get_random_publication(),\n",
    "    ),     \n",
    "    options=GenerationOptions(\n",
    "        template_text=\"In which venue has the paper '[paper_title]' been published?\",\n",
    "        additional_requirements=[\n",
    "            \"The generated question should include the title of the paper.\",\n",
    "            \"The context should only include the triple that contains the venue of the paper\",\n",
    "        ],\n",
    "        validate_contexts=False,\n",
    "        convert_path_to_text=False,\n",
    "        classify_questions=False,\n",
    "    )     \n",
    ")\n",
    "qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Who are the authors of the paper '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the authors of the paper.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Who are the authors of the paper '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the authors of the paper.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "\n",
    "qa_pairs.extend(ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the replication package link of the paper '[paper_title]'?\",\n",
    "            additional_requirements=[],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P168010\",\n",
    "        restriction_text=\"Replication Package Link\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate())\n",
    "\n",
    "qa_strategy = FromTopicEntityGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "        topic_entity=graph.get_entity_by_id(\"R870141\"), # We manually selected a publication that includes a replication package link\n",
    "    ),     \n",
    "    options=GenerationOptions(\n",
    "        template_text=\"What is the replication package link of the paper '[paper_title]'?\",\n",
    "        additional_requirements=[\n",
    "            \"The generated question should include the title of the paper.\",\n",
    "            \"The context should only include the triple of replication package link.\",\n",
    "        ],\n",
    "        validate_contexts=False,\n",
    "        convert_path_to_text=False,\n",
    "        classify_questions=False,\n",
    "    )     \n",
    ")\n",
    "qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "qa_pairs.extend(ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have been published by the author [author name] in the year [publication year]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "qa_pairs.extend(ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers have the research level [research level]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162008\",\n",
    "        restriction_text=\"research level\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many publications have been published by the author [author name]?\",\n",
    "            additional_requirements=[],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many publications have the paper class with the name [paper class name]?\",\n",
    "            additional_requirements=[],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have the paper class [paper class] ranked by their publication year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the paper class and publication year of the paper\",\n",
    "                \"The answer should be a list of publication titles in chronological order\",\n",
    "                \"Ensure that the list is ordered correctly based on the publication year in descending order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have been published by the author [author name] ranked by their publication year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the authors and publication year of the paper\",\n",
    "                \"The answer should be a list of publication titles in chronological order\",\n",
    "                \"Ensure that the list is ordered correctly based on the publication year in descending order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = PaperComparisonGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        comparison_options=PaperComparisonGeneratorOptions(\n",
    "            first_publication=graph.get_random_publication(),\n",
    "            second_publication=graph.get_random_publication(),\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"In what publication year has the publication '[paper title 1]' been published in comparison to the publication '[paper title 2]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the publication years of the papers.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = PaperComparisonGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        comparison_options=PaperComparisonGeneratorOptions(\n",
    "            first_publication=graph.get_random_publication(),\n",
    "            second_publication=graph.get_random_publication(),\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the paper class of the paper '[paper title 1]' in comparison to the publication '[paper title 2]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the paper classes of the papers.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which author has published the most publications with the paper class [paper class name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples in your context\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"authors\",\n",
    "                split_clusters=True,\n",
    "            )\n",
    "        ],\n",
    "        only_use_cluster_with_most_triples=True\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the paper class that the author [author name] has published the most?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the research object and the sub-property.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of papers that have been published by the author [author name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the research object and the sub-property.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of paper classes that have been published by the author [author name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the research object and the sub-property.\",\n",
    "                \"First list the amount of times the entity was used in the year\",\n",
    "                \"Then give a final statement about the proportion\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\",\n",
    "                information_value_restriction=\"True\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Does the paper '[paper_title]' use tool support?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the tool support of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"Just give a concise answer to the question.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Does the paper '[paper_title]' use tool support?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the tool support of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"Just give a concise answer to the question.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation method used in the paper '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that specifically mention the evaluation method of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"The answer should be similar to the following: 'The evaluation method used in the paper is [evaluation method]'.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation method used in the paper '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that specifically mention the evaluation method of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"The answer should be similar to the following: 'The evaluation method used in the paper is [evaluation method]'.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Does the paper '[paper_title]' have a replication package?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the replication package of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"Just give a concise answer to the question.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Does the paper '[paper_title]' have a replication package?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the replication package of the paper.\",\n",
    "                \"Do not use any IDs in your answer. Do not make any assumptions beyond the given triples.\",\n",
    "                \"Just give a concise answer to the question.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field,\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What are the evaluation methods of the publication '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain the evaluation methods of the paper\",\n",
    "                \"Evaluation Methods are encoded in the triples of the format (R870531:Evaluation Method Entity, Name, [Evaluation Method Name])\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R871346\"), # A publication with multiple evaluation methods\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What are the evaluation methods of the publication '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain the evaluation methods of the paper\",\n",
    "                \"Evaluation Methods are encoded in the triples of the format (R870531:Evaluation Method Entity, Name, [Evaluation Method Name])\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field,\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What are the threats to validity of the paper '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication(),\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What are the threats to validity of the paper '[paper_title]'?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(4):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field,\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods does the paper with the title '[paper title]' have?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"You can only use triples of the form: (Evaluation, Evaluation method, [method name])\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication(),\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods does the paper with the title '[paper title]' have?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"You can only use triples of the form: (Evaluation, Evaluation method, [method name])\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods are used by the author [author name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods of the papers\",\n",
    "                \"Evaluation Methods are encoded in the triples of the format (R870531:Evaluation Method Entity, Name, [Evaluation Method Name])\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field,\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"How many threats to validity does the paper with the title '[paper title]' have?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication(),\n",
    "            maximum_subgraph_size=100\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"How many threats to validity does the paper with the title '[paper title]' have?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which threads to validity does the publication '[paper title]' have, ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which threads to validity does the publication '[paper title]' have, ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the threads to validity of the paper.\",\n",
    "                \"Only include the threats that have a 'True' boolean value\",\n",
    "                \"You can only use triples that explicitly have 'Threat to Validity' in the object\"\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which sub-properties does the publication '[paper title]' have, ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the sub-properties of the paper.\",\n",
    "                \"Your answer should list all sub-properties in descending alphabetical order.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R871674\"), # A paper that has multiple sub-properties\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which sub-properties does the publication '[paper title]' have, ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples that contain information about the sub-properties of the paper.\",\n",
    "                \"Your answer should list all sub-properties in descending alphabetical order.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which sub-property has been used the most in the year [year] with papers of the paper class [paper class]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\",\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which evaluation method has been used the most in the year [year] with papers of the paper class [paper class]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\",\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = PaperComparisonGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        comparison_options=PaperComparisonGeneratorOptions(\n",
    "            first_publication=graph.get_random_publication(),\n",
    "            second_publication=graph.get_random_publication(),\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What evaluation methods does the paper [paper_title_1] use compared to the paper [paper_title_2]?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples of the evaluation methods of the papers.\",\n",
    "                \"Evaluation Methods are encoded in the triples of the format (R870531:Evaluation Method Entity, Name, [Evaluation Method Name])\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(2):\n",
    "    qa_strategy = PaperComparisonGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_o3_mini,\n",
    "        comparison_options=PaperComparisonGeneratorOptions(\n",
    "            first_publication=graph.get_random_publication(),\n",
    "            second_publication=graph.get_random_publication(),\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What paper class does the paper [paper_title_1] have compared to the paper [paper_title_2]?\",\n",
    "            additional_requirements=[\n",
    "                \"The generated question should include the title of the paper.\",\n",
    "                \"The context should only include the triples of the paper class that are marked as being 'True'\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of the evaluation methods that have been published by the author [author name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the research object and the sub-property.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of research objects that have been published by the author [author name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the research object and the sub-property.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(1):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R868403\"), # We manually selected this publication as it is the only one that has robustness as a property\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which paper includes the evaluation sub-property robustness?\",\n",
    "            additional_requirements=[\n",
    "                \"leave the template as is and do not change it\",\n",
    "                \"The context should only include the robustness sub-property triple. Therefore only one triple can be in the context.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(1):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R873171\"), # We manually selected this publication as it is the only one that has Recovery as a property\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which paper includes the evaluation sub-property Recovery?\",\n",
    "            additional_requirements=[\n",
    "                \"leave the template as is and do not change it\",\n",
    "                \"The context should only include the Recovery sub-property triple. Therefore only one triple can be in the context.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(1):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R872980\"), # We manually selected this publication as it is the only one that has Limit of detection as a property\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which paper includes the evaluation sub-property Limit of detection?\",\n",
    "            additional_requirements=[\n",
    "                \"leave the template as is and do not change it\",\n",
    "                \"The context should only include the Limit of detection sub-property triple. Therefore only one triple can be in the context.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(1):\n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_entity_by_id(\"R873116\"), # We manually selected this publication as it is the only one that has verification as evaluation method\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"Which paper includes the evaluation method Verification?\",\n",
    "            additional_requirements=[\n",
    "                \"leave the template as is and do not change it\",\n",
    "                \"The context should only include the Verification evaluation method triple. Therefore only one triple can be in the context.\",\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have the evaluation method [method]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the evaluation methods of the papers.\",\n",
    "                \"Evaluation Methods are encoded in the triples of the format (R870531:Evaluation Method Entity, Name, [Evaluation Method Name])\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have the evaluation sub-property [sub-property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the sub-property of the papers.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"W [sub property name]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"available\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications evaluate the research object [research object name] with the sub-property [sub-property name] and have input data available?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_predicate_restriction=\"available\",\n",
    "                information_value_restriction=\"True\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many publications have the evaluation sub-property [sub-property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the sub-propertys of the papers.\",\n",
    "                \"The answer should count the number of publications that have the sub-property.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many publications have the evaluation method [method]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the evaluation methods of the papers.\",\n",
    "                \"The answer should count the number of publications that have the evaluation method\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers, that discuss the [threat to validity] as a threat to validity, apply [evaluation method] as a evaluation method?\",\n",
    "            additional_requirements=[\n",
    "                \"The context needs to include all triples given to you.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "                split_clusters=True\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P123037\",\n",
    "        restriction_text=\"Threats To Validity\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have the evaluation method [evaluation method name] ranked by the publication year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the evaluation sub-properties of the papers.\",\n",
    "                \"The answer should be a list of publication titles ranked by publication year\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications have the evaluation sub-property [evaluation sub-property name] ranked by the publication year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples of the evaluation sub-properties of the papers.\",\n",
    "                \"The answer should be a list of publication titles ranked by publication year\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of papers that have the research object [research object name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"You must include all the triples given to you in the context\",\n",
    "                \"First list the amount of times the entity was used in the year\",\n",
    "                \"Then give a final statement about the proportion\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of papers that have the evaluation method [evaluation method name] per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "                \"First list the amount of times the entity was used in the year\",\n",
    "                \"Then give a final statement about the proportion\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers with the evaluation method [evaluation method name] have their input data marked as [input type] compared to those with the input data marked as [input type]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=[\"available\", \"None\"]\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers have used the sub-property [sub-property name] compared to those that used the sub-property [sub-property name], with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                information_value_restriction=[\"Portability\", \"Usability\"]\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers use the evaluation method [evaluation method name], but do not have any input data available?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "                \"Evaluation methods are encoded as: (Evaluation Method Entity, Name, [Method Name])\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"None\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers use the research object [research object name], but do not have any input data available?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you.\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"None\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the most used paper class for publications with the sub-property [sub property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\",\n",
    "                information_value_restriction=\"True\",           \n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the most used paper class for publications with the evaluation method [method name] and the research object [resarch object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",        \n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\",\n",
    "                information_value_restriction=\"True\",           \n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want that the answer only contains one specific paper which is a special case in our data\n",
    "# as it is the only one that is from the type Philosophical Papers. Therefore we use the \n",
    "# FromTopicEntityGenerator to generate the question\n",
    "qa_pairs = []\n",
    "qa_strategy = FromTopicEntityGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "        topic_entity=graph.get_entity_by_id(\"R873077\"), # The id of the paper\n",
    "        maximum_subgraph_size=100\n",
    "    ), \n",
    "    options=GenerationOptions(\n",
    "        template_text=\"Which research object is used with the paper class [paper class]?\",\n",
    "        additional_requirements=[\n",
    "            \"The context should only include the triple that contains the paper class and research object of the paper\",\n",
    "            \"You can assume that it is the only paper with that specific conditions\",\n",
    "            \"The context should contain two triples: (R873092:Paper Class, [the class name], True) and (Research Object Entity, Name, [research object name])\",\n",
    "        ],\n",
    "        validate_contexts=False,\n",
    "        convert_path_to_text=False,\n",
    "        classify_questions=False,\n",
    "    )     \n",
    ")\n",
    "qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which research objects are used in conjunction with the [evaluation method name] evaluation method?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which evaluation methods are used in conjunction with the evaluation sub-property [evaluation sub property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation sub-properties and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods are used in conjunction with the evaluation sub-property [evaluation sub property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and evaluation sub-properties of the paper\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods are used in conjunction with the [evaluation sub property name] sub-property?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\"\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many research objects are used in conjunction with the [evaluation sub property name] sub-property?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation sub-properties and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which evaluation methods are used in conjunction with the evaluation sub-property [evaluation sub property name] ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and evaluation sub-properties of the paper\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which research objects are used in conjunction with the [evaluation sub property name] sub-property ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation sub-properties and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which research objects have occurrences, where the evaluation guideline marked as 'false'?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Has Guideline\",\n",
    "                information_value_restriction=\"False\"\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which research objects that have the evaluation method [evaluation method name] do not use evaluation guidelines?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all context given to you\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Has Guideline\",\n",
    "                information_value_restriction=\"False\",\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\"\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which evaluation sub-properties are used in the research object [research object name] but do not use evaluation guidelines?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all context given to you\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Has Guideline\",\n",
    "                information_value_restriction=\"False\",\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\"\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the most used sub-property that is used with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P123038\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the most used evaluation method that is used with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods and research objects of the paper\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=(\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            ),\n",
    "        )\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P123038\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How often is the sub-property [sub-property name] used in comparison to the sub-property [sub-property] with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                information_value_restriction=[\"Context coverage\", \"Satisfaction\"]\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How often is the sub-property [sub-property name] used in comparison to the sub-property [sub-property] with the evaulation method [evaluation method name]?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                information_value_restriction=[\"Maintainability\", \"Usability\"]\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of the evaluation method [evaluation method name] that is applied per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "                \"First list the amount of times the entity was used in the year\",\n",
    "                \"Then give a final statement about the proportion\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of the research object [research object name] that is applied per year?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "                \"Only generate one answer and question pair\",\n",
    "                \"First list the amount of times the entity was used in the year\",\n",
    "                \"Then give a final statement about the proportion\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(3):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation sub-property used on the research object [research object name] in the publication '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation sub-property used on the research object [research object name] in the publication '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = []\n",
    "for _ in range(3):\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=research_field\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation method used on the research object [research object name] in the publication '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "    \n",
    "    qa_strategy = FromTopicEntityGenerator(\n",
    "        graph=graph,\n",
    "        llm_adapter=gpt_4o_mini,\n",
    "        from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "            topic_entity=graph.get_random_publication()\n",
    "        ),     \n",
    "        options=GenerationOptions(\n",
    "            template_text=\"What is the evaluation method used on the research object [research object name] in the publication '[paper title]'?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            validate_contexts=False,\n",
    "            convert_path_to_text=False,\n",
    "            classify_questions=False,\n",
    "        )     \n",
    "    )\n",
    "    qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation methods have been published by the author [author name] with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should list the evaluation methods\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation methods have been published by the author [author name] with the sub-property [sub-property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should list the evaluation methods\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods have been published by the author [author name] with the sub-property [sub-property name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should count the number of evaluation methods\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods have been published by the author [author name] with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should count the number of evaluation methods\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation methods have been published by the author [author name] with the research object [research object name] ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should count the number of evaluation methods\",\n",
    "                \"The answer should be a list of evaluation methods in descending alphabetical order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation sub-properties have been published by the author [author name] with the evaluation method [evaluation method name] ranked in descending alphabetical order?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should list the evaluation methods\",\n",
    "                \"The answer should be ranked in descending alphabetical order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_o3_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many research objects with the name [research object name] have been published in 2018 in comparison to 2020?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"Research object are formatted as '('Research Object Entity', Name, [Research Object Name]) in the contexts given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2018\", \"2020\"]\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many evaluation methods with the name [evaluation method name] have been published in 2020 in comparison to 2021?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2020\", \"2021\"],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation methods that have been published by the author [author name] have no evaluation guidelines?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should list the evaluation methods\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Has Guideline\",\n",
    "                information_value_restriction=\"False\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What research objects, that have been published by the author [author name] have no evaluation method?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The answer you generate should list the research objects that have no evaluation method\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "                information_value_restriction=\"False\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What research objects, that have been published by the author [author name] have their input data set to none?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The answer you generate should list the research objects that have no evaluation method\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"None\"\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator( #RERUN\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the sub-property that appears the most for the research object [research object name] published by the author [author name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Sub-Property\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What evaluation method has been used the most in the paper class [paper class name] with the research object [research object name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should count the number of evaluation methods\",\n",
    "                \"The answer should be a list of evaluation methods in descending alphabetical order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=20,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which is the most used evaluation method that is used with the research object [research object name] in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "                \"The Evaluation methods are formatted as '('Evaluation Method Entity', Name, [Evaluation Method]) in the contexts given to you\",\n",
    "                \"The answer you generate should be a list of the evaluation methods\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=20,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of the research objects that are used in conjunction with the evaluation method [evaluation method name] between 2019 and 2021?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2019\", \"2021\"],\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Object\"\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of the evaluation methods that are used in conjunction with the sub-property [sub property name] between 2017 and 2019?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2017\", \"2018\", \"2019\"],\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Case 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want that the answer only contains one specific paper which is a special case in our data\n",
    "# as it is the only one that is from the type Philosophical Papers. Therefore we use the \n",
    "# FromTopicEntityGenerator to generate the question\n",
    "qa_pairs = []\n",
    "qa_strategy = FromTopicEntityGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    from_topic_entity_options=FromTopicEntityGeneratorOptions(\n",
    "        topic_entity=graph.get_entity_by_id(\"R873077\") # The id of the paper\n",
    "    ),     \n",
    "    options=GenerationOptions(\n",
    "        template_text=\"Which paper that includes the research object [research object name] has the paper class [paper class name]?\",\n",
    "        additional_requirements=[\n",
    "            \"The context should only include the triple that contains the paper class and research object of the paper\",\n",
    "            \"You can assume that it is the only paper with that specific conditions\"\n",
    "        ],\n",
    "        validate_contexts=False,\n",
    "        convert_path_to_text=False,\n",
    "        classify_questions=False,\n",
    "    )     \n",
    ")\n",
    "qa_pairs.extend(qa_strategy.generate())\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which paper that includes the evaluation method [evaluation method name] is authored by [author name]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=2,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator( #RERUN\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers used the evaluation method [evaluation method name] in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=4,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers used [evaluation method] as a evaluation method in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers used [research object] as a research object in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers used [evaluation method] as a evaluation method in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers used [evaluation method] as a evaluation method in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers used [research object] as a research object in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers used [evaluation method] as a evaluation method in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications, ranked in descending order of their publication year, have [paper class] as their paper class and include the evaluation method [evaluation method name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The answer should be list thhe publication titles in chronological order\",\n",
    "                \"Ensure that the list is ordered correctly based on the publication year in descending order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"paper class\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False,\n",
    "        use_predicate_as_value=True,\n",
    "        restriction_value=\"True\"\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which publications, ranked in descending order of their publication year, have [author name] as an author have and have evaluation method [evaluation_method_name]?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should only include the triples that contain the evaluation methods of the paper\",\n",
    "                \"The answer should be a list of publication titles in chronological order\",\n",
    "                \"Ensure that the list is ordered correctly based on the publication year in descending order\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Evaluation method\",\n",
    "                split_clusters=True\n",
    "            ),            \n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P154073\",\n",
    "        restriction_text=\"authors\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers with the research object [research object name] do not have input data available in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"You must ensure that all the triples given to you are included in the context\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"None\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which papers with the evaluation method [evaluation method] do not have tool support available in the year [year]?\",\n",
    "            additional_requirements=[\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"Uses Input Data\",\n",
    "                information_value_restriction=\"True\",\n",
    "                information_value_predicate_restriction=\"None\"\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers have the research object [research object name] in the year 2020 in comparison to 2021?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2020\", \"2021\"],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"How many papers have the sub-property [sub property name] in the year 2017 in comparison to 2020?\",\n",
    "            additional_requirements=[\n",
    "                \"Include all triples that are provided to you as contexts!\"\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2017\", \"2020\"],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P162024\",\n",
    "        restriction_text=\"Sub-Property\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        enable_caching=True,\n",
    "        golden_triple_minimum=6\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which paper class has the most papers with the evaluation method [method name] in the publication year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\",\n",
    "                information_value_restriction=\"True\"               \n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(#RERUN\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"Which paper class has the most papers with the research object [object name] in the publication year [year]?\",\n",
    "            additional_requirements=[\n",
    "                \"You need to include all context ids in your output. For example, if there are 4 contexts your output has to be [0, 1, 2, 3]\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                split_clusters=True\n",
    "            ),\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"paper class\",\n",
    "                information_value_restriction=\"True\",            \n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of papers with the evaluation method [evaluation method name] between 2019 and 2021?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2019\", \"2020\", \"2021\"],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P59089\",\n",
    "        restriction_text=\"Evaluation method\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pairs = ClusterBasedQuestionGenerator(\n",
    "    graph=graph,\n",
    "    llm_adapter=gpt_4o_mini,\n",
    "    generator_options=ClusterGeneratorOptions(\n",
    "        generation_options=GenerationOptions(\n",
    "            template_text=\"What is the proportion of papers with the research object [research object name] between 2018 and 2020?\",\n",
    "            additional_requirements=[\n",
    "                \"The context should include all triples given to you\",\n",
    "            ],\n",
    "            convert_path_to_text=False,\n",
    "            validate_contexts=False,\n",
    "            classify_questions=False,\n",
    "        ),\n",
    "        additional_restrictions=[\n",
    "            AdditionalInformationRestriction(\n",
    "                information_predicate=\"publication year\",\n",
    "                information_value_restriction=[\"2018\", \"2019\", \"2020\"],\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    cluster_options=ClusterStrategyOptions(\n",
    "        topic_entity=research_field,\n",
    "        restriction_type=\"P47032\",\n",
    "        restriction_text=\"Object\",\n",
    "        cluster_eps=0.1,\n",
    "        cluster_metric=\"cosine\",\n",
    "        cluster_emb_config=embedding_config,\n",
    "        soft_limit_qa_pairs=10,\n",
    "        golden_triple_limit=10,\n",
    "        golden_triple_minimum=6,\n",
    "        enable_caching=True,\n",
    "        skip_clusters_with_only_one_root=False\n",
    "    )\n",
    ").generate()\n",
    "\n",
    "print_qa_pairs(qa_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
