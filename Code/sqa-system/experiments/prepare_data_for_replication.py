
# ----- HOW TO USE THIS SCRIPT -----
# This script is intended to prepare the ORKG graph (Sandbox Environment) with the data that we used during
# our experiments and update the QA Datasets with the new IDs that the ORKG has generated.
# This is necessary, in the case that in the future the data is no longer available in the ORKG graph.
# The QA Datasets need to be updated as well in this case, as the IDs generated by the ORKG will not match
# the IDs in the QA Datasets anymore.
#
# What this script does:
# 1. It checks whether the data is still available in the ORKG graph.
# 2. If the data is not available, it will upload the data to the ORKG graph.
# 3. It then runs update scripts which will fetch all the QA Datasets that we used during our experiments
#    and update the IDs in the datasets in-place.
# 4. You now can re-run the experiments in this folder.
#
# How to run this script:
# 1. You need to have installed the SQA system according to the README.md file.
# 2. Then run this script in a terminal using `python prepare_data_for_replication.py`.
# ---- END OF HOW TO USE THIS SCRIPT ----

import os
import json
from sqa_system.core.data.file_path_manager import FilePathManager
from sqa_system.core.config.models import KnowledgeGraphConfig
from sqa_system.qa_generator import QADatasetToGraphConverter


def prepare_graph_and_dataset(graph_config_path: str, qa_dataset_path: str):
    """
    Prepares the ORKG graph and the QA dataset for the experiments.
    It checks if the data is available in the ORKG graph and if not, it uploads the data.
    It also updates the QA dataset with the new IDs that the ORKG has generated.

    Args:
        graph_config_path (str): Path to the graph config file.
        qa_dataset_path (str): Path to the QA dataset file.
    """
    print("Checking if the data is available in the ORKG graph...")
    print(f"Graph config path: {graph_config_path}")
    print(f"QA dataset path: {qa_dataset_path}")

    with open(graph_config_path, encoding="utf-8") as f:
        kg_config_dict = json.load(f)
    
    kg_config = KnowledgeGraphConfig.from_dict(
        kg_config_dict
    )
    
    # Here we set the configuration to update modus
    kg_config.additional_params["force_cache_update"] = True
        
    preparer = QADatasetToGraphConverter(
        kg_config=kg_config,
        string_replacements={
            "Has Evaluation Guideline": "Has Guideline",
            "Threats to validity": "threat to validity",
            "Tool is available": "available",
            "Tool was used but is not available": "used",
            "No tool used": "none",
            "Uses Tool Support": "Tool Support",
            "Input data was used but is not available": "used",
            "Input data is available": "available",
            "No input data used": "none",
            "Uses Input Data": "Input Data"
        },
        update_in_place=True
    )
    print("ORKG Graph check successful. All data should now be available.")
    print("Starting check for QA dataset...")
    if not preparer.validate_qa_dataset(qa_dataset_path):
        print("QA dataset is not valid. Starting Conversion...")
        preparer.run_conversion(
            qa_dataset_path=qa_dataset_path,
            research_field_id="R659055"
        )
    else:
        print("QA dataset is valid. No conversion needed.")
        return
    print("Finished checking the QA dataset and the ORKG data.")


def get_graph_config_for_qa_dataset(qa_dataset_path: str, graph_config_paths: list[str]) -> str:
    """
    Gets the graph config path for the given QA dataset path based on the file name.

    Args:
        qa_dataset_path (str): Path to the QA dataset file.
        graph_config_paths (list[str]): List of graph config paths.

    Returns:
        str: Path to the graph config file.
    """
    file_name = os.path.basename(qa_dataset_path)
    keywords = [
        "deep_distributed",
        "deep_centralized",
        "flat_distributed",
        "flat_centralized"
    ]
    for keyword in keywords:
        if keyword in file_name:
            for path in graph_config_paths:
                if keyword in path:
                    return path
    raise ValueError(f"Unknown graph config for dataset: {file_name}")


if __name__ == "__main__":
    fpm = FilePathManager()
    current_dir = os.path.dirname(os.path.realpath(__file__))
    graph_config_paths = fpm.get_files_in_folder(
        folder_path=fpm.combine_paths(
            current_dir, "2_experiment", "graph_configs"),
        file_type="json"
    )

    qa_dataset_paths = fpm.get_files_in_folder(
        folder_path=fpm.combine_paths(
            current_dir, "qa_datasets", "qa_datasets"),
        file_type="csv"
    )

    for qa_dataset_path in qa_dataset_paths:
        prepare_graph_and_dataset(
            graph_config_path=get_graph_config_for_qa_dataset(
                qa_dataset_path, graph_config_paths),
            qa_dataset_path=qa_dataset_path
        )
