
@inproceedings{xiong_explicit_2017,
	location = {Perth Australia},
	title = {Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding},
	isbn = {978-1-4503-4913-0},
	url = {https://dl.acm.org/doi/10.1145/3038912.3052558},
	doi = {10.1145/3038912.3052558},
	abstract = {This paper introduces Explicit Semantic Ranking ({ESR}), a new ranking technique that leverages knowledge graph embedding. Analysis of the query log from our academic search engine, {SemanticScholar}.org, reveals that a major error source is its inability to understand the meaning of research concepts in queries. To addresses this challenge, {ESR} represents queries and documents in the entity space and ranks them based on their semantic connections from their knowledge graph embedding. Experiments demonstrate {ESR}’s ability in improving Semantic Scholar’s online production system, especially on hard queries where word-based ranking fails.},
	eventtitle = {{WWW} '17: 26th International World Wide Web Conference},
	pages = {1271--1279},
	booktitle = {Proceedings of the 26th International Conference on World Wide Web},
	publisher = {International World Wide Web Conferences Steering Committee},
	author = {Xiong, Chenyan and Power, Russell and Callan, Jamie},
	urldate = {2024-07-09},
	date = {2017-04-03},
	langid = {english},
	keywords = {academic search engine, Embedding-based, {KG}-based, {LLM}, notion, {RAG}},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\98HT82LH\\Xiong et al. - 2017 - Explicit Semantic Ranking for Academic Search via Knowledge Graph Embedding.pdf:application/pdf},
}

@misc{es_ragas_2023,
	title = {{RAGAS}: Automated Evaluation of Retrieval Augmented Generation},
	url = {http://arxiv.org/abs/2309.15217},
	shorttitle = {{RAGAS}},
	abstract = {We introduce {RAGAS} (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation ({RAG}) pipelines. {RAG} systems are composed of a retrieval and an {LLM} based generation module, and provide {LLMs} with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating {RAG} architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the {LLM} to exploit such passages in a faithful way, or the quality of the generation itself. With {RAGAS}, we put forward a suite of metrics which can be used to evaluate these different dimensions without having to rely on ground truth human annotations. We posit that such a framework can crucially contribute to faster evaluation cycles of {RAG} architectures, which is especially important given the fast adoption of {LLMs}.},
	number = {{arXiv}:2309.15217},
	publisher = {{arXiv}},
	author = {Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
	urldate = {2024-07-09},
	date = {2023-09-26},
	langid = {english},
	keywords = {Embedding-based, {LLM}, notion, {RAG}, Framework, Metrics, Retrieval Evaluation, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\Y5AAYZLN\\Es et al. - 2023 - RAGAS Automated Evaluation of Retrieval Augmented Generation.pdf:application/pdf},
}

@misc{liu_web_2022,
	title = {Web of Scholars: A Scholar Knowledge Graph},
	url = {http://arxiv.org/abs/2202.11311},
	shorttitle = {Web of Scholars},
	abstract = {In this work, we demonstrate a novel system, namely Web of Scholars, which integrates state-of-the-art mining techniques to search, mine, and visualize complex networks behind scholars in the field of Computer Science. Relying on the knowledge graph, it provides services for fast, accurate, and intelligent semantic querying as well as powerful recommendations. In addition, in order to realize information sharing, it provides open {API} to be served as the underlying architecture for advanced functions. Web of Scholars takes advantage of knowledge graph, which means that it will be able to access more knowledge if more search exist. It can be served as a useful and interoperable tool for scholars to conduct in-depth analysis within Science of Science.},
	number = {{arXiv}:2202.11311},
	publisher = {{arXiv}},
	author = {Liu, Jiaying and Ren, Jing and Zheng, Wenqing and Chi, Lianhua and Lee, Ivan and Xia, Feng},
	urldate = {2024-07-09},
	date = {2022-02-23},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2202.11311 [cs]},
	note = {Read\_Status: New
Read\_Status\_Date: 2024-07-09T18:07:56.067Z},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\FYS3NQ9P\\Liu et al. - 2022 - Web of Scholars A Scholar Knowledge Graph.pdf:application/pdf},
}

@misc{feng_trends_2023,
	title = {Trends in Integration of Knowledge and Large Language Models: A Survey and Taxonomy of Methods, Benchmarks, and Applications},
	url = {http://arxiv.org/abs/2311.05876},
	shorttitle = {Trends in Integration of Knowledge and Large Language Models},
	abstract = {Large language models ({LLMs}) exhibit superior performance on various natural language tasks, but they are susceptible to issues stemming from outdated data and domain-specific limitations. In order to address these challenges, researchers have pursued two primary strategies, knowledge editing and retrieval augmentation, to enhance {LLMs} by incorporating external information from different aspects. Nevertheless, there is still a notable absence of a comprehensive survey. In this paper, we propose a review to discuss the trends in integration of knowledge and large language models, including taxonomy of methods, benchmarks, and applications. In addition, we conduct an in-depth analysis of different methods and point out potential research directions in the future. We hope this survey offers the community quick access and a comprehensive overview of this research area, with the intention of inspiring future research endeavors.},
	number = {{arXiv}:2311.05876},
	publisher = {{arXiv}},
	author = {Feng, Zhangyin and Ma, Weitao and Yu, Weijiang and Huang, Lei and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and liu, Ting},
	urldate = {2024-07-09},
	date = {2023-12-07},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\VMNQD835\\Feng et al. - 2023 - Trends in Integration of Knowledge and Large Language Models A Survey and Taxonomy of Methods, Benc.pdf:application/pdf},
}

@article{pan_unifying_2024,
	title = {Unifying Large Language Models and Knowledge Graphs: A Roadmap},
	volume = {36},
	issn = {1041-4347, 1558-2191, 2326-3865},
	doi = {10.1109/TKDE.2024.3352100},
	shorttitle = {Unifying Large Language Models and Knowledge Graphs},
	abstract = {Large language models ({LLMs}), such as {ChatGPT} and {GPT}4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, {LLMs} are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs ({KGs}), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. {KGs} can enhance {LLMs} by providing external knowledge for inference and interpretability. Meanwhile, {KGs} are difficult to construct and evolve by nature, which challenges the existing methods in {KGs} to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify {LLMs} and {KGs} together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of {LLMs} and {KGs}. Our roadmap consists of three general frameworks, namely, 1) {KG}-enhanced {LLMs}, which incorporate {KGs} during the pre-training and inference phases of {LLMs}, or for the purpose of enhancing understanding of the knowledge learned by {LLMs}; 2) {LLM}-augmented {KGs}, that leverage {LLMs} for different {KG} tasks such as embedding, completion, construction, graph-to-text generation, and question answering; and 3) Synergized {LLMs} + {KGs}, in which {LLMs} and {KGs} play equal roles and work in a mutually beneficial way to enhance both {LLMs} and {KGs} for bidirectional reasoning driven by both data and knowledge. We review and summarize existing efforts within these three frameworks in our roadmap and pinpoint their future research directions.},
	pages = {3580--3599},
	number = {7},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Pan, Shirui and Luo, Linhao and Wang, Yufei and Chen, Chen and Wang, Jiapu and Wu, Xindong},
	date = {2024-07},
	langid = {english},
	keywords = {{KG}-based, {KG}-fundamentals, {LLM}, notion, {RAG}, Survey},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\X3S7EAA4\\Pan et al. - 2024 - Unifying Large Language Models and Knowledge Graphs A Roadmap.pdf:application/pdf},
}

@misc{kaplan_unifying_2022,
	title = {Unifying Classification Schemes for Software Engineering Meta-Research},
	url = {http://arxiv.org/abs/2209.10491},
	abstract = {Background: Classifications in meta-research enable researchers to cope with an increasing body of scientific knowledge. They provide a framework for, e.g., distinguishing methods, reports, reproducibility, and evaluation in a knowledge field as well as a common terminology. Both eases sharing, understanding and evolution of knowledge. In software engineering ({SE}), there are several classifications that describe the nature of {SE} research (e.g., classifications of research methods, replication types, types of {SE} contributions). Regarding the consolidation of the large body of classified knowledge in {SE} research, a generally applicable classification scheme is crucial. Moreover, the commonalities and differences among different classification schemes have rarely been studied. Due to the fact that classifications are documented textual, it is hard to catalog, reuse, and compare them. To the best of our knowledge, there is no research work so far that addresses documentation and systematic investigation of classifications in {SE} meta-research.},
	number = {{arXiv}:2209.10491},
	publisher = {{arXiv}},
	author = {Kaplan, Angelika and Kühn, Thomas and Reussner, Ralf},
	urldate = {2024-07-09},
	date = {2022-09-21},
	langid = {english},
	keywords = {notion},
	file = {Kaplan et al. - 2022 - Unifying Classification Schemes for Software Engin.pdf:C\:\\Users\\Marco\\Zotero\\storage\\VAUMQ555\\Kaplan et al. - 2022 - Unifying Classification Schemes for Software Engin.pdf:application/pdf},
}

@misc{sun_think--graph_2024,
	title = {Think-on-Graph: Deep and Responsible Reasoning of Large Language Model on Knowledge Graph},
	url = {http://arxiv.org/abs/2307.07697},
	shorttitle = {Think-on-Graph},
	abstract = {Although large language models ({LLMs}) have achieved significant success in various tasks, they often struggle with hallucination problems, especially in scenarios requiring deep and responsible reasoning. These issues could be partially addressed by introducing external knowledge graphs ({KG}) in {LLM} reasoning. In this paper, we propose a new {LLM}-{KG} integrating paradigm “{LLM} ⊗ {KG}” which treats the {LLM} as an agent to interactively explore related entities and relations on {KGs} and perform reasoning based on the retrieved knowledge. We further implement this paradigm by introducing a new approach called Think-on-Graph ({ToG}), in which the {LLM} agent iteratively executes beam search on {KG}, discovers the most promising reasoning paths, and returns the most likely reasoning results. We use a number of well-designed experiments to examine and illustrate the following advantages of {ToG}: 1) compared with {LLMs}, {ToG} has better deep reasoning power; 2) {ToG} has the ability of knowledge traceability and knowledge correctability by leveraging {LLMs} reasoning and expert feedback; 3) {ToG} provides a flexible plugand-play framework for different {LLMs}, {KGs} and prompting strategies without any additional training cost; 4) the performance of {ToG} with small {LLM} models could exceed large {LLM} such as {GPT}-4 in certain scenarios and this reduces the cost of {LLM} deployment and application. As a training-free method with lower computational cost and better generality, {ToG} achieves overall {SOTA} in 6 out of 9 datasets where most previous {SOTAs} rely on additional training. Our code is publicly available at https://github.com/{IDEA}-{FinAI}/{ToG}.},
	number = {{arXiv}:2307.07697},
	publisher = {{arXiv}},
	author = {Sun, Jiashuo and Xu, Chengjin and Tang, Lumingyuan and Wang, Saizhuo and Lin, Chen and Gong, Yeyun and Ni, Lionel M. and Shum, Heung-Yeung and Guo, Jian},
	urldate = {2024-07-09},
	date = {2024-03-24},
	langid = {english},
	keywords = {notion, finished},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\LQJGLAXZ\\Sun et al. - 2024 - Think-on-Graph Deep and Responsible Reasoning of Large Language Model on Knowledge Graph.pdf:application/pdf},
}

@misc{kinney_semantic_2023,
	title = {The Semantic Scholar Open Data Platform},
	url = {http://arxiv.org/abs/2301.10140},
	abstract = {The volume of scientiﬁc output is creating an urgent need for automated tools to help scientists keep up with developments in their ﬁeld. Semantic Scholar (S2) is an open data platform and website aimed at accelerating science by helping scholars discover and understand scientiﬁc literature. We combine public and proprietary data sources using state-of-theart techniques for scholarly {PDF} content extraction and automatic knowledge graph construction to build the Semantic Scholar Academic Graph, the largest open scientiﬁc literature graph to-date, with 200M+ papers, 80M+ authors, 550M+ paper-authorship edges, and 2.4B+ citation edges. The graph includes advanced semantic features such as structurally parsed text, natural language summaries, and vector embeddings. In this paper, we describe the components of the S2 data processing pipeline and the associated {APIs} offered by the platform. We will update this living document to reﬂect changes as we add new data offerings and improve existing services.},
	number = {{arXiv}:2301.10140},
	publisher = {{arXiv}},
	author = {Kinney, Rodney and Anastasiades, Chloe and Authur, Russell and Beltagy, Iz and Bragg, Jonathan and Buraczynski, Alexandra and Cachola, Isabel and Candra, Stefan and Chandrasekhar, Yoganand and Cohan, Arman and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Etzioni, Oren and Evans, Rob and Feldman, Sergey and Gorney, Joseph and Graham, David and Hu, Fangzhou and Huff, Regan and King, Daniel and Kohlmeier, Sebastian and Kuehl, Bailey and Langan, Michael and Lin, Daniel and Liu, Haokun and Lo, Kyle and Lochner, Jaron and {MacMillan}, Kelsey and Murray, Tyler and Newell, Chris and Rao, Smita and Rohatgi, Shaurya and Sayre, Paul and Shen, Zejiang and Singh, Amanpreet and Soldaini, Luca and Subramanian, Shivashankar and Tanaka, Amber and Wade, Alex D. and Wagner, Linda and Wang, Lucy Lu and Wilhelm, Chris and Wu, Caroline and Yang, Jiangjiang and Zamarron, Angele and Van Zuylen, Madeleine and Weld, Daniel S.},
	urldate = {2024-07-09},
	date = {2023-01-24},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\M3MR5S5Z\\Kinney et al. - 2023 - The Semantic Scholar Open Data Platform.pdf:application/pdf},
}

@incollection{vrandecic_towards_2018,
	location = {Cham},
	title = {Towards Empty Answers in {SPARQL}: Approximating Querying with {RDF} Embedding},
	volume = {11136},
	isbn = {978-3-030-00670-9 978-3-030-00671-6},
	url = {https://link.springer.com/10.1007/978-3-030-00671-6_30},
	shorttitle = {Towards Empty Answers in {SPARQL}},
	abstract = {The {LOD} cloud oﬀers a plethora of {RDF} data sources where users discover items of interest by issuing {SPARQL} queries. A common query problem for users is to face with empty answers: given a {SPARQL} query that returns nothing, how to reﬁne the query to obtain a nonempty set? In this paper, we propose an {RDF} graph embedding based framework to solve the {SPARQL} empty-answer problem in terms of a continuous vector space. We ﬁrst project the {RDF} graph into a continuous vector space by an entity context preserving translational embedding model which is specially designed for {SPARQL} queries. Then, given a {SPARQL} query that returns an empty set, we partition it into several parts and compute approximate answers by leveraging {RDF} embeddings and the translation mechanism. We also generate alternative queries for returned answers, which helps users recognize their expectations and reﬁne the original query ﬁnally. To validate the eﬀectiveness and eﬃciency of our framework, we conduct extensive experiments on the realworld {RDF} dataset. The results show that our framework can significantly improve the quality of approximate answers and speed up the generation of alternative queries.},
	pages = {513--529},
	booktitle = {The Semantic Web – {ISWC} 2018},
	publisher = {Springer International Publishing},
	author = {Wang, Meng and Wang, Ruijie and Liu, Jun and Chen, Yihe and Zhang, Lei and Qi, Guilin},
	editor = {Vrandečić, Denny and Bontcheva, Kalina and Suárez-Figueroa, Mari Carmen and Presutti, Valentina and Celino, Irene and Sabou, Marta and Kaffee, Lucie-Aimée and Simperl, Elena},
	urldate = {2024-07-09},
	date = {2018},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\EVBYJ2DF\\Wang et al. - 2018 - Towards Empty Answers in SPARQL Approximating Querying with RDF Embedding.pdf:application/pdf},
}

@inproceedings{thambi_towards_2022,
	location = {Coimbatore, India},
	title = {Towards Improving the Performance of Question Answering System using Knowledge Graph - A Survey},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-0052-7},
	url = {https://ieeexplore.ieee.org/document/9742802/},
	doi = {10.1109/ICAIS53314.2022.9742802},
	abstract = {A question answering system attempts to give a precise response to questions posed in natural language. Question answering systems must capture some context information in order to accurately answer non-trivial queries. Question answering is a challenging research issue because of the contextual interconnected information. Knowledge graphs are a form of data representation that is both structured and interconnected. The process of retrieval of exact answer from a knowledge graph for a given question is named as knowledge graph question answering ({KGQA}). Different ways for executing {KGQA} exist depending on the type of query, searching patterns over the knowledge graph, ranking of candidate answers, and so on. This paper surveys various approaches for {KGQA}, the major challenges, the current techniques for evaluations and the research gaps in {KGQA}.},
	eventtitle = {2022 Second International Conference on Artificial Intelligence and Smart Energy ({ICAIS})},
	pages = {672--679},
	booktitle = {2022 Second International Conference on Artificial Intelligence and Smart Energy ({ICAIS})},
	publisher = {{IEEE}},
	author = {Thambi, Sincy V. and {ReghuRaj}, P. C.},
	urldate = {2024-07-09},
	date = {2022-02-23},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\BEPW2UTB\\Thambi and ReghuRaj - 2022 - Towards Improving the Performance of Question Answering System using Knowledge Graph - A Survey.pdf:application/pdf},
}

@article{peswani_survey_nodate,
	title = {Survey: Knowledge Graph Assisted Deep Learning Based Question Answering System},
	abstract = {Question Answering ({QA}) is one of the essential downstream tasks in {NLP}. Question Answering in the maintenance domain requires us to utilize user manuals containing instructions to operate the device safely. It is a challenging task as it requires handling multiple types of questions, such as factoid, non-factoid, confirmation, and list types. Moreover, there is a scarcity of data available for the maintenance domain. To tackle this task, we need a pipeline that contains a question-answering system and an information retrieval module. The information retrieval module retrieves all the relevant paragraphs from the user manual to answer the questions. Question Answering module answers all types of questions using the extracted paragraphs.},
	author = {Peswani, Harsh and Bhattacharyya, Pushpak},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\U5XMYVPA\\Peswani and Bhattacharyya - Survey Knowledge Graph Assisted Deep Learning Based Question Answering System.pdf:application/pdf},
}

@article{farber_microsoft_2022,
	title = {The Microsoft Academic Knowledge Graph enhanced: Author name disambiguation, publication classification, and embeddings},
	volume = {3},
	issn = {2641-3337},
	doi = {10.1162/qss_a_00183},
	shorttitle = {The Microsoft Academic Knowledge Graph enhanced},
	abstract = {Although several large knowledge graphs have been proposed in the scholarly field, such graphs are limited with respect to several data quality dimensions such as accuracy and coverage. In this article, we present methods for enhancing the Microsoft Academic Knowledge Graph ({MAKG}), a recently published large-scale knowledge graph containing metadata about scientific publications and associated authors, venues, and affiliations. Based on a qualitative analysis of the {MAKG}, we address three aspects. First, we adopt and evaluate unsupervised approaches for large-scale author name disambiguation. Second, we develop and evaluate methods for tagging publications by their discipline and by keywords, facilitating enhanced search and recommendation of publications and associated entities. Third, we compute and evaluate embeddings for all 239 million publications, 243 million authors, 49,000 journals, and 16,000 conference entities in the {MAKG} based on several state-of-the-art embedding techniques. Finally, we provide statistics for the updated {MAKG}. Our final {MAKG} is publicly available at https://makg.org and can be used for the search or recommendation of scholarly entities, as well as enhanced scientific impact quantification.},
	pages = {51--98},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	author = {Färber, Michael and Ao, Lin},
	date = {2022-04-12},
	langid = {english},
	keywords = {notion},
	file = {Färber and Ao - 2022 - The Microsoft Academic Knowledge Graph enhanced A.pdf:C\:\\Users\\Marco\\Zotero\\storage\\BV9Z6NML\\Färber and Ao - 2022 - The Microsoft Academic Knowledge Graph enhanced A.pdf:application/pdf},
}

@article{khalid_supporting_2020,
	title = {Supporting Scholarly Search by Query Expansion and Citation Analysis},
	volume = {10},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {1792-8036, 2241-4487},
	url = {http://etasr.com/index.php/ETASR/article/view/3655},
	doi = {10.48084/etasr.3655},
	abstract = {Published scholarly articles have increased exponentially in recent years. This growth has brought challenges for academic researchers in locating the most relevant papers in their fields of interest. The reasons for this vary. There is the fundamental problem of synonymy and polysemy, the query terms might be too short, thus making it difficult to distinguish between papers. Also, a new researcher has limited knowledge and often is not sure about what she is looking for until the results are displayed. These issues obstruct scholarly retrieval systems in locating highly relevant publications for a given search query. Researchers seek to tackle these issues. However, the user's intent cannot be addressed entirely by introducing a direct information retrieval technique. In this paper, a novel approach is proposed, which combines query expansion and citation analysis for supporting the scholarly search. It is a two-stage academic search process. Upon receiving the initial search query, in the first stage, the retrieval system provides a ranked list of results. In the second stage, the highest-scoring Term Frequency–Inverse Document Frequency ({TF}-{IDF}) terms are obtained from a few top-ranked papers for query expansion behind the scene. In both stages, citation analysis is used in further refining the quality of the academic search. The originality of the approach lies in the combined exploitation of both query expansion by pseudo relevance feedback and citation networks analysis that may bring the most relevant papers to the top of the search results list. The approach is evaluated on the {ACL} dataset. The experimental results reveal that the technique is effective and robust for locating relevant papers regarding normalized Discounted Cumulative Gain ({nDCG}), precision, and recall.},
	pages = {6102--6108},
	number = {4},
	journaltitle = {Engineering, Technology \& Applied Science Research},
	shortjournal = {Eng. Technol. Appl. Sci. Res.},
	author = {Khalid, S. and Wu, S.},
	urldate = {2024-07-09},
	date = {2020-08-16},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\V28Z6N4X\\Khalid and Wu - 2020 - Supporting Scholarly Search by Query Expansion and Citation Analysis.pdf:application/pdf},
}

@misc{jiang_structgpt_2023,
	title = {{StructGPT}: A General Framework for Large Language Model to Reason over Structured Data},
	url = {http://arxiv.org/abs/2305.09645},
	shorttitle = {{StructGPT}},
	abstract = {In this paper, we aim to improve the reasoning ability of large language models ({LLMs}) over structured data in a unified way. Inspired by the studies on tool augmentation for {LLMs}, we develop an Iterative Reading-{thenReasoning} ({IRR}) framework to solve question answering tasks based on structured data, called {StructGPT}. In this framework, we construct the specialized interfaces to collect relevant evidence from structured data (i.e., reading), and let {LLMs} concentrate on the reasoning task based on the collected information (i.e., reasoning). Specially, we propose an invokinglinearization-generation procedure to support {LLMs} in reasoning on the structured data with the help of the interfaces. By iterating this procedure with provided interfaces, our approach can gradually approach the target answers to a given query. Experiments conducted on three types of structured data show that {StructGPT} greatly improves the performance of {LLMs}, under the few-shot and zero-shot settings. Our codes and data are publicly available at https://github.com/{RUCAIBox}/{StructGPT}.},
	number = {{arXiv}:2305.09645},
	publisher = {{arXiv}},
	author = {Jiang, Jinhao and Zhou, Kun and Dong, Zican and Ye, Keming and Zhao, Wayne Xin and Wen, Ji-Rong},
	urldate = {2024-07-09},
	date = {2023-10-23},
	langid = {english},
	keywords = {notion, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\PR953RX5\\Jiang et al. - 2023 - StructGPT A General Framework for Large Language Model to Reason over Structured Data.pdf:application/pdf},
}

@misc{moiseev_skill_2022,
	title = {{SKILL}: Structured Knowledge Infusion for Large Language Models},
	url = {http://arxiv.org/abs/2205.08184},
	shorttitle = {{SKILL}},
	abstract = {Large language models ({LLMs}) have demonstrated human-level performance on a vast spectrum of natural language tasks. However, it is largely unexplored whether they can better internalize knowledge from a structured data, such as a knowledge graph, or from text. In this work, we propose a method to infuse structured knowledge into {LLMs}, by directly training T5 models on factual triples of knowledge graphs ({KGs}). We show that models pretrained on Wikidata {KG} with our method outperform the T5 baselines on {FreebaseQA} and {WikiHop}, as well as the Wikidata-answerable subset of {TriviaQA} and {NaturalQuestions}. The models pre-trained on factual triples compare competitively with the ones on natural language sentences that contain the same knowledge. Trained on a smaller size {KG}, {WikiMovies}, we saw 3× improvement of exact match score on {MetaQA} task compared to T5 baseline. The proposed method has an advantage that no alignment between the knowledge graph and text corpus is required in curating training data. This makes our method particularly useful when working with industry-scale knowledge graphs.},
	number = {{arXiv}:2205.08184},
	publisher = {{arXiv}},
	author = {Moiseev, Fedor and Dong, Zhe and Alfonseca, Enrique and Jaggi, Martin},
	urldate = {2024-07-09},
	date = {2022-05-17},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\BTJYBTQ3\\Moiseev et al. - 2022 - SKILL Structured Knowledge Infusion for Large Language Models.pdf:application/pdf},
}

@incollection{sun_scholar_2021,
	location = {Singapore},
	title = {Scholar Knowledge Graph and Its Application in Scholar Encyclopedia},
	volume = {1330},
	isbn = {978-981-16-2539-8 978-981-16-2540-4},
	url = {https://link.springer.com/10.1007/978-981-16-2540-4_41},
	abstract = {Knowledge graphs ({KGs}) are developing rapidly and has a wide range of applications in various ﬁelds. But there have been relatively few Chinese encyclopedias with the scholar knowledge graph so far. Therefore, we rely on the academic social network-{SCHOLAT} to complete the construction of scholar {KG}. The purpose of this scholar {KG} is to solve the problems that the information of scholars is not updated in time and scattered in the organizations’ websites. This paper introduces in detail the key technologies for constructing scholar {KG}, such as knowledge extraction, organization and management of scholar {KG}. At the same time, we take advantages of {KG} in semantic expression into account. We apply the scholar {KG} to the scholar encyclopedia system. Based on the scholar {KG}, we completed the scholar recommendation on the Scholar Encyclopedia. On the other hand, the scholar {KG} will also provide the basis for more intelligent applications of {SCHOLAT} in the future.},
	pages = {573--582},
	booktitle = {Computer Supported Cooperative Work and Social Computing},
	publisher = {Springer Singapore},
	author = {Hu, Kun and Li, Jianguo and Chen, Wande and Yuan, Chengzhe and Xu, Qiang and Tang, Yong},
	editor = {Sun, Yuqing and Liu, Dongning and Liao, Hao and Fan, Hongfei and Gao, Liping},
	urldate = {2024-07-09},
	date = {2021},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\AI86MQFZ\\Hu et al. - 2021 - Scholar Knowledge Graph and Its Application in Scholar Encyclopedia.pdf:application/pdf},
}

@misc{gao_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Large Language Models: A Survey},
	url = {http://arxiv.org/abs/2312.10997},
	shorttitle = {Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation ({RAG}) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domainspecific information. {RAG} synergistically merges {LLMs}’ intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of {RAG} paradigms, encompassing the Naive {RAG}, the Advanced {RAG}, and the Modular {RAG}. It meticulously scrutinizes the tripartite foundation of {RAG} frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-theart technologies embedded in each of these critical components, providing a profound understanding of the advancements in {RAG} systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development 1.},
	number = {{arXiv}:2312.10997},
	publisher = {{arXiv}},
	author = {Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
	urldate = {2024-07-09},
	date = {2024-03-27},
	langid = {english},
	keywords = {notion, technical\_report},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\WFWTC9IK\\Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf},
}

@misc{sciavolino_simple_2022,
	title = {Simple Entity-Centric Questions Challenge Dense Retrievers},
	url = {http://arxiv.org/abs/2109.08535},
	abstract = {Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct {EntityQuestions}, a set of simple, entity-rich questions based on facts from Wikidata (e.g., "Where was Arve Furset born?"), and observe that dense retrievers drastically underperform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.},
	number = {{arXiv}:2109.08535},
	publisher = {{arXiv}},
	author = {Sciavolino, Christopher and Zhong, Zexuan and Lee, Jinhyuk and Chen, Danqi},
	urldate = {2024-07-09},
	date = {2022-02-21},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7NAH5V5F\\Sciavolino et al. - 2022 - Simple Entity-Centric Questions Challenge Dense Retrievers.pdf:application/pdf},
}

@misc{wang_searching_2024,
	title = {Searching for Best Practices in Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2407.01219},
	abstract = {Retrieval-augmented generation ({RAG}) techniques have proven to be effective in integrating up-to-date information, mitigating hallucinations, and enhancing response quality, particularly in specialized domains. While many {RAG} approaches have been proposed to enhance large language models through query-dependent retrievals, these approaches still suffer from their complex implementation and prolonged response times. Typically, a {RAG} workflow involves multiple processing steps, each of which can be executed in various ways. Here, we investigate existing {RAG} approaches and their potential combinations to identify optimal {RAG} practices. Through extensive experiments, we suggest several strategies for deploying {RAG} that balance both performance and efficiency. Moreover, we demonstrate that multimodal retrieval techniques can significantly enhance question-answering capabilities about visual inputs and accelerate the generation of multimodal content using a “retrieval as generation” strategy. Resources are available at https://github.com/{FudanDNN}-{NLP}/{RAG}.},
	number = {{arXiv}:2407.01219},
	publisher = {{arXiv}},
	author = {Wang, Xiaohua and Wang, Zhenghua and Gao, Xuan and Zhang, Feiran and Wu, Yixin and Xu, Zhibo and Shi, Tianyuan and Wang, Zhengyuan and Li, Shizheng and Qian, Qi and Yin, Ruicheng and Lv, Changze and Zheng, Xiaoqing and Huang, Xuanjing},
	urldate = {2024-07-09},
	date = {2024-07-01},
	langid = {english},
	keywords = {notion},
	file = {Wang et al. - 2024 - Searching for Best Practices in Retrieval-Augmente.pdf:C\:\\Users\\Marco\\Zotero\\storage\\6LUL3RVZ\\Wang et al. - 2024 - Searching for Best Practices in Retrieval-Augmente.pdf:application/pdf},
}

@article{beel_research-paper_2016,
	title = {Research-paper recommender systems: a literature survey},
	volume = {17},
	issn = {1432-5012, 1432-1300},
	url = {http://link.springer.com/10.1007/s00799-015-0156-0},
	doi = {10.1007/s00799-015-0156-0},
	shorttitle = {Research-paper recommender systems},
	pages = {305--338},
	number = {4},
	journaltitle = {International Journal on Digital Libraries},
	shortjournal = {Int J Digit Libr},
	author = {Beel, Joeran and Gipp, Bela and Langer, Stefan and Breitinger, Corinna},
	urldate = {2024-07-09},
	date = {2016-11},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\GIBDL5ZA\\Beel et al. - 2016 - Research-paper recommender systems a literature survey.pdf:application/pdf},
}

@article{sakr_relational_2010,
	title = {Relational processing of {RDF} queries: a survey},
	volume = {38},
	issn = {0163-5808},
	url = {https://dl.acm.org/doi/10.1145/1815948.1815953},
	doi = {10.1145/1815948.1815953},
	shorttitle = {Relational processing of {RDF} queries},
	abstract = {The Resource Description Framework ({RDF}) is a ﬂexible model for representing information about resources in the web. With the increasing amount of {RDF} data which is becoming available, eﬃcient and scalable management of {RDF} data has become a fundamental challenge to achieve the Semantic Web vision. The {RDF} model has attracted the attention of the database community and many researchers have proposed diﬀerent solutions to store and query {RDF} data eﬃciently. This survey focuses on using relational query processors to store and query {RDF} data. We provide an overview of the diﬀerent approaches and classify them according to their storage and query evaluation strategies.},
	pages = {23--28},
	number = {4},
	journaltitle = {{ACM} {SIGMOD} Record},
	shortjournal = {{SIGMOD} Rec.},
	author = {Sakr, Sherif and Al-Naymat, Ghazi},
	urldate = {2024-07-09},
	date = {2010-06-27},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\Y6NWXY5J\\Sakr and Al-Naymat - 2010 - Relational processing of RDF queries a survey.pdf:application/pdf},
}

@misc{deng_rephrase_2024,
	title = {Rephrase and Respond: Let Large Language Models Ask Better Questions for Themselves},
	url = {http://arxiv.org/abs/2311.04205},
	shorttitle = {Rephrase and Respond},
	abstract = {Misunderstandings arise not only in interpersonal communication but also between humans and Large Language Models ({LLMs}). Such discrepancies can make {LLMs} interpret seemingly unambiguous questions in unexpected ways, yielding incorrect responses. While it is widely acknowledged that the quality of a prompt, such as a question, significantly impacts the quality of the response provided by {LLMs}, a systematic method for crafting questions that {LLMs} can better comprehend is still underdeveloped. In this paper, we present a method named ‘Rephrase and Respond’ ({RaR}), which allows {LLMs} to rephrase and expand questions posed by humans and provide responses in a single prompt. This approach serves as a simple yet effective prompting method for improving performance. We also introduce a two-step variant of {RaR}, where a rephrasing {LLM} first rephrases the question and then passes the original and rephrased questions together to a different responding {LLM}. This facilitates the effective utilization of rephrased questions generated by one {LLM} with another. Our experiments demonstrate that our methods significantly improve the performance of different models across a wide range to tasks. We further provide a comprehensive discussion with the popular Chain-of-Thought ({CoT}) methods. We show that {RaR} is complementary to {CoT} and can be combined with {CoT} to achieve even better performance. Our work not only contributes to enhancing {LLM} performance efficiently and effectively but also sheds light on a fair evaluation of {LLM} capabilities.},
	number = {{arXiv}:2311.04205},
	publisher = {{arXiv}},
	author = {Deng, Yihe and Zhang, Weitong and Chen, Zixiang and Gu, Quanquan},
	urldate = {2024-07-09},
	date = {2024-04-18},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\8XT7H4YD\\Deng et al. - 2024 - Rephrase and Respond Let Large Language Models Ask Better Questions for Themselves.pdf:application/pdf},
}

@misc{lewis_retrieval-augmented_2021,
	title = {Retrieval-Augmented Generation for Knowledge-Intensive {NLP} Tasks},
	url = {http://arxiv.org/abs/2005.11401},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when ﬁne-tuned on downstream {NLP} tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-speciﬁc architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pretrained models with a differentiable access mechanism to explicit non-parametric memory have so far been only investigated for extractive downstream tasks. We explore a general-purpose ﬁne-tuning recipe for retrieval-augmented generation ({RAG}) — models which combine pre-trained parametric and non-parametric memory for language generation. We introduce {RAG} models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two {RAG} formulations, one which conditions on the same retrieved passages across the whole generated sequence, and another which can use different passages per token. We ﬁne-tune and evaluate our models on a wide range of knowledgeintensive {NLP} tasks and set the state of the art on three open domain {QA} tasks, outperforming parametric seq2seq models and task-speciﬁc retrieve-and-extract architectures. For language generation tasks, we ﬁnd that {RAG} models generate more speciﬁc, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	number = {{arXiv}:2005.11401},
	publisher = {{arXiv}},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	urldate = {2024-07-09},
	date = {2021-04-12},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\KVAZBPJ7\\Lewis et al. - 2021 - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.pdf:application/pdf},
}

@misc{sun_open_2018,
	title = {Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text},
	url = {http://arxiv.org/abs/1809.00782},
	abstract = {Open Domain Question Answering ({QA}) is evolving from complex pipelined systems to end-to-end deep neural networks. Specialized neural models have been developed for extracting answers from either text alone or Knowledge Bases ({KBs}) alone. In this paper we look at a more practical setting, namely {QA} over the combination of a {KB} and entitylinked text, which is appropriate when an incomplete {KB} is available with a large text corpus. Building on recent advances in graph representation learning we propose a novel model, {GRAFT}-Net, for extracting answers from a question-speciﬁc subgraph containing text and {KB} entities and relations. We construct a suite of benchmark tasks for this problem, varying the difﬁculty of questions, the amount of training data, and {KB} completeness. We show that {GRAFT}-Net is competitive with the state-of-the-art when tested using either {KBs} or text alone, and vastly outperforms existing methods in the combined setting.},
	number = {{arXiv}:1809.00782},
	publisher = {{arXiv}},
	author = {Sun, Haitian and Dhingra, Bhuwan and Zaheer, Manzil and Mazaitis, Kathryn and Salakhutdinov, Ruslan and Cohen, William W.},
	urldate = {2024-07-09},
	date = {2018-09-03},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\VCZFBSF5\\Sun et al. - 2018 - Open Domain Question Answering Using Early Fusion of Knowledge Bases and Text.pdf:application/pdf},
}

@inproceedings{wu_pdfmef_2015,
	location = {Palisades {NY} {USA}},
	title = {{PDFMEF}: A Multi-Entity Knowledge Extraction Framework for Scholarly Documents and Semantic Search},
	isbn = {978-1-4503-3849-3},
	url = {https://dl.acm.org/doi/10.1145/2815833.2815834},
	doi = {10.1145/2815833.2815834},
	shorttitle = {{PDFMEF}},
	abstract = {We introduce {PDFMEF}, a multi-entity knowledge extraction framework for scholarly documents in the {PDF} format. It is implemented with a framework that encapsulates opensource extraction tools. Currently, it leverages {PDFBox} and {TET} for full text extraction, the scholarly document ﬁlter described in [5] for document classiﬁcation, {GROBID} for header extraction, {ParsCit} for citation extraction, {PDFFigures} for ﬁgure and table extraction, and algorithm extraction [27]. While it can be run as a whole, the extraction tool in each module is highly customizable. Users can substitute default extractors with other extraction tools they prefer by writing a thin wrapper to implement the abstracts. The framework is designed to be scalable and is capable of running in parallel using a multi-processing technique in Python. Experiments indicate that the system with default setups is {CPU} bounded, and leaves a small footprint in the memory, which makes it best to run on a multi-core machine. The best performance using a dedicated server of 16 cores takes 1.3 seconds on average to process one {PDF} document. It is used to index extracted information and help users to quickly locate relevant results in published scholarly documents and to eﬃciently construct a large knowledge base in order to build a semantic scholarly search engine. Part of it is running on {CiteSeerX} digital library search engine.},
	eventtitle = {K-{CAP} 2015: Knowledge Capture Conference},
	pages = {1--8},
	booktitle = {Proceedings of the 8th International Conference on Knowledge Capture},
	publisher = {{ACM}},
	author = {Wu, Jian and Killian, Jason and Yang, Huaiyu and Williams, Kyle and Choudhury, Sagnik Ray and Tuarob, Suppawong and Caragea, Cornelia and Giles, C. Lee},
	urldate = {2024-07-09},
	date = {2015-10-07},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\5WMJGV5S\\Wu et al. - 2015 - PDFMEF A Multi-Entity Knowledge Extraction Framework for Scholarly Documents and Semantic Search.pdf:application/pdf},
}

@misc{yasunaga_qa-gnn_2022,
	title = {{QA}-{GNN}: Reasoning with Language Models and Knowledge Graphs for Question Answering},
	url = {http://arxiv.org/abs/2104.06378},
	shorttitle = {{QA}-{GNN}},
	abstract = {The problem of answering questions using knowledge from pre-trained language models ({LMs}) and knowledge graphs ({KGs}) presents two challenges: given a {QA} context (question and answer choice), methods need to (i) identify relevant knowledge from large {KGs}, and (ii) perform joint reasoning over the {QA} context and {KG}. In this work, we propose a new model, {QA}-{GNN}, which addresses the above challenges through two key innovations: (i) relevance scoring, where we use {LMs} to estimate the importance of {KG} nodes relative to the given {QA} context, and (ii) joint reasoning, where we connect the {QA} context and {KG} to form a joint graph, and mutually update their representations through graph neural networks. We evaluate our model on {QA} benchmarks in the commonsense ({CommonsenseQA}, {OpenBookQA}) and biomedical ({MedQA}-{USMLE}) domains. {QA}-{GNN} outperforms existing {LM} and {LM}+{KG} models, and exhibits capabilities to perform interpretable and structured reasoning, e.g., correctly handling negation in questions. Our code and data are available at https: //github.com/michiyasunaga/qagnn.},
	number = {{arXiv}:2104.06378},
	publisher = {{arXiv}},
	author = {Yasunaga, Michihiro and Ren, Hongyu and Bosselut, Antoine and Liang, Percy and Leskovec, Jure},
	urldate = {2024-07-09},
	date = {2022-12-12},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\NTWY7DTJ\\Yasunaga et al. - 2022 - QA-GNN Reasoning with Language Models and Knowledge Graphs for Question Answering.pdf:application/pdf},
}

@misc{hoshi_ralle_2023,
	title = {{RaLLe}: A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models},
	url = {http://arxiv.org/abs/2308.10633},
	shorttitle = {{RaLLe}},
	abstract = {Retrieval-augmented large language models (R-{LLMs}) combine pre-trained large language models ({LLMs}) with information retrieval systems to improve the accuracy of factual question-answering. However, current libraries for building R-{LLMs} provide high-level abstractions without sufficient transparency for evaluating and optimizing prompts within specific inference processes such as retrieval and generation. To address this gap, we present {RALLE}, an open-source framework designed to facilitate the development, evaluation, and optimization of R-{LLMs} for knowledgeintensive tasks. With {RALLE}, developers can easily develop and evaluate R-{LLMs}, improving hand-crafted prompts, assessing individual inference processes, and objectively measuring overall system performance quantitatively. By leveraging these features, developers can enhance the performance and accuracy of their R-{LLMs} in knowledge-intensive generation tasks. We open-source our code at https: //github.com/yhoshi3/{RaLLe}.},
	number = {{arXiv}:2308.10633},
	publisher = {{arXiv}},
	author = {Hoshi, Yasuto and Miyashita, Daisuke and Ng, Youyang and Tatsuno, Kento and Morioka, Yasuhiro and Torii, Osamu and Deguchi, Jun},
	urldate = {2024-07-09},
	date = {2023-10-16},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\9I26DVG6\\Hoshi et al. - 2023 - RaLLe A Framework for Developing and Evaluating Retrieval-Augmented Large Language Models.pdf:application/pdf},
}

@article{jaradeh_open_2019,
	title = {Open Research Knowledge Graph: Towards Machine Actionability in Scholarly Communication},
	abstract = {Despite improved digital access to scientific publications in the last decades, the fundamental principles of scholarly communication remain unchanged and continue to be largely document-based. The document-oriented workflows in science publication have reached the limits of adequacy as highlighted by recent discussions on the increasing proliferation of scientific literature, the deficiency of peer-review and the reproducibility crisis. In this article, we present first steps towards representing scholarly knowledge semantically with knowledge graphs. We expand the currently popular {RDF} graph-based knowledge representation formalism to capture annotations, such as provenance information and describe how to manage such knowledge in a graph data base. We report on the results of a first experimental evaluation of the concept and its implementations with the participants of an international conference.},
	author = {Jaradeh, Mohamad Yaser and Auer, Sören and Prinz, Manuel and Kovtun, Viktor and Kismihók, Gábor and Stocker, Markus},
	date = {2019},
	langid = {english},
	keywords = {notion, technical\_report},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\32H6IYGW\\Jaradeh et al. - 2019 - Open Research Knowledge Graph Towards Machine Actionability in Scholarly Communication.pdf:application/pdf},
}

@inproceedings{jaradeh_open_2019-1,
	title = {Open Research Knowledge Graph: Next Generation Infrastructure for Semantic Scholarly Knowledge},
	doi = {10.1145/3360901.3364435},
	shorttitle = {Open Research Knowledge Graph},
	abstract = {Despite improved digital access to scholarly knowledge in recent decades, scholarly communication remains exclusively documentbased. In this form, scholarly knowledge is hard to process automatically. We present the first steps towards a knowledge graph based infrastructure that acquires scholarly knowledge in machine actionable form thus enabling new possibilities for scholarly knowledge curation, publication and processing. The primary contribution is to present, evaluate and discuss multi-modal scholarly knowledge acquisition, combining crowdsourced and automated techniques. We present the results of the first user evaluation of the infrastructure with the participants of a recent international conference. Results suggest that users were intrigued by the novelty of the proposed infrastructure and by the possibilities for innovative scholarly knowledge processing it could enable.},
	pages = {243--246},
	booktitle = {Proceedings of the 10th International Conference on Knowledge Capture},
	publisher = {{ACM}},
	author = {Jaradeh, Mohamad Yaser and Oelen, Allard and Farfar, Kheir Eddine and Prinz, Manuel and D'Souza, Jennifer and Kismihók, Gábor and Stocker, Markus and Auer, Sören},
	date = {2019-09-23},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\RE7L8BSU\\Jaradeh et al. - 2019 - Open Research Knowledge Graph Next Generation Infrastructure for Semantic Scholarly Knowledge.pdf:application/pdf},
}

@article{mons_nano-publication_nodate,
	title = {Nano-Publication in the e-science era},
	abstract = {The rate of data production in the Life Sciences has now reached such proportions that to consider it irresponsible to fund data generation without proper concomitant funding and infrastructure for storing, analyzing and exchanging the information and knowledge contained in, and extracted from, those data, is not an exaggerated position any longer. The chasm between data production and data handling has become so wide, that many data go unnoticed or at least run the risk of relative obscurity, fail to reveal the information contained in the data set or remains inaccessible due to ambiguity, or financial or legal toll-barriers. As a result, inconsistency, ambiguity and redundancy of data and information on the Web are becoming impediments to the performance of comprehensive information extraction and analysis. This paper attempts a stepwise explanation of the use of richly annotated {RDFstatements} as carriers of unambiguous, meta-analyzed information in the form of traceable nano-publications.},
	author = {Mons, Barend and Velterop, Jan},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\2GREIM66\\Mons and Velterop - Nano-Publication in the e-science era.pdf:application/pdf},
}

@misc{jin_large_2024,
	title = {Large Language Models on Graphs: A Comprehensive Survey},
	url = {http://arxiv.org/abs/2312.02783},
	shorttitle = {Large Language Models on Graphs},
	abstract = {Large language models ({LLMs}), such as {GPT}4 and {LLaMA}, are creating significant advancements in natural language processing, due to their strong text encoding/decoding ability and newly found emergent capability (e.g., reasoning). While {LLMs} are mainly designed to process pure texts, there are many real-world scenarios where text data is associated with rich structure information in the form of graphs (e.g., academic networks, and e-commerce networks) or scenarios where graph data are paired with rich textual information (e.g., molecules with descriptions). Besides, although {LLMs} have shown their pure textbased reasoning ability, it is underexplored whether such ability can be generalized to graphs (i.e., graph-based reasoning). In this paper, we provide a systematic review of scenarios and techniques related to large language models on graphs. We first summarize potential scenarios of adopting {LLMs} on graphs into three categories, namely pure graphs, text-attributed graphs, and text-paired graphs. We then discuss detailed techniques for utilizing {LLMs} on graphs, including {LLM} as Predictor, {LLM} as Encoder, and {LLM} as Aligner, and compare the advantages and disadvantages of different schools of models. Furthermore, we discuss the real-world applications of such methods and summarize open-source codes and benchmark datasets. Finally, we conclude with potential future research directions in this fast-growing field. The related source can be found at https://github.com/{PeterGriffinJin}/ Awesome-Language-Model-on-Graphs.},
	number = {{arXiv}:2312.02783},
	publisher = {{arXiv}},
	author = {Jin, Bowen and Liu, Gang and Han, Chi and Jiang, Meng and Ji, Heng and Han, Jiawei},
	urldate = {2024-07-09},
	date = {2024-02-01},
	langid = {english},
	keywords = {{KG}-based, {LLM}, notion, Retriever, Survey, Datasets, {KG}-fundamentals},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\R2FSMHBJ\\Jin et al. - 2024 - Large Language Models on Graphs A Comprehensive Survey.pdf:application/pdf},
}

@misc{muennighoff_mteb_2023,
	title = {{MTEB}: Massive Text Embedding Benchmark},
	url = {http://arxiv.org/abs/2210.07316},
	shorttitle = {{MTEB}},
	abstract = {Text embeddings are commonly evaluated on a small set of datasets from a single task not covering their possible applications to other tasks. It is unclear whether state-of-the-art embeddings on semantic textual similarity ({STS}) can be equally well applied to other tasks like clustering or reranking. This makes progress in the ﬁeld difﬁcult to track, as various models are constantly being proposed without proper evaluation. To solve this problem, we introduce the Massive Text Embedding Benchmark ({MTEB}). {MTEB} spans 8 embedding tasks covering a total of 58 datasets and 112 languages. Through the benchmarking of 33 models on {MTEB}, we establish the most comprehensive benchmark of text embeddings to date. We ﬁnd that no particular text embedding method dominates across all tasks. This suggests that the ﬁeld has yet to converge on a universal text embedding method and scale it up sufﬁciently to provide state-of-the-art results on all embedding tasks. {MTEB} comes with open-source code and a public leaderboard at https: //github.com/embeddings-benchm ark/mteb.},
	number = {{arXiv}:2210.07316},
	publisher = {{arXiv}},
	author = {Muennighoff, Niklas and Tazi, Nouamane and Magne, Loïc and Reimers, Nils},
	urldate = {2024-07-09},
	date = {2023-03-19},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\WKTVRKQR\\Muennighoff et al. - 2023 - MTEB Massive Text Embedding Benchmark.pdf:application/pdf},
}

@thesis{noauthor_master_nodate,
	title = {Master Thesis},
	type = {phdthesis},
	keywords = {academic search engine, Embedding-based, {KG}-based, {LLM}, notion, {RAG}, Framework, Metrics, Retrieval Evaluation, literature {QA} System},
}

@misc{baek_knowledge-augmented_2023,
	title = {Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2306.04136},
	abstract = {Large Language Models ({LLMs}) are capable of performing zero-shot closed-book question answering tasks, based on their internal knowledge stored in parameters during pre-training. However, such internalized knowledge might be insufficient and incorrect, which could lead {LLMs} to generate factually wrong answers. Furthermore, fine-tuning {LLMs} to update their knowledge is expensive. To this end, we propose to augment the knowledge directly in the input of {LLMs}. Specifically, we first retrieve the relevant facts to the input question from the knowledge graph based on semantic similarities between the question and its associated facts. After that, we prepend the retrieved facts to the input question in the form of the prompt, which is then forwarded to {LLMs} to generate the answer. Our framework, {KnowledgeAugmented} language model {PromptING} ({KAPING}), requires no model training, thus completely zero-shot. We validate the performance of our {KAPING} framework on the knowledge graph question answering task, that aims to answer the user’s question based on facts over a knowledge graph, on which ours outperforms relevant zero-shot baselines by up to 48\% in average, across multiple {LLMs} of various sizes.},
	number = {{arXiv}:2306.04136},
	publisher = {{arXiv}},
	author = {Baek, Jinheon and Aji, Alham Fikri and Saffari, Amir},
	urldate = {2024-07-09},
	date = {2023-06-07},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\AXK9AGBE\\Baek et al. - 2023 - Knowledge-Augmented Language Model Prompting for Zero-Shot Knowledge Graph Question Answering.pdf:application/pdf},
}

@inproceedings{wang_knowledge_2022,
	location = {Dalian, China},
	title = {Knowledge Graph-Based Semantic Ranking for Efficient Semantic Query},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-7061-2},
	url = {https://ieeexplore.ieee.org/document/9972953/},
	doi = {10.1109/ICCSNT56096.2022.9972953},
	abstract = {The latent domain knowledge plays an important role in helping to improve efficacy of information search. In order to utilize and mine the latent semantic knowledge behind the query, we propose a knowledge graph-based semantic ranking method for efficient semantic query, where the keywords-based syntactical query is semantically enriched by mining out the latent semantic knowledge of keywords based on knowledge graph. First, a neural network-based knowledge graph embedding model is proposed, where word embedding and entity embedding are learned and optimized using an endto-end approach, and the semantic knowledge from knowledge graph is fused into the distributed representations of entities. Second, the word-level similarity and entity-level similarity between queries and documents are modeled by interaction matrices, and the strong matching features and soft matching features of the interaction matrices are extracted respectively. Third, we rank documents according to the relevance scores calculated by the above two features. At last, the experiments were made over the related datasets against the traditional keywords-based method, and the results show that the method proposed in this paper can effectively improve the ranking performance.},
	eventtitle = {2022 {IEEE} 10th International Conference on Computer Science and Network Technology ({ICCSNT})},
	pages = {75--79},
	booktitle = {2022 {IEEE} 10th International Conference on Computer Science and Network Technology ({ICCSNT})},
	publisher = {{IEEE}},
	author = {Wang, Xuan and Lin, Jianchao and Ren, Chunhui and Chen, Jinming},
	urldate = {2024-07-09},
	date = {2022-10-22},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\VNAE8WTC\\Wang et al. - 2022 - Knowledge Graph-Based Semantic Ranking for Efficient Semantic Query.pdf:application/pdf},
}

@misc{wang_knowledge_2023,
	title = {Knowledge Graph Prompting for Multi-Document Question Answering},
	url = {http://arxiv.org/abs/2308.11730},
	abstract = {The ‘pre-train, prompt, predict’ paradigm of large language models ({LLMs}) has achieved remarkable success in opendomain question answering ({OD}-{QA}). However, few works explore this paradigm in multi-document question answering ({MD}-{QA}), a task demanding a thorough understanding of the logical associations among the contents and structures of documents. To fill this crucial gap, we propose a Knowledge Graph Prompting ({KGP}) method to formulate the right context in prompting {LLMs} for {MD}-{QA}, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph ({KG}) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or document structural relations. For graph traversal, we design an {LLMbased} graph traversal agent that navigates across nodes and gathers supporting passages assisting {LLMs} in {MD}-{QA}. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of {KGP} for {MD}-{QA}, signifying the potential of leveraging graphs in enhancing the prompt design and retrieval augmented generation for {LLMs}. Our code: https://github.com/{YuWVandy}/{KG}-{LLM}-{MDQA}.},
	number = {{arXiv}:2308.11730},
	publisher = {{arXiv}},
	author = {Wang, Yu and Lipka, Nedim and Rossi, Ryan A. and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
	urldate = {2024-07-09},
	date = {2023-12-25},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\8TQXWKAM\\Wang et al. - 2023 - Knowledge Graph Prompting for Multi-Document Question Answering.pdf:application/pdf},
}

@misc{kojima_large_2023,
	title = {Large Language Models are Zero-Shot Reasoners},
	url = {http://arxiv.org/abs/2205.11916},
	abstract = {Pretrained large language models ({LLMs}) are widely used in many sub-ﬁelds of natural language processing ({NLP}) and generally known as excellent few-shot learners with task-speciﬁc exemplars. Notably, chain of thought ({CoT}) prompting, a recent technique for eliciting complex multi-step reasoning through step-bystep answer examples, achieved the state-of-the-art performances in arithmetics and symbolic reasoning, difﬁcult system-2 tasks that do not follow the standard scaling laws for {LLMs}. While these successes are often attributed to {LLMs}’ ability for few-shot learning, we show that {LLMs} are decent zero-shot reasoners by simply adding “Let’s think step by step” before each answer. Experimental results demonstrate that our Zero-shot-{CoT}, using the same single prompt template, signiﬁcantly outperforms zero-shot {LLM} performances on diverse benchmark reasoning tasks including arithmetics ({MultiArith}, {GSM}8K, {AQUA}-{RAT}, {SVAMP}), symbolic reasoning (Last Letter, Coin Flip), and other logical reasoning tasks (Date Understanding, Tracking Shufﬂed Objects), without any hand-crafted few-shot examples, e.g. increasing the accuracy on {MultiArith} from 17.7\% to 78.7\% and {GSM}8K from 10.4\% to 40.7\% with large-scale {InstructGPT} model (text-davinci002), as well as similar magnitudes of improvements with another off-the-shelf large model, 540B parameter {PaLM}. The versatility of this single prompt across very diverse reasoning tasks hints at untapped and understudied fundamental zero-shot capabilities of {LLMs}, suggesting high-level, multi-task broad cognitive capabilities may be extracted by simple prompting. We hope our work not only serves as the minimal strongest zero-shot baseline for the challenging reasoning benchmarks, but also highlights the importance of carefully exploring and analyzing the enormous zero-shot knowledge hidden inside {LLMs} before crafting ﬁnetuning datasets or few-shot exemplars.},
	number = {{arXiv}:2205.11916},
	publisher = {{arXiv}},
	author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
	urldate = {2024-07-09},
	date = {2023-01-29},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\B55AN9E5\\Kojima et al. - 2023 - Large Language Models are Zero-Shot Reasoners.pdf:application/pdf},
}

@misc{wang_knowledgpt_2023,
	title = {{KnowledGPT}: Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases},
	url = {http://arxiv.org/abs/2308.11761},
	shorttitle = {{KnowledGPT}},
	abstract = {Large language models ({LLMs}) have demonstrated impressive impact in the field of natural language processing, but they still struggle with several issues regarding, such as completeness, timeliness, faithfulness and adaptability. While recent efforts have focuses on connecting {LLMs} with external knowledge sources, the integration of knowledge bases ({KBs}) remains understudied and faces several challenges. In this paper, we introduce {KnowledGPT}, a comprehensive framework to bridge {LLMs} with various knowledge bases, facilitating both the retrieval and storage of knowledge. The retrieval process employs the program of thought prompting, which generates search language for {KBs} in code format with pre-defined functions for {KB} operations. Besides retrieval, {KnowledGPT} offers the capability to store knowledge in a personalized {KB}, catering to individual user demands. With extensive experiments, we show that by integrating {LLMs} with {KBs}, {KnowledGPT} properly answers a broader range of questions requiring world knowledge compared with vanilla {LLMs}, utilizing both knowledge existing in widelyknown {KBs} and extracted into personalized {KBs}.},
	number = {{arXiv}:2308.11761},
	publisher = {{arXiv}},
	author = {Wang, Xintao and Yang, Qianwen and Qiu, Yongting and Liang, Jiaqing and He, Qianyu and Gu, Zhouhong and Xiao, Yanghua and Wang, Wei},
	urldate = {2024-07-09},
	date = {2023-08-17},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\VEQ43IYE\\Wang et al. - 2023 - KnowledGPT Enhancing Large Language Models with Retrieval and Storage Access on Knowledge Bases.pdf:application/pdf},
}

@article{wang_knowledge_2017,
	title = {Knowledge Graph Embedding: A Survey of Approaches and Applications},
	volume = {29},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1041-4347},
	url = {http://ieeexplore.ieee.org/document/8047276/},
	doi = {10.1109/TKDE.2017.2754499},
	shorttitle = {Knowledge Graph Embedding},
	abstract = {Knowledge graph ({KG}) embedding is to embed components of a {KG} including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the {KG}. It can beneﬁt a variety of downstream tasks such as {KG} completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the {KG} are ﬁrst introduced. We describe the overall framework, speciﬁc model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus speciﬁcally on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we brieﬂy introduce how {KG} embedding can be applied to and beneﬁt a wide variety of downstream tasks such as {KG} completion, relation extraction, question answering, and so forth.},
	pages = {2724--2743},
	number = {12},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	shortjournal = {{IEEE} Trans. Knowl. Data Eng.},
	author = {Wang, Quan and Mao, Zhendong and Wang, Bin and Guo, Li},
	urldate = {2024-07-09},
	date = {2017-12-01},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\8DF2KHKP\\Wang et al. - 2017 - Knowledge Graph Embedding A Survey of Approaches and Applications.pdf:application/pdf},
}

@article{reinanda_knowledge_2020,
	title = {Knowledge Graphs: An Information Retrieval Perspective},
	volume = {14},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/article/Details/INR-063},
	doi = {10.1561/1500000063},
	shorttitle = {Knowledge Graphs},
	pages = {289--444},
	number = {4},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	shortjournal = {{FNT} in Information Retrieval},
	author = {Reinanda, Ridho and Meij, Edgar and De Rijke, Maarten},
	urldate = {2024-07-09},
	date = {2020},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\UMEUIYIS\\Reinanda et al. - 2020 - Knowledge Graphs An Information Retrieval Perspective.pdf:application/pdf},
}

@book{fensel_knowledge_2020,
	location = {Cham},
	title = {Knowledge Graphs: Methodology, Tools and Selected Use Cases},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-030-37438-9 978-3-030-37439-6},
	url = {http://link.springer.com/10.1007/978-3-030-37439-6},
	shorttitle = {Knowledge Graphs},
	publisher = {Springer International Publishing},
	author = {Fensel, Dieter and Şimşek, Umutcan and Angele, Kevin and Huaman, Elwin and Kärle, Elias and Panasiuk, Oleksandra and Toma, Ioan and Umbrich, Jürgen and Wahler, Alexander},
	urldate = {2024-07-09},
	date = {2020},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\9MY5DMI2\\Fensel et al. - 2020 - Knowledge Graphs Methodology, Tools and Selected Use Cases.pdf:application/pdf},
}

@misc{pan_large_2023,
	title = {Large Language Models and Knowledge Graphs: Opportunities and Challenges},
	url = {http://arxiv.org/abs/2308.06374},
	shorttitle = {Large Language Models and Knowledge Graphs},
	abstract = {Large Language Models ({LLMs}) have taken Knowledge Representation -- and the world -- by storm. This inflection point marks a shift from explicit knowledge representation to a renewed focus on the hybrid representation of both explicit knowledge and parametric knowledge. In this position paper, we will discuss some of the common debate points within the community on {LLMs} (parametric knowledge) and Knowledge Graphs (explicit knowledge) and speculate on opportunities and visions that the renewed focus brings, as well as related research topics and challenges.},
	number = {{arXiv}:2308.06374},
	publisher = {{arXiv}},
	author = {Pan, Jeff Z. and Razniewski, Simon and Kalo, Jan-Christoph and Singhania, Sneha and Chen, Jiaoyan and Dietze, Stefan and Jabeen, Hajira and Omeliyanenko, Janna and Zhang, Wen and Lissandrini, Matteo and Biswas, Russa and de Melo, Gerard and Bonifati, Angela and Vakaj, Edlira and Dragoni, Mauro and Graux, Damien},
	urldate = {2024-07-09},
	date = {2023-08-11},
	langid = {english},
	keywords = {notion},
	file = {Pan et al. - 2023 - Large Language Models and Knowledge Graphs Opport.pdf:C\:\\Users\\Marco\\Zotero\\storage\\E9BPGXJI\\Pan et al. - 2023 - Large Language Models and Knowledge Graphs Opport.pdf:application/pdf},
}

@misc{edge_local_2024,
	title = {From Local to Global: A Graph {RAG} Approach to Query-Focused Summarization},
	url = {http://arxiv.org/abs/2404.16130},
	shorttitle = {From Local to Global},
	abstract = {The use of retrieval-augmented generation ({RAG}) to retrieve relevant information from an external knowledge source enables large language models ({LLMs}) to answer questions over private and/or previously unseen document collections. However, {RAG} fails on global questions directed at an entire text corpus, such as “What are the main themes in the dataset?”, since this is inherently a queryfocused summarization ({QFS}) task, rather than an explicit retrieval task. Prior {QFS} methods, meanwhile, fail to scale to the quantities of text indexed by typical {RAG} systems. To combine the strengths of these contrasting methods, we propose a Graph {RAG} approach to question answering over private text corpora that scales with both the generality of user questions and the quantity of source text to be indexed. Our approach uses an {LLM} to build a graph-based text index in two stages: first to derive an entity knowledge graph from the source documents, then to pregenerate community summaries for all groups of closely-related entities. Given a question, each community summary is used to generate a partial response, before all partial responses are again summarized in a final response to the user. For a class of global sensemaking questions over datasets in the 1 million token range, we show that Graph {RAG} leads to substantial improvements over a na¨ıve {RAG} baseline for both the comprehensiveness and diversity of generated answers. An open-source, Python-based implementation of both global and local Graph {RAG} approaches is forthcoming at https://aka.ms/graphrag.},
	number = {{arXiv}:2404.16130},
	publisher = {{arXiv}},
	author = {Edge, Darren and Trinh, Ha and Cheng, Newman and Bradley, Joshua and Chao, Alex and Mody, Apurva and Truitt, Steven and Larson, Jonathan},
	urldate = {2024-07-09},
	date = {2024-04-24},
	langid = {english},
	keywords = {notion, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\JYGE97MA\\Edge et al. - 2024 - From Local to Global A Graph RAG Approach to Query-Focused Summarization.pdf:application/pdf},
}

@article{dessi_generating_2021,
	title = {Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques within the scholarly domain},
	volume = {116},
	issn = {0167739X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167739X2033003X},
	doi = {10.1016/j.future.2020.10.026},
	abstract = {The continuous growth of scientific literature brings innovations and, at the same time, raises new challenges. One of them is related to the fact that its analysis has become difficult due to the high volume of published papers for which manual effort for annotations and management is required. Novel technological infrastructures are needed to help researchers, research policy makers, and companies to time-efficiently browse, analyse, and forecast scientific research. Knowledge graphs i.e., large networks of entities and relationships, have proved to be effective solution in this space. Scientific knowledge graphs focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. However, the current generation of knowledge graphs lacks of an explicit representation of the knowledge presented in the research papers. As such, in this paper, we present a new architecture that takes advantage of Natural Language Processing and Machine Learning methods for extracting entities and relationships from research publications and integrates them in a large-scale knowledge graph. Within this research work, we (i) tackle the challenge of knowledge extraction by employing several state-ofthe-art Natural Language Processing and Text Mining tools, (ii) describe an approach for integrating entities and relationships generated by these tools, (iii) show the advantage of such an hybrid system over alternative approaches, and (vi) as a chosen use case, we generated a scientific knowledge graph including 109,105 triples, extracted from 26,827 abstracts of papers within the Semantic Web domain. As our approach is general and can be applied to any domain, we expect that it can facilitate the management, analysis, dissemination, and processing of scientific knowledge.},
	pages = {253--264},
	journaltitle = {Future Generation Computer Systems},
	shortjournal = {Future Generation Computer Systems},
	author = {Dessì, Danilo and Osborne, Francesco and Reforgiato Recupero, Diego and Buscaldi, Davide and Motta, Enrico},
	urldate = {2024-07-09},
	date = {2021-03},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\YUELXHUU\\Dessì et al. - 2021 - Generating knowledge graphs by employing Natural Language Processing and Machine Learning techniques.pdf:application/pdf},
}

@misc{he_g-retriever_2024,
	title = {G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering},
	url = {http://arxiv.org/abs/2402.07630},
	shorttitle = {G-Retriever},
	abstract = {Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models ({LLMs}) and graph neural networks ({GNNs}) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering ({GraphQA}) benchmark with data collected from different tasks. Then, we propose our G-Retriever method, introducing the first retrieval-augmented generation ({RAG}) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the {LLM}'s context window size, G-Retriever performs {RAG} over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.{\textasciitilde}{\textbackslash}footnote\{Our codes and datasets are available at: {\textbackslash}url\{https://github.com/{XiaoxinHe}/G-Retriever\}\}},
	number = {{arXiv}:2402.07630},
	publisher = {{arXiv}},
	author = {He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V. and Laurent, Thomas and {LeCun}, Yann and Bresson, Xavier and Hooi, Bryan},
	urldate = {2024-07-09},
	date = {2024-05-27},
	langid = {english},
	keywords = {notion, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\ALFTRN89\\He et al. - 2024 - G-Retriever Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering.pdf:application/pdf},
}

@misc{fan_graph_2024,
	title = {Graph Machine Learning in the Era of Large Language Models ({LLMs})},
	url = {http://arxiv.org/abs/2404.14928},
	abstract = {Graphs play an important role in representing complex relationships in various domains like social networks, knowledge graphs, and molecular discovery. With the advent of deep learning, Graph Neural Networks ({GNNs}) have emerged as a cornerstone in Graph Machine Learning (Graph {ML}), facilitating the representation and processing of graph structures. Recently, {LLMs} have demonstrated unprecedented capabilities in language tasks and are widely adopted in a variety of applications such as computer vision and recommender systems. This remarkable success has also attracted interest in applying {LLMs} to the graph domain. Increasing efforts have been made to explore the potential of {LLMs} in advancing Graph {ML}’s generalization, transferability, and few-shot learning ability.},
	number = {{arXiv}:2404.14928},
	publisher = {{arXiv}},
	author = {Fan, Wenqi and Wang, Shijie and Huang, Jiani and Chen, Zhikai and Song, Yu and Tang, Wenzhuo and Mao, Haitao and Liu, Hui and Liu, Xiaorui and Yin, Dawei and Li, Qing},
	urldate = {2024-07-09},
	date = {2024-06-03},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\6DK79ANL\\Fan et al. - 2024 - Graph Machine Learning in the Era of Large Language Models (LLMs).pdf:application/pdf},
}

@misc{yu_evaluation_2024,
	title = {Evaluation of Retrieval-Augmented Generation: A Survey},
	url = {http://arxiv.org/abs/2405.07437},
	shorttitle = {Evaluation of Retrieval-Augmented Generation},
	abstract = {Retrieval-Augmented Generation ({RAG}) has emerged as a pivotal innovation in natural language processing, enhancing generative models by incorporating external information retrieval. Evaluating {RAG} systems, however, poses distinct challenges due to their hybrid structure and reliance on dynamic knowledge sources. We consequently enhanced an extensive survey and proposed an analysis framework for benchmarks of {RAG} systems, {RGAR} (Retrieval, Generation, Additional Requirement), designed to systematically analyze {RAG} benchmarks by focusing on measurable outputs and established truths. Specifically, we scrutinize and contrast multiple quantifiable metrics of the Retrieval and Generation component, such as relevance, accuracy, and faithfulness, of the internal links within the current {RAG} evaluation methods, covering the possible output and ground truth pairs. We also analyze the integration of additional requirements of different works, discuss the limitations of current benchmarks, and propose potential directions for further research to address these shortcomings and advance the field of {RAG} evaluation. In conclusion, this paper collates the challenges associated with {RAG} evaluation. It presents a thorough analysis and examination of existing methodologies for {RAG} benchmark design based on the proposed {RGAR} framework.},
	number = {{arXiv}:2405.07437},
	publisher = {{arXiv}},
	author = {Yu, Hao and Gan, Aoran and Zhang, Kai and Tong, Shiwei and Liu, Qi and Liu, Zhaofeng},
	urldate = {2024-07-09},
	date = {2024-07-03},
	langid = {english},
	keywords = {notion, {RAG}, Metrics, Retrieval Evaluation, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\NLHKGZND\\Yu et al. - 2024 - Evaluation of Retrieval-Augmented Generation A Survey.pdf:application/pdf},
}

@misc{salemi_evaluating_2024,
	title = {Evaluating Retrieval Quality in Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2404.13781},
	abstract = {Evaluating retrieval-augmented generation ({RAG}) presents challenges, particularly for retrieval models within these systems. Traditional end-to-end evaluation methods are computationally expensive. Furthermore, evaluation of the retrieval model’s performance based on query-document relevance labels shows a small correlation with the {RAG} system’s downstream performance. We propose a novel evaluation approach, {eRAG}, where each document in the retrieval list is individually utilized by the large language model within the {RAG} system. The output generated for each document is then evaluated based on the downstream task ground truth labels. In this manner, the downstream performance for each document serves as its relevance label. We employ various downstream task metrics to obtain document-level annotations and aggregate them using set-based or ranking metrics. Extensive experiments on a wide range of datasets demonstrate that {eRAG} achieves a higher correlation with downstream {RAG} performance compared to baseline methods, with improvements in Kendall’s 𝜏 correlation ranging from 0.168 to 0.494. Additionally, {eRAG} offers significant computational advantages, improving runtime and consuming up to 50 times less {GPU} memory than end-to-end evaluation.},
	number = {{arXiv}:2404.13781},
	publisher = {{arXiv}},
	author = {Salemi, Alireza and Zamani, Hamed},
	urldate = {2024-07-09},
	date = {2024-04-21},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\8ATU2A77\\Salemi and Zamani - 2024 - Evaluating Retrieval Quality in Retrieval-Augmented Generation.pdf:application/pdf},
}

@misc{hu_grag_2024,
	title = {{GRAG}: Graph Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2405.16506},
	shorttitle = {{GRAG}},
	abstract = {While Retrieval-Augmented Generation ({RAG}) enhances the accuracy and relevance of responses by generative language models, it falls short in graph-based contexts where both textual and topological information are important. Naive {RAG} approaches inherently neglect the structural intricacies of textual graphs, resulting in a critical gap in the generation process. To address this challenge, we introduce Graph Retrieval-Augmented Generation ({GRAG}), which significantly enhances both the retrieval and generation processes by emphasizing the importance of subgraph structures. Unlike {RAG} approaches that focus solely on text-based entity retrieval, {GRAG} maintains an acute awareness of graph topology, which is crucial for generating contextually and factually coherent responses. Our {GRAG} approach consists of four main stages: indexing of k-hop ego-graphs, graph retrieval, soft pruning to mitigate the impact of irrelevant entities, and generation with pruned textual subgraphs. {GRAG}’s core workflow—retrieving textual subgraphs followed by soft pruning—efficiently identifies relevant subgraph structures while avoiding the computational infeasibility typical of exhaustive subgraph searches, which are {NP}-hard. Moreover, we propose a novel prompting strategy that achieves lossless conversion from textual subgraphs to hierarchical text descriptions. Extensive experiments on graph multi-hop reasoning benchmarks demonstrate that in scenarios requiring multi-hop reasoning on textual graphs, our {GRAG} approach significantly outperforms current state-of-the-art {RAG} methods while effectively mitigating hallucinations.},
	number = {{arXiv}:2405.16506},
	publisher = {{arXiv}},
	author = {Hu, Yuntong and Lei, Zhihan and Zhang, Zheng and Pan, Bo and Ling, Chen and Zhao, Liang},
	urldate = {2024-07-09},
	date = {2024-05-26},
	langid = {english},
	keywords = {{KG}-based, {LLM}, notion, {RAG}, Metrics, Retrieval Evaluation, Retriever, Datasets},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\AQCWCEAQ\\Hu et al. - 2024 - GRAG Graph Retrieval-Augmented Generation.pdf:application/pdf},
}

@misc{yang_give_2024,
	title = {Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling},
	url = {http://arxiv.org/abs/2306.11489},
	shorttitle = {Give Us the Facts},
	abstract = {Recently, {ChatGPT}, a representative large language model ({LLM}), has gained considerable attention. Due to their powerful emergent abilities, recent {LLMs} are considered as a possible alternative to structured knowledge bases like knowledge graphs ({KGs}). However, while {LLMs} are proficient at learning probabilistic language patterns and engaging in conversations with humans, they, like previous smaller pre-trained language models ({PLMs}), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven {PLMs} with knowledge-based {KGs} to incorporate explicit factual knowledge into {PLMs}, thus improving their performance in generating texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing {PLMs} with {KGs}, detailing existing knowledge graph enhanced pre-trained language models ({KGPLMs}) as well as their applications. Inspired by existing studies on {KGPLM}, this paper proposes enhancing {LLMs} with {KGs} by developing knowledge graph-enhanced large language models ({KGLLMs}). {KGLLM} provides a solution to enhance {LLMs}’ factual reasoning ability, opening up new avenues for {LLM} research.},
	number = {{arXiv}:2306.11489},
	publisher = {{arXiv}},
	author = {Yang, Linyao and Chen, Hongyang and Li, Zhao and Ding, Xiao and Wu, Xindong},
	urldate = {2024-07-09},
	date = {2024-01-30},
	langid = {english},
	keywords = {notion, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\Q3SKWLC6\\Yang et al. - 2024 - Give Us the Facts Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Mod.pdf:application/pdf},
}

@article{guan_knowledge_2019,
	title = {Knowledge graph embedding with concepts},
	volume = {164},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705118304945},
	doi = {10.1016/j.knosys.2018.10.008},
	abstract = {Knowledge graph embedding aims to embed the entities and relationships of a knowledge graph in lowdimensional vector spaces, which can be widely applied to many tasks. Existing models for knowledge graph embedding primarily concentrate on entity–relation–entity triplets, or interact with the text corpus. However, triplets are less informative, and the in-domain text corpus is not always available, making the embedding results deviate from the actual meaning. At the same time, our mental world contains many concepts about worldly facts. For human cognition, compared to knowledge that we learned, common-sense concepts are more basic and general, and they play important roles in human knowledge accumulation. In this paper, based on common-sense concepts information of entities from a concept graph, we propose a Knowledge Graph Embedding with Concepts ({KEC}) model that embeds entities and concepts of entities jointly into a semantic space. The fact triplets from a knowledge graph are adjusted by the common-sense concept information of entities from a concept graph. Our model not only focuses on the relevance between entities but also focuses on their concepts. Thus, this model offers precise semantic embedding. We evaluate our method on the tasks of knowledge graph completion and entity classification. Experimental results show that our model outperforms other baselines on the two tasks.},
	pages = {38--44},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Guan, Niannian and Song, Dandan and Liao, Lejian},
	urldate = {2024-07-09},
	date = {2019-01},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\3CDUPG6L\\Guan et al. - 2019 - Knowledge graph embedding with concepts.pdf:application/pdf},
}

@inproceedings{konersmann_evaluation_2022,
	location = {Honolulu, {HI}, {USA}},
	title = {Evaluation Methods and Replicability of Software Architecture Research Objects},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-1728-0},
	doi = {10.1109/ICSA53651.2022.00023},
	abstract = {Objective: We aim at assessing the current state of practice of evaluating {SA} research objects and replication artifact provision in full technical conference papers from 2017 to 2021.
Method: We ﬁrst create a categorization of papers regarding their evaluation and provision of replication artifacts. In a systematic literature review ({SLR}) with 153 papers we then investigate how {SA} research objects are evaluated and how artifacts are made available.
Results: We found that technical experiments (28\%) and case studies (29\%) are the most frequently used evaluation methods over all research objects. Functional suitability (46\% of evaluated properties) and performance (29\%) are the most evaluated properties. 17 papers (11\%) provide replication packages and 97 papers (63\%) explicitly state threats to validity. 17\% of papers reference guidelines for evaluations and 14\% of papers reference guidelines for threats to validity.
Conclusions: Our results indicate that the generalizability and repeatability of evaluations could be improved to enhance the maturity of the ﬁeld; although, there are valid reasons for contributions to not publish their data. We derive from our ﬁndings a set of four proposals for improving the state of practice in evaluating software architecture research objects. Researchers can use our results to ﬁnd recommendations on relevant properties to evaluate and evaluation methods to use and to identify reusable evaluation artifacts to compare their novel ideas with other research. Reviewers can use our results to compare the evaluation and replicability of submissions with the state of the practice.},
	eventtitle = {2022 {IEEE} 19th International Conference on Software Architecture ({ICSA})},
	pages = {157--168},
	booktitle = {2022 {IEEE} 19th International Conference on Software Architecture ({ICSA})},
	publisher = {{IEEE}},
	author = {Konersmann, Marco and Kaplan, Angelika and Kuhn, Thomas and Heinrich, Robert and Koziolek, Anne and Reussner, Ralf and Jurjens, Jan and al-Doori, Mahmood and Boltz, Nicolas and Ehl, Marco and Fuchs, Dominik and Groser, Katharina and Hahner, Sebastian and Keim, Jan and Lohr, Matthias and Saglam, Timur and Schulz, Sophie and Toberg, Jan-Philipp},
	date = {2022-03},
	langid = {english},
	keywords = {notion},
	file = {Konersmann et al. - 2022 - Evaluation Methods and Replicability of Software A.pdf:C\:\\Users\\Marco\\Zotero\\storage\\JWG6XETV\\Konersmann et al. - 2022 - Evaluation Methods and Replicability of Software A.pdf:application/pdf},
}

@misc{alinejad_evaluating_2024,
	title = {Evaluating the Retrieval Component in {LLM}-Based Question Answering Systems},
	url = {http://arxiv.org/abs/2406.06458},
	abstract = {Question answering systems ({QA}) utilizing Large Language Models ({LLMs}) heavily depend on the retrieval component to provide them with domain-speciﬁc information and reduce the risk of generating inaccurate responses or hallucinations. Although the evaluation of retrievers dates back to the early research in Information Retrieval, assessing their performance within {LLM}-based chatbots remains a challenge.},
	number = {{arXiv}:2406.06458},
	publisher = {{arXiv}},
	author = {Alinejad, Ashkan and Kumar, Krtin and Vahdat, Ali},
	urldate = {2024-07-09},
	date = {2024-06-10},
	langid = {english},
	keywords = {notion},
	file = {Alinejad et al. - 2024 - Evaluating the Retrieval Component in LLM-Based Qu.pdf:C\:\\Users\\Marco\\Zotero\\storage\\ZATJXABW\\Alinejad et al. - 2024 - Evaluating the Retrieval Component in LLM-Based Qu.pdf:application/pdf},
}

@misc{ovadia_fine-tuning_2024,
	title = {Fine-Tuning or Retrieval? Comparing Knowledge Injection in {LLMs}},
	url = {http://arxiv.org/abs/2312.05934},
	shorttitle = {Fine-Tuning or Retrieval?},
	abstract = {Large language models ({LLMs}) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of {LLMs} on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation ({RAG}). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, {RAG} consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that {LLMs} struggle to learn new factual information through unsupervised fine-tuning, and that exposing them to numerous variations of the same fact during training could alleviate this problem.},
	number = {{arXiv}:2312.05934},
	publisher = {{arXiv}},
	author = {Ovadia, Oded and Brief, Menachem and Mishaeli, Moshik and Elisha, Oren},
	urldate = {2024-07-09},
	date = {2024-01-30},
	langid = {english},
	keywords = {notion},
	file = {Ovadia et al. - 2024 - Fine-Tuning or Retrieval Comparing Knowledge Inje.pdf:C\:\\Users\\Marco\\Zotero\\storage\\FI4MFSLW\\Ovadia et al. - 2024 - Fine-Tuning or Retrieval Comparing Knowledge Inje.pdf:application/pdf},
}

@article{duan_constraint-based_nodate,
	title = {Constraint-Based Question Answering with Knowledge Graph},
	abstract = {{WebQuestions} and {SimpleQuestions} are two benchmark data-sets commonly used in recent knowledge-based question answering ({KBQA}) work. Most questions in them are ‘simple’ questions which can be answered based on a single relation in the knowledge base. Such data-sets lack the capability of evaluating {KBQA} systems on complicated questions. Motivated by this issue, we release a new data-set, namely {ComplexQuestions}1, aiming to measure the quality of {KBQA} systems on ‘multi-constraint’ questions which require multiple knowledge base relations to get the answer. Beside, we propose a novel systematic {KBQA} approach to solve multi-constraint questions. Compared to state-of-the-art methods, our approach not only obtains comparable results on the two existing benchmark data-sets, but also achieves signiﬁcant improvements on the {ComplexQuestions}.},
	author = {Duan, Nan and Yan, Zhao and Zhou, Ming and Zhao, Tiejun},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\PPI9QJQL\\Duan et al. - Constraint-Based Question Answering with Knowledge Graph.pdf:application/pdf},
}

@misc{banerjee_dblp-quad_2023,
	title = {{DBLP}-{QuAD}: A Question Answering Dataset over the {DBLP} Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2303.13351},
	shorttitle = {{DBLP}-{QuAD}},
	abstract = {In this work we create a question answering dataset over the {DBLP} scholarly knowledge graph ({KG}). {DBLP} is an on-line reference for bibliographic information on major computer science publications that indexes over 4.4 million publications published by more than 2.2 million authors. Our dataset consists of 10,000 question answer pairs with the corresponding {SPARQL} queries which can be executed over the {DBLP} {KG} to fetch the correct answer. {DBLP}-{QuAD} is the largest scholarly question answering dataset.},
	number = {{arXiv}:2303.13351},
	publisher = {{arXiv}},
	author = {Banerjee, Debayan and Awale, Sushil and Usbeck, Ricardo and Biemann, Chris},
	urldate = {2024-07-09},
	date = {2023-03-29},
	langid = {english},
	keywords = {notion, finished, related\_dataset},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\5CNDQ4X4\\Banerjee et al. - 2023 - DBLP-QuAD A Question Answering Dataset over the DBLP Scholarly Knowledge Graph.pdf:application/pdf},
}

@article{mai_combining_nodate,
	title = {Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines},
	abstract = {The past decades have witnessed a rapid increase in the global scientiﬁc output as measured by published papers. Exploring a scientiﬁc ﬁeld and searching for relevant papers and authors seems like a needle-in-a-haystack problem. Although many academic search engines have been developed to accelerate this retrieval process, most of them rely on content-based methods and feature engineering. In this work, we present an entity retrieval prototype system on top of {IOS} Press {LD} Connect which utilizes both textual and structure information. Paragraph vector and knowledge graph embedding are used to embed papers and entities into low dimensional hidden space. Next, the semantic similarity between papers and entities can be measured based on the learned embedding models. Two benchmark datasets have been collected from Semantic Scholar and {DBLP} to evaluate the performance of our entity retrieval models. Results show that paragraph vectors are eﬀective at capturing the similarity and relatedness among papers and knowledge graph embedding models can preserve the inherent structure of the original knowledge graph and hence assist in link prediction tasks such as co-author inference.},
	author = {Mai, Gengchen and Janowicz, Krzysztof and Yan, Bo},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\FX5E6LUM\\Mai et al. - Combining Text Embedding and Knowledge Graph Embedding Techniques for Academic Search Engines.pdf:application/pdf},
}

@inproceedings{bornea_building_2013,
	location = {New York New York {USA}},
	title = {Building an efficient {RDF} store over a relational database},
	isbn = {978-1-4503-2037-5},
	url = {https://dl.acm.org/doi/10.1145/2463676.2463718},
	doi = {10.1145/2463676.2463718},
	abstract = {Efﬁcient storage and querying of {RDF} data is of increasing importance, due to the increased popularity and widespread acceptance of {RDF} on the web and in the enterprise. In this paper, we describe a novel storage and query mechanism for {RDF} which works on top of existing relational representations. Reliance on relational representations of {RDF} means that one can take advantage of 35+ years of research on efﬁcient storage and querying, industrial-strength transaction support, locking, security, etc. However, there are signiﬁcant challenges in storing {RDF} in relational, which include data sparsity and schema variability. We describe novel mechanisms to shred {RDF} into relational, and novel query translation techniques to maximize the advantages of this shredded representation. We show that these mechanisms result in consistently good performance across multiple {RDF} benchmarks, even when compared with current state-of-the-art stores. This work provides the basis for {RDF} support in {DB}2 v.10.1.},
	eventtitle = {{SIGMOD}/{PODS}'13: International Conference on Management of Data},
	pages = {121--132},
	booktitle = {Proceedings of the 2013 {ACM} {SIGMOD} International Conference on Management of Data},
	publisher = {{ACM}},
	author = {Bornea, Mihaela A. and Dolby, Julian and Kementsietsidis, Anastasios and Srinivas, Kavitha and Dantressangle, Patrick and Udrea, Octavian and Bhattacharjee, Bishwaranjan},
	urldate = {2024-07-09},
	date = {2013-06-22},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\K5GIQW38\\Bornea et al. - 2013 - Building an efficient RDF store over a relational database.pdf:application/pdf},
}

@article{ali_survey_2022,
	title = {A survey of {RDF} stores \& {SPARQL} engines for querying knowledge graphs},
	volume = {31},
	issn = {1066-8888, 0949-877X},
	url = {https://link.springer.com/10.1007/s00778-021-00711-3},
	doi = {10.1007/s00778-021-00711-3},
	abstract = {{RDF} has seen increased adoption in recent years, prompting the standardization of the {SPARQL} query language for {RDF}, and the development of local and distributed engines for processing {SPARQL} queries. This survey paper provides a comprehensive review of techniques and systems for querying {RDF} knowledge graphs. While other reviews on this topic tend to focus on the distributed setting, the main focus of the work is on providing a comprehensive survey of state-of-the-art storage, indexing and query processing techniques for efﬁciently evaluating {SPARQL} queries in a local setting (on one machine). To keep the survey self-contained, we also provide a short discussion on graph partitioning techniques used in the distributed setting. We conclude by discussing contemporary research challenges for further improving {SPARQL} query engines. An extended version also provides a survey of over one hundred {SPARQL} query engines and the techniques they use, along with twelve benchmarks and their features.},
	pages = {1--26},
	number = {3},
	journaltitle = {The {VLDB} Journal},
	shortjournal = {The {VLDB} Journal},
	author = {Ali, Waqas and Saleem, Muhammad and Yao, Bin and Hogan, Aidan and Ngomo, Axel-Cyrille Ngonga},
	urldate = {2024-07-09},
	date = {2022-05},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\WTXEPP3B\\Ali et al. - 2022 - A survey of RDF stores & SPARQL engines for querying knowledge graphs.pdf:application/pdf},
}

@incollection{ishita_creating_2020,
	location = {Cham},
	title = {Creating a Scholarly Knowledge Graph from Survey Article Tables},
	volume = {12504},
	isbn = {978-3-030-64451-2 978-3-030-64452-9},
	url = {https://link.springer.com/10.1007/978-3-030-64452-9_35},
	abstract = {Due to the lack of structure, scholarly knowledge remains hardly accessible for machines. Scholarly knowledge graphs have been proposed as a solution. Creating such a knowledge graph requires manual eﬀort and domain experts, and is therefore time-consuming and cumbersome. In this work, we present a human-in-the-loop methodology used to build a scholarly knowledge graph leveraging literature survey articles. Survey articles often contain manually curated and high-quality tabular information that summarizes ﬁndings published in the scientiﬁc literature. Consequently, survey articles are an excellent resource for generating a scholarly knowledge graph. The presented methodology consists of ﬁve steps, in which tables and references are extracted from {PDF} articles, tables are formatted and ﬁnally ingested into the knowledge graph. To evaluate the methodology, 92 survey articles, containing 160 survey tables, have been imported in the graph. In total, 2 626 papers have been added to the knowledge graph using the presented methodology. The results demonstrate the feasibility of our approach, but also indicate that manual eﬀort is required and thus underscore the important role of human experts.},
	pages = {373--389},
	booktitle = {Digital Libraries at Times of Massive Societal Transition},
	publisher = {Springer International Publishing},
	author = {Oelen, Allard and Stocker, Markus and Auer, Sören},
	editor = {Ishita, Emi and Pang, Natalie Lee San and Zhou, Lihong},
	urldate = {2024-07-09},
	date = {2020},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\JXZM9MCJ\\Oelen et al. - 2020 - Creating a Scholarly Knowledge Graph from Survey Article Tables.pdf:application/pdf},
}

@inproceedings{ammar_construction_2018,
	location = {New Orleans - Louisiana},
	title = {Construction of the Literature Graph in Semantic Scholar},
	url = {http://aclweb.org/anthology/N18-3011},
	doi = {10.18653/v1/N18-3011},
	abstract = {We describe a deployed scalable system for organizing published scientiﬁc literature into a heterogeneous graph to facilitate algorithmic manipulation and discovery. The resulting literature graph consists of more than 280M nodes, representing papers, authors, entities and various interactions between them (e.g., authorships, citations, entity mentions). We reduce literature graph construction into familiar {NLP} tasks (e.g., entity extraction and linking), point out research challenges due to diﬀerences from standard formulations of these tasks, and report empirical results for each task. The methods described in this paper are used to enable semantic features in www.semanticscholar.org.},
	eventtitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 3 (Industry Papers)},
	pages = {84--91},
	booktitle = {Proceedings of the 2018 Conference of the North American Chapter of           the Association for Computational Linguistics: Human Language           Technologies, Volume 3 (Industry Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Ammar, Waleed and Groeneveld, Dirk and Bhagavatula, Chandra and Beltagy, Iz and Crawford, Miles and Downey, Doug and Dunkelberger, Jason and Elgohary, Ahmed and Feldman, Sergey and Ha, Vu and Kinney, Rodney and Kohlmeier, Sebastian and Lo, Kyle and Murray, Tyler and Ooi, Hsu-Han and Peters, Matthew and Power, Joanna and Skjonsberg, Sam and Wang, Lucy and Willhelm, Chris and Yuan, Zheng and Zuylen, Madeleine and {Oren}},
	urldate = {2024-07-09},
	date = {2018},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\U3XZWDD4\\Ammar et al. - 2018 - Construction of the Literature Graph in Semantic Scholar.pdf:application/pdf},
}

@article{khalid_bert-embedding_nodate,
	title = {{BERT}-Embedding and Citation Network Analysis based Query Expansion Technique for Scholarly Search},
	abstract = {The enormous growth of research publications has made it challenging for academic search engines to bring the most relevant papers against the given search query. Numerous solutions have been proposed over the years to improve the effectiveness of academic search, including exploiting query expansion and citation analysis. Query expansion techniques mitigate the mismatch between the language used in a query and indexed documents. However, these techniques can suffer from introducing non-relevant information while expanding the original query. Recently, contextualized model {BERT} to document retrieval has been quite successful in query expansion. Motivated by such issues and inspired by the success of {BERT}, this paper proposes a novel approach called {QeBERT}. {QeBERT} exploits {BERT}-based embedding and Citation Network Analysis ({CNA}) in query expansion for improving scholarly search. Specifically, we use the context-aware {BERT}-embedding and {CNA} for query expansion in {PseudoRelevance} Feedback ({PRF}) fashion. Initial experimental results on the {ACL} dataset show that {BERT}-embedding can provide a valuable augmentation to query expansion and improve search relevance when combined with {CNA}.},
	author = {Khalid, Shah and Khusro, Shah and Alam, Aftab and Wahid, Abdul},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\W6GTF9A7\\Khalid et al. - BERT-Embedding and Citation Network Analysis based Query Expansion Technique for Scholarly Search.pdf:application/pdf},
}

@misc{agrawal_can_2024,
	title = {Can Knowledge Graphs Reduce Hallucinations in {LLMs}? : A Survey},
	url = {http://arxiv.org/abs/2311.07914},
	shorttitle = {Can Knowledge Graphs Reduce Hallucinations in {LLMs}?},
	abstract = {The contemporary {LLMs} are prone to producing hallucinations, stemming mainly from the knowledge gaps within the models. To address this critical limitation, researchers employ diverse strategies to augment the {LLMs} by incorporating external knowledge, aiming to reduce hallucinations and enhance reasoning accuracy. Among these strategies, leveraging knowledge graphs as a source of external information has demonstrated promising results. In this survey, we comprehensively review these knowledgegraph-based augmentation techniques in {LLMs}, focusing on their efficacy in mitigating hallucinations. We systematically categorize these methods into three overarching groups, offering methodological comparisons and performance evaluations. Lastly, this survey explores the current trends and challenges associated with these techniques and outlines potential avenues for future research in this emerging field.},
	number = {{arXiv}:2311.07914},
	publisher = {{arXiv}},
	author = {Agrawal, Garima and Kumarage, Tharindu and Alghamdi, Zeyad and Liu, Huan},
	urldate = {2024-07-09},
	date = {2024-03-15},
	langid = {english},
	keywords = {{KG}-based, {LLM}, notion, Retriever, Survey},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\U2ULYEVB\\Agrawal et al. - 2024 - Can Knowledge Graphs Reduce Hallucinations in LLMs  A Survey.pdf:application/pdf},
}

@article{abu-salih_domain-specific_2021,
	title = {Domain-specific knowledge graphs: A survey},
	volume = {185},
	issn = {10848045},
	doi = {10.1016/j.jnca.2021.103076},
	shorttitle = {Domain-specific knowledge graphs},
	abstract = {Knowledge Graphs ({KGs}) have made a qualitative leap and effected a real revolution in knowledge representation. This is leveraged by the underlying structure of the {KG} which underpins a better comprehension, reasoning and interpretation of knowledge for both human and machine. Therefore, {KGs} continue to be used as the main means of tackling a plethora of real-life problems in various domains. However, there is no consensus in regard to a plausible and inclusive definition of a domain-specific {KG}. Further, in conjunction with several limitations and deficiencies, various domain-specific {KG} construction approaches are far from perfect. This survey is the first to offer a comprehensive definition of a domain-specific {KG}. Also, the paper presents a thorough review of the state-of-the-art approaches drawn from academic works relevant to seven domains of knowledge. An examination of current approaches reveals a range of limitations and deficiencies. At the same time, uncharted territories on the research map are highlighted to tackle extant issues in the literature and point to directions for future research.},
	pages = {103076},
	journaltitle = {Journal of Network and Computer Applications},
	shortjournal = {Journal of Network and Computer Applications},
	author = {Abu-Salih, Bilal},
	date = {2021-07},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7ZGQ9HZP\\Abu-Salih - 2021 - Domain-specific knowledge graphs A survey.pdf:application/pdf},
}

@article{zhong_comprehensive_2024,
	title = {A Comprehensive Survey on Automatic Knowledge Graph Construction},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	url = {https://dl.acm.org/doi/10.1145/3618295},
	doi = {10.1145/3618295},
	abstract = {Automatic knowledge graph construction aims at manufacturing structured human knowledge. To this end, much effort has historically been spent extracting informative fact patterns from different data sources. However, more recently, research interest has shifted to acquiring conceptualized structured knowledge beyond informative data. In addition, researchers have also been exploring new ways of handling sophisticated construction tasks in diversified scenarios. Thus, there is a demand for a systematic review of paradigms to organize knowledge structures beyond data-level mentions. To meet this demand, we comprehensively survey more than 300 methods to summarize the latest developments in knowledge graph construction. A knowledge graph is built in three steps: knowledge acquisition, knowledge refinement, and knowledge evolution. The processes of knowledge acquisition are reviewed in detail, including obtaining entities with fine-grained types and their conceptual linkages to knowledge graphs; resolving coreferences; and extracting entity relationships in complex scenarios. The survey covers models for knowledge refinement, including knowledge graph completion, and knowledge fusion. Methods to handle knowledge evolution are also systematically presented, including condition knowledge acquisition, condition knowledge graph completion, and knowledge dynamic. We present the paradigms to compare the distinction among these methods along the axis of the data environment, motivation, and architecture. Additionally, we also provide briefs on accessible resources that can help readers to develop practical knowledge graph systems. The survey concludes with discussions on the challenges and possible directions for future exploration.},
	pages = {1--62},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Zhong, Lingfeng and Wu, Jia and Li, Qian and Peng, Hao and Wu, Xindong},
	urldate = {2024-07-09},
	date = {2024-04-30},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\CT96C27I\\Zhong et al. - 2024 - A Comprehensive Survey on Automatic Knowledge Graph Construction.pdf:application/pdf},
}

@book{serles_introduction_2024,
	location = {Cham},
	title = {An Introduction to Knowledge Graphs},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-031-45255-0 978-3-031-45256-7},
	url = {https://link.springer.com/10.1007/978-3-031-45256-7},
	publisher = {Springer Nature Switzerland},
	author = {Serles, Umutcan and Fensel, Dieter},
	urldate = {2024-07-09},
	date = {2024},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\REQGD4KK\\Serles and Fensel - 2024 - An Introduction to Knowledge Graphs.pdf:application/pdf},
}

@inproceedings{pang_survey_2022,
	location = {Xiamen, China},
	title = {A Survey on Information Retrieval Method for Knowledge Graph Complex Question Answering},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-6654-6533-5},
	url = {https://ieeexplore.ieee.org/document/10055934/},
	doi = {10.1109/CAC57257.2022.10055934},
	abstract = {Knowledge graph question answering ({KGQA}) uses structured knowledge in the knowledge graph ({KG}) to answer natural language questions. Currently, simple question answering has been well solved, but the performance of complex question answering needs to be improved. In this survey, we ﬁrst present the background knowledge about complex question answering based on {KG}. Then the information retrieval-based ({IR}-based) method is introduced. We describe each procedure in the {IRbased} method in detail and investigate the related tools. Finally, the work is summarized and future research is envisioned based on the problems of current the {IR}-based method directions. With this survey, we aim to provide novices in the ﬁeld of {KGQA} with an entry point to a suitable the {IR}-based method, where each step of the method can be understood.},
	eventtitle = {2022 China Automation Congress ({CAC})},
	pages = {1059--1064},
	booktitle = {2022 China Automation Congress ({CAC})},
	publisher = {{IEEE}},
	author = {Pang, Junbiao and Zhang, Yongheng and Deng, Jiaxin and Zhu, Xiaoqing},
	urldate = {2024-07-09},
	date = {2022-11-25},
	langid = {english},
	keywords = {{KG}-based, notion, Retrieval Evaluation, Retriever, Survey, {KG}-fundamentals},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\H4QZZSQY\\Pang et al. - 2022 - A Survey on Information Retrieval Method for Knowledge Graph Complex Question Answering.pdf:application/pdf},
}

@article{chen_review_2020,
	title = {A review: Knowledge reasoning over knowledge graph},
	volume = {141},
	issn = {09574174},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0957417419306669},
	doi = {10.1016/j.eswa.2019.112948},
	shorttitle = {A review},
	abstract = {Mining valuable hidden knowledge from large-scale data relies on the support of reasoning technology. Knowledge graphs, as a new type of knowledge representation, have gained much attention in natural language processing. Knowledge graphs can effectively organize and represent knowledge so that it can be eﬃciently utilized in advanced applications. Recently, reasoning over knowledge graphs has become a hot research topic, since it can obtain new knowledge and conclusions from existing data. Herein we review the basic concept and deﬁnitions of knowledge reasoning and the methods for reasoning over knowledge graphs. Speciﬁcally, we dissect the reasoning methods into three categories: rule-based reasoning, distributed representation-based reasoning and neural network-based reasoning. We also review the related applications of knowledge graph reasoning, such as knowledge graph completion, question answering, and recommender systems. Finally, we discuss the remaining challenges and research opportunities for knowledge graph reasoning.},
	pages = {112948},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Chen, Xiaojun and Jia, Shengbin and Xiang, Yang},
	urldate = {2024-07-09},
	date = {2020-03},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\U7FYB5QJ\\Chen et al. - 2020 - A review Knowledge reasoning over knowledge graph.pdf:application/pdf},
}

@misc{zhang_bertscore_2020,
	title = {{BERTScore}: Evaluating Text Generation with {BERT}},
	url = {http://arxiv.org/abs/1904.09675},
	shorttitle = {{BERTScore}},
	abstract = {We propose {BERTSCORE}, an automatic evaluation metric for text generation. Analogously to common metrics, {BERTSCORE} computes a similarity score for each token in the candidate sentence with each token in the reference sentence. However, instead of exact matches, we compute token similarity using contextual embeddings. We evaluate using the outputs of 363 machine translation and image captioning systems. {BERTSCORE} correlates better with human judgments and provides stronger model selection performance than existing metrics. Finally, we use an adversarial paraphrase detection task to show that {BERTSCORE} is more robust to challenging examples when compared to existing metrics.},
	number = {{arXiv}:1904.09675},
	publisher = {{arXiv}},
	author = {Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
	urldate = {2024-07-09},
	date = {2020-02-24},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZS69NW7U\\Zhang et al. - 2020 - BERTScore Evaluating Text Generation with BERT.pdf:application/pdf},
}

@inproceedings{huang_embedding-based_2020,
	title = {Embedding-based Retrieval in Facebook Search},
	url = {http://arxiv.org/abs/2006.11632},
	doi = {10.1145/3394486.3403305},
	abstract = {Search in social networks such as Facebook poses different challenges than in classical web search: besides the query text, it is important to take into account the searcher’s context to provide relevant results. Their social graph is an integral part of this context and is a unique aspect of Facebook search. While embedding-based retrieval ({EBR}) has been applied in web search engines for years, Facebook search was still mainly based on a Boolean matching model. In this paper, we discuss the techniques for applying {EBR} to a Facebook Search system. We introduce the unified embedding framework developed to model semantic embeddings for personalized search, and the system to serve embedding-based retrieval in a typical search system based on an inverted index. We discuss various tricks and experiences on end-to-end optimization of the whole system, including {ANN} parameter tuning and full-stack optimization. Finally, we present our progress on two selected advanced topics about modeling. We evaluated {EBR} on verticals1 for Facebook Search with significant metrics gains observed in online A/B experiments. We believe this paper will provide useful insights and experiences to help people on developing embedding-based retrieval systems in search engines.},
	pages = {2553--2561},
	booktitle = {Proceedings of the 26th {ACM} {SIGKDD} International Conference on Knowledge Discovery \& Data Mining},
	author = {Huang, Jui-Ting and Sharma, Ashish and Sun, Shuying and Xia, Li and Zhang, David and Pronin, Philip and Padmanabhan, Janani and Ottaviano, Giuseppe and Yang, Linjun},
	urldate = {2024-07-09},
	date = {2020-08-23},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\E6RCX8WT\\Huang et al. - 2020 - Embedding-based Retrieval in Facebook Search.pdf:application/pdf},
}

@article{vaswani_attention_nodate,
	title = {Attention is All you Need},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	langid = {english},
	keywords = {notion},
	file = {Vaswani et al. - Attention is All you Need.pdf:C\:\\Users\\Marco\\Zotero\\storage\\NP7RMKQR\\Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@misc{wu_retrieve-rewrite-answer_2023,
	title = {Retrieve-Rewrite-Answer: A {KG}-to-Text Enhanced {LLMs} Framework for Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2309.11206},
	shorttitle = {Retrieve-Rewrite-Answer},
	abstract = {Despite their competitive performance on knowledge-intensive tasks, large language models ({LLMs}) still have limitations in memorizing all world knowledge especially long tail knowledge. In this paper, we study the {KG}-augmented language model approach for solving the knowledge graph question answering ({KGQA}) task that requires rich world knowledge. Existing work has shown that retrieving {KG} knowledge to enhance {LLMs} prompting can significantly improve {LLMs} performance in {KGQA}. However, their approaches lack a well-formed verbalization of {KG} knowledge, i.e., they ignore the gap between {KG} representations and textual representations. To this end, we propose an answer-sensitive {KG}-to-Text approach that can transform {KG} knowledge into well-textualized statements most informative for {KGQA}. Based on this approach, we propose a {KG}-to-Text enhanced {LLMs} framework for solving the {KGQA} task. Experiments on several {KGQA} benchmarks show that the proposed {KG}-to-Text augmented {LLMs} approach outperforms previous {KG}-augmented {LLMs} approaches regarding answer accuracy and usefulness of knowledge statements.},
	number = {{arXiv}:2309.11206},
	publisher = {{arXiv}},
	author = {Wu, Yike and Hu, Nan and Bi, Sheng and Qi, Guilin and Ren, Jie and Xie, Anhuan and Song, Wei},
	urldate = {2024-07-09},
	date = {2023-09-21},
	keywords = {{KG}-based, {LLM}, notion, Retrieval Evaluation, Retriever, Datasets, training-based},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\94WKA94H\\Wu et al. - 2023 - Retrieve-Rewrite-Answer A KG-to-Text Enhanced LLMs Framework for Knowledge Graph Question Answering.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\DD9BL24H\\2309.html:text/html},
}

@misc{he_rethinking_2022,
	title = {Rethinking with Retrieval: Faithful Large Language Model Inference},
	url = {http://arxiv.org/abs/2301.00303},
	shorttitle = {Rethinking with Retrieval},
	abstract = {Despite the success of large language models ({LLMs}) in various natural language processing ({NLP}) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist {LLMs}. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for {LLMs}. To address this issue, we propose a novel post-processing approach, rethinking with retrieval ({RR}), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought ({CoT}) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of {LLMs}. We evaluate the effectiveness of {RR} through extensive experiments with {GPT}-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that {RR} can produce more faithful explanations and improve the performance of {LLMs}.},
	number = {{arXiv}:2301.00303},
	publisher = {{arXiv}},
	author = {He, Hangfeng and Zhang, Hongming and Roth, Dan},
	urldate = {2024-07-09},
	date = {2022-12-31},
	keywords = {{KG}-based, notion, Retriever},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\YGCHVU2J\\He et al. - 2022 - Rethinking with Retrieval Faithful Large Language Model Inference.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\V6XKQYC3\\2301.html:text/html},
}

@misc{trivedi_interleaving_2023,
	title = {Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions},
	url = {http://arxiv.org/abs/2212.10509},
	abstract = {Prompting-based large language models ({LLMs}) are surprisingly powerful at generating natural language reasoning steps or Chains-of-Thoughts ({CoT}) for multi-step question answering ({QA}). They struggle, however, when the necessary knowledge is either unavailable to the {LLM} or not up-to-date within its parameters. While using the question to retrieve relevant text from an external knowledge source helps {LLMs}, we observe that this one-step retrieve-and-read approach is insufficient for multi-step {QA}. Here, {\textbackslash}textit\{what to retrieve\} depends on {\textbackslash}textit\{what has already been derived\}, which in turn may depend on {\textbackslash}textit\{what was previously retrieved\}. To address this, we propose {IRCoT}, a new approach for multi-step {QA} that interleaves retrieval with steps (sentences) in a {CoT}, guiding the retrieval with {CoT} and in turn using retrieved results to improve {CoT}. Using {IRCoT} with {GPT}3 substantially improves retrieval (up to 21 points) as well as downstream {QA} (up to 15 points) on four datasets: {HotpotQA}, 2WikiMultihopQA, {MuSiQue}, and {IIRC}. We observe similar substantial gains in out-of-distribution ({OOD}) settings as well as with much smaller models such as Flan-T5-large without additional training. {IRCoT} reduces model hallucination, resulting in factually more accurate {CoT} reasoning. Code, data, and prompts are available at {\textbackslash}url\{https://github.com/stonybrooknlp/ircot\}},
	number = {{arXiv}:2212.10509},
	publisher = {{arXiv}},
	author = {Trivedi, Harsh and Balasubramanian, Niranjan and Khot, Tushar and Sabharwal, Ashish},
	urldate = {2024-07-09},
	date = {2023-06-22},
	keywords = {{KG}-based, notion, Retriever},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\Z7BNHQMQ\\Trivedi et al. - 2023 - Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\SW9BEJQM\\2212.html:text/html},
}

@misc{liu_towards_2024,
	title = {Towards Graph Foundation Models: A Survey and Beyond},
	url = {http://arxiv.org/abs/2310.11829},
	shorttitle = {Towards Graph Foundation Models},
	abstract = {Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase significant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is witnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation models to generalize and adapt motivate graph machine learning researchers to discuss the potential of developing a new graph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various graph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this new domain. To this end, this article introduces the concept of Graph Foundation Models ({GFMs}), and offers an exhaustive explanation of their key characteristics and underlying technologies. We proceed to classify the existing work related to {GFMs} into three distinct categories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough review of the current state of {GFMs}, this article also outlooks potential avenues for future research in this rapidly evolving domain.},
	number = {{arXiv}:2310.11829},
	publisher = {{arXiv}},
	author = {Liu, Jiawei and Yang, Cheng and Lu, Zhiyuan and Chen, Junze and Li, Yibo and Zhang, Mengmei and Bai, Ting and Fang, Yuan and Sun, Lichao and Yu, Philip S. and Shi, Chuan},
	urldate = {2024-07-10},
	date = {2024-06-30},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\56AUMUI6\\Liu et al. - 2024 - Towards Graph Foundation Models A Survey and Beyond.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\XB4A3M8S\\2310.html:text/html},
}

@misc{kang_knowledge_2023,
	title = {Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation},
	url = {http://arxiv.org/abs/2305.18846},
	abstract = {Language models have achieved impressive performances on dialogue generation tasks. However, when generating responses for a conversation that requires factual knowledge, they are far from perfect, due to an absence of mechanisms to retrieve, encode, and reflect the knowledge in the generated responses. Some knowledge-grounded dialogue generation methods tackle this problem by leveraging facts from Knowledge Graphs ({KGs}); however, they do not guarantee that the model utilizes a relevant piece of knowledge from the {KG}. To overcome this limitation, we propose {SUbgraph} Retrieval-augmented {GEneration} ({SURGE}), a framework for generating context-relevant and knowledge-grounded dialogues with the {KG}. Specifically, our {SURGE} framework first retrieves the relevant subgraph from the {KG}, and then enforces consistency across facts by perturbing their word embeddings conditioned by the retrieved subgraph. Then, we utilize contrastive learning to ensure that the generated texts have high similarity to the retrieved subgraphs. We validate our {SURGE} framework on {OpendialKG} and {KOMODIS} datasets, showing that it generates high-quality dialogues that faithfully reflect the knowledge from {KG}.},
	number = {{arXiv}:2305.18846},
	publisher = {{arXiv}},
	author = {Kang, Minki and Kwak, Jin Myung and Baek, Jinheon and Hwang, Sung Ju},
	urldate = {2024-07-14},
	date = {2023-05-30},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\EEXHXE3W\\Kang et al. - 2023 - Knowledge Graph-Augmented Language Models for Knowledge-Grounded Dialogue Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2B3RQNLB\\2305.html:text/html},
}

@inproceedings{kaplan_combining_2024,
	title = {Combining Knowledge Graphs and Large Language Models to Ease Knowledge Access in Software Architecture Research},
	url = {https://publikationen.bibliothek.kit.edu/1000171637},
	pages = {76},
	booktitle = {{SemTech}4STLD 2024 : Semantic Technologies and Deep Learning Models for Scientific, Technical and Legal Data 2024 : Second International Workshop on Semantic Technologies and Deep Learning Models for Scientific, Technical and Legal Data ({SemTech}4STLD) co-located with the Extended Semantic Web Conference 2024 ({ESWC} 2024). Ed.: R. Dessi},
	author = {Kaplan, Angelika and Keim, Jan and Schneider, Marco and Koziolek, Anne and Reussner, Ralf},
	urldate = {2024-07-16},
	date = {2024},
	langid = {german},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SYSBQ242\\Kaplan et al. - 2024 - Combining Knowledge Graphs and Large Language Models to Ease Knowledge Access in Software Architectu.pdf:application/pdf},
}

@inproceedings{konersmann_replication_2022,
	title = {Replication Package of "Evaluation Methods and Replicability of Software Architecture Research Objects"},
	url = {https://ieeexplore.ieee.org/document/9779823},
	doi = {10.1109/ICSA-C54293.2022.00021},
	abstract = {In "Evaluation Methods and Replicability of Software Architecture Research" [1], we present a systematic literature review to assess the state-of-practice of evaluating software architecture research objects and providing replication artifacts in 153 full technical conference papers published at the International and European Conference on Software Architecture ({ICSA} respectively {ECSA}) from 2017 to 2021.},
	eventtitle = {2022 {IEEE} 19th International Conference on Software Architecture Companion ({ICSA}-C)},
	pages = {58--58},
	booktitle = {2022 {IEEE} 19th International Conference on Software Architecture Companion ({ICSA}-C)},
	author = {Konersmann, Marco and Kaplan, Angelika and Kühn, Thomas and Heinrich, Robert and Koziolek, Anne and Reussner, Ralf and Jürjens, Jan and Al-Doori, Mahmood and Boltz, Nicolas and Ehl, Marco and Fuchß, Dominik and Großer, Katharina and Hahner, Sebastian and Keim, Jan and Lohr, Matthias and Sağlam, Timur and Schulz, Sophie and Töberg, Jan-Philipp},
	urldate = {2024-07-18},
	date = {2022-03},
	keywords = {notion},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\T2I23U3U\\9779823.html:text/html},
}

@article{berners-lee_scientific_2001,
	title = {Scientific publishing on the 'semantic web'},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature28055},
	doi = {10.1038/nature28055},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Berners-Lee, Tim and Hendler, James},
	urldate = {2024-07-19},
	date = {2001-04-12},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\FX4GRKVM\\Berners-Lee and Hendler - 2001 - Scientific publishing on the 'semantic web'.pdf:application/pdf},
}

@article{von_hippel_improve_2023,
	title = {Improve academic search engines to reduce scholars’ biases},
	volume = {7},
	rights = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-022-01518-0},
	doi = {10.1038/s41562-022-01518-0},
	pages = {157--158},
	number = {2},
	journaltitle = {Nature Human Behaviour},
	shortjournal = {Nat Hum Behav},
	author = {von Hippel, Paul T. and Buck, Stuart},
	urldate = {2024-07-19},
	date = {2023-02},
	langid = {english},
	keywords = {notion, technical\_report},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6MKDUGTE\\von Hippel and Buck - 2023 - Improve academic search engines to reduce scholars’ biases.pdf:application/pdf},
}

@misc{zhang_sirens_2023,
	title = {Siren's Song in the {AI} Ocean: A Survey on Hallucination in Large Language Models},
	url = {http://arxiv.org/abs/2309.01219},
	doi = {10.48550/arXiv.2309.01219},
	shorttitle = {Siren's Song in the {AI} Ocean},
	abstract = {While large language models ({LLMs}) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: {LLMs} occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of {LLMs} in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by {LLMs}. We present taxonomies of the {LLM} hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating {LLM} hallucination, and discuss potential directions for future research.},
	number = {{arXiv}:2309.01219},
	publisher = {{arXiv}},
	author = {Zhang, Yue and Li, Yafu and Cui, Leyang and Cai, Deng and Liu, Lemao and Fu, Tingchen and Huang, Xinting and Zhao, Enbo and Zhang, Yu and Chen, Yulong and Wang, Longyue and Luu, Anh Tuan and Bi, Wei and Shi, Freda and Shi, Shuming},
	urldate = {2024-07-21},
	date = {2023-09-24},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\2A8479QT\\Zhang et al. - 2023 - Siren's Song in the AI Ocean A Survey on Hallucination in Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\RLDUDDRD\\2309.html:text/html},
}

@misc{wei_emergent_2022,
	title = {Emergent Abilities of Large Language Models},
	doi = {10.48550/arXiv.2206.07682},
	abstract = {Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.},
	number = {{arXiv}:2206.07682},
	publisher = {{arXiv}},
	author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and Chi, Ed H. and Hashimoto, Tatsunori and Vinyals, Oriol and Liang, Percy and Dean, Jeff and Fedus, William},
	date = {2022-10-26},
	keywords = {notion},
	file = {arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\223KRHKP\\2206.html:text/html;Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GPWXXLY7\\Wei et al. - 2022 - Emergent Abilities of Large Language Models.pdf:application/pdf},
}

@misc{li_academic_2022,
	title = {Academic Search Engines: Constraints, Bugs, and Recommendation},
	url = {http://arxiv.org/abs/2211.00361},
	doi = {10.48550/arXiv.2211.00361},
	shorttitle = {Academic Search Engines},
	abstract = {Background: Academic search engines (i.e., digital libraries and indexers) play an increasingly important role in systematic reviews however these engines do not seem to effectively support such reviews, e.g., researchers confront usability issues with the engines when conducting their searches. Aims: To investigate whether the usability issues are bugs (i.e., faults in the search engines) or constraints, and to provide recommendations to search-engine providers and researchers on how to tackle these issues. Method: Using snowball-sampling from tertiary studies, we identify a set of 621 secondary studies in software engineering. By physically re-attempting the searches for all of these 621 studies, we effectively conduct regression testing for 42 search engines. Results: We identify 13 bugs for eight engines, and also identify other constraints. We provide recommendations for tackling these issues. Conclusions: There is still a considerable gap between the search-needs of researchers and the usability of academic search engines. It is not clear whether search-engine developers are aware of this gap. Also, the evaluation, by academics, of academic search engines has not kept pace with the development, by search-engine providers, of those search engines. Thus, the gap between evaluation and development makes it harder to properly understand the gap between the search-needs of researchers and search-features of the search engines.},
	number = {{arXiv}:2211.00361},
	publisher = {{arXiv}},
	author = {Li, Zheng and Rainer, Austen},
	urldate = {2024-07-21},
	date = {2022-11-01},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\DWKX2AQB\\Li and Rainer - 2022 - Academic Search Engines Constraints, Bugs, and Recommendation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\R4YS5QBU\\2211.html:text/html},
}

@article{berners-lee_scientific_2001-1,
	title = {Scientific publishing on the 'semantic web'},
	rights = {http://www.springer.com/tdm},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/nature28055},
	doi = {10.1038/nature28055},
	journaltitle = {Nature},
	shortjournal = {Nature},
	author = {Berners-Lee, Tim and Hendler, James},
	urldate = {2024-07-21},
	date = {2001-04-12},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7YTSNM7W\\Berners-Lee and Hendler - 2001 - Scientific publishing on the 'semantic web'.pdf:application/pdf},
}

@article{auer_sciqa_2023,
	title = {The {SciQA} Scientific Question Answering Benchmark for Scholarly Knowledge},
	volume = {13},
	rights = {2023 The Author(s)},
	issn = {2045-2322},
	doi = {10.1038/s41598-023-33607-z},
	abstract = {Knowledge graphs have gained increasing popularity in the last decade in science and technology. However, knowledge graphs are currently relatively simple to moderate semantic structures that are mainly a collection of factual statements. Question answering ({QA}) benchmarks and systems were so far mainly geared towards encyclopedic knowledge graphs such as {DBpedia} and Wikidata. We present {SciQA} a scientific {QA} benchmark for scholarly knowledge. The benchmark leverages the Open Research Knowledge Graph ({ORKG}) which includes almost 170,000 resources describing research contributions of almost 15,000 scholarly articles from 709 research fields. Following a bottom-up methodology, we first manually developed a set of 100 complex questions that can be answered using this knowledge graph. Furthermore, we devised eight question templates with which we automatically generated further 2465 questions, that can also be answered with the {ORKG}. The questions cover a range of research fields and question types and are translated into corresponding {SPARQL} queries over the {ORKG}. Based on two preliminary evaluations, we show that the resulting {SciQA} benchmark represents a challenging task for next-generation {QA} systems. This task is part of the open competitions at the 22nd International Semantic Web Conference 2023 as the Scholarly Question Answering over Linked Data ({QALD}) Challenge.},
	pages = {7240},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Auer, Sören and Barone, Dante A. C. and Bartz, Cassiano and Cortes, Eduardo G. and Jaradeh, Mohamad Yaser and Karras, Oliver and Koubarakis, Manolis and Mouromtsev, Dmitry and Pliukhin, Dmitrii and Radyush, Daniil and Shilin, Ivan and Stocker, Markus and Tsalapati, Eleni},
	date = {2023-05-04},
	langid = {english},
	keywords = {finished, notion, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EELWCWD3\\Auer et al. - 2023 - The SciQA Scientific Question Answering Benchmark for Scholarly Knowledge.pdf:application/pdf},
}

@misc{sarmah_hybridrag_2024,
	title = {{HybridRAG}: Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Information Extraction},
	url = {http://arxiv.org/abs/2408.04948},
	doi = {10.48550/arXiv.2408.04948},
	shorttitle = {{HybridRAG}},
	abstract = {Extraction and interpretation of intricate information from unstructured text data arising in financial applications, such as earnings call transcripts, present substantial challenges to large language models ({LLMs}) even using the current best practices to use Retrieval Augmented Generation ({RAG}) (referred to as {VectorRAG} techniques which utilize vector databases for information retrieval) due to challenges such as domain specific terminology and complex formats of the documents. We introduce a novel approach based on a combination, called {HybridRAG}, of the Knowledge Graphs ({KGs}) based {RAG} techniques (called {GraphRAG}) and {VectorRAG} techniques to enhance question-answer (Q\&A) systems for information extraction from financial documents that is shown to be capable of generating accurate and contextually relevant answers. Using experiments on a set of financial earning call transcripts documents which come in the form of Q\&A format, and hence provide a natural set of pairs of ground-truth Q\&As, we show that {HybridRAG} which retrieves context from both vector database and {KG} outperforms both traditional {VectorRAG} and {GraphRAG} individually when evaluated at both the retrieval and generation stages in terms of retrieval accuracy and answer generation. The proposed technique has applications beyond the financial domain},
	number = {{arXiv}:2408.04948},
	publisher = {{arXiv}},
	author = {Sarmah, Bhaskarjit and Hall, Benika and Rao, Rohan and Patel, Sunil and Pasquali, Stefano and Mehta, Dhagash},
	urldate = {2024-08-27},
	date = {2024-08-09},
	eprinttype = {arxiv},
	eprint = {2408.04948 [cs, q-fin, stat]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\YBQVQ9TP\\Sarmah et al. - 2024 - HybridRAG Integrating Knowledge Graphs and Vector Retrieval Augmented Generation for Efficient Info.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\RC7GYRAR\\2408.html:text/html},
}

@online{noauthor_promotion_nodate,
	title = {Promotion of Open Science in Requirements Engineering: Leveraging the Open Research Knowledge Graph for {FAIR} Scientific Information (Requirements Engineering 2024 - Tutorials) - Requirements Engineering 2024},
	url = {https://conf.researchr.org/details/RE-2024/RE-2024-tutorials/1/Promotion-of-Open-Science-in-Requirements-Engineering-Leveraging-the-Open-Research-K},
	shorttitle = {Promotion of Open Science in Requirements Engineering},
	abstract = {The 32nd {IEEE} International Requirements Engineering Conference ({RE}’24) will continue the successful tutorial program of the {RE} conference series. {RE}’24 tutorials will focus on various requirements-related topics of interest to industry, academia, and government. Tutorial attendees can expect to leave a tutorial with new ideas and skills applicable to their profession or research area. 
We invite you to submit proposals for full-day (approx. 7-hour) or half-day (approx. 3.5-hour) tutorials. 
Call for Tutorial Proposals 
We welcome tutorial proposals related to requirements engineering and  ...},
	urldate = {2024-09-06},
	keywords = {notion},
	file = {Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\Z35C4CF7\\Promotion-of-Open-Science-in-Requirements-Engineering-Leveraging-the-Open-Research-K.html:text/html},
}

@article{karras_promotion_2024,
	title = {Promotion of Open Science in Requirements Engineering: Leveraging the Open Research Knowledge Graph for {FAIR} Scientific Information},
	rights = {{CC} {BY} 3.0 {DE}},
	url = {https://www.repo.uni-hannover.de/handle/123456789/17943},
	doi = {10.15488/17809},
	shorttitle = {Promotion of Open Science in Requirements Engineering},
	abstract = {[Background.] Despite improved digital access to publications as digitized artifacts, they remain document-based and often behind paywalls, impeding open science. Researchers must push beyond the established boundary of publications as digitized documents. Open science infrastructures support them in organizing and (re-)using publications and their information so that they are Findable, Accessible, Interoperable, and Reusable ({FAIR}) for humans and machines in the long term. The Open Research Knowledge Graph ({ORKG}) is one sustainably governed infrastructure for {FAIR} scientific information, with successful use cases in requirements engineering ({RE}). [Objective.] This tutorial aims to familiarize the participants with open science and empower them to leverage the {ORKG} for {FAIR} scientific information. [Method.] The half-day tutorial consists of three sessions: 1) A short theoretical introduction to open science and the {ORKG} regarding their importance, benefits, and incentives, 2) A practical session with hands-on exercises for learning skills and practical experiences to leverage the {ORKG}, and 3) A feedback session for reflection. [Results.] The tutorial raises awareness for open science in {RE}, introduces the {ORKG}, fosters networking, and, in the best case, establishes future collaborations. The participants become familiar with open science and the {ORKG} by learning skills and gaining practical experience to leverage the {ORKG} themselves. [Conclusions.] The transition away from digitized documents and towards {FAIR} scientific information is a long-term endeavor. We must gradually sensitize researchers to this transition while guiding and empowering them to leverage existing solutions as an integral part of their work. The tutorial pursues this endeavor to promote open science in {RE}.},
	author = {Karras, Oliver and Ferrari, Alessio and Fucci, Davide and Dell'Anna, Davide},
	urldate = {2024-09-06},
	date = {2024},
	note = {Publisher: Hannover : Institutionelles Repositorium der Leibniz Universität Hannover},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\NUU5QN3Y\\Karras et al. - 2024 - Promotion of Open Science in Requirements Engineering Leveraging the Open Research Knowledge Graph.pdf:application/pdf},
}

@book{ilangovan_open_2024,
	title = {Open Research Knowledge Graph},
	isbn = {978-3-68952-038-0},
	pagetotal = {148},
	publisher = {Cuvillier Verlag},
	author = {Ilangovan, Vinodh and Auer, Sören and Stocker, Markus and Vogt, Lars and Tiwari, Sanju},
	date = {2024-07-05},
	langid = {english},
	keywords = {notion},
	file = {Open Research Knowledge Graph – Cuvillier Verlag:C\:\\Users\\Marco\\Zotero\\storage\\9PJ2B7IJ\\9037-open-research-knowledge-graph.html:text/html;PDF:C\:\\Users\\Marco\\Zotero\\storage\\9ZZNP8QA\\Open Research Knowledge Graph – Cuvillier Verlag.pdf:application/pdf},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the {GSM}8K benchmark of math word problems, surpassing even finetuned {GPT}-3 with a verifier.},
	number = {{arXiv}:2201.11903},
	publisher = {{arXiv}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\8UE8X45Y\\Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\MAI8XMMH\\2201.html:text/html;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3BI3S8CH\\2201.html:text/html},
}

@misc{asai_self-rag_2023,
	title = {Self-{RAG}: Learning to Retrieve, Generate, and Critique through Self-Reflection},
	url = {http://arxiv.org/abs/2310.11511},
	shorttitle = {Self-{RAG}},
	abstract = {Despite their remarkable capabilities, large language models ({LLMs}) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation ({RAG}), an ad hoc approach that augments {LMs} with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes {LM} versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-{RAG}) that enhances an {LM}'s quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary {LM} that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its own generations using special tokens, called reflection tokens. Generating reflection tokens makes the {LM} controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-{RAG} (7B and 13B parameters) significantly outperforms state-of-the-art {LLMs} and retrieval-augmented models on a diverse set of tasks. Specifically, Self-{RAG} outperforms {ChatGPT} and retrieval-augmented Llama2-chat on Open-domain {QA}, reasoning and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models.},
	number = {{arXiv}:2310.11511},
	publisher = {{arXiv}},
	author = {Asai, Akari and Wu, Zeqiu and Wang, Yizhong and Sil, Avirup and Hajishirzi, Hannaneh},
	urldate = {2024-09-09},
	date = {2023-10-17},
	eprinttype = {arxiv},
	eprint = {2310.11511 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\IK28AVM7\\Asai et al. - 2023 - Self-RAG Learning to Retrieve, Generate, and Critique through Self-Reflection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\MEQ2V259\\2310.html:text/html},
}

@misc{kim_kg-gpt_2023,
	title = {{KG}-{GPT}: A General Framework for Reasoning on Knowledge Graphs Using Large Language Models},
	doi = {10.48550/arXiv.2310.11220},
	shorttitle = {{KG}-{GPT}},
	abstract = {While large language models ({LLMs}) have made considerable advancements in understanding and generating unstructured text, their application in structured data remains underexplored. Particularly, using {LLMs} for complex reasoning tasks on knowledge graphs ({KGs}) remains largely untouched. To address this, we propose {KG}-{GPT}, a multi-purpose framework leveraging {LLMs} for tasks employing {KGs}. {KG}-{GPT} comprises three steps: Sentence Segmentation, Graph Retrieval, and Inference, each aimed at partitioning sentences, retrieving relevant graph components, and deriving logical conclusions, respectively. We evaluate {KG}-{GPT} using {KG}-based fact verification and {KGQA} benchmarks, with the model showing competitive and robust performance, even outperforming several fully-supervised models. Our work, therefore, marks a significant step in unifying structured and unstructured data processing within the realm of {LLMs}.},
	number = {{arXiv}:2310.11220},
	publisher = {{arXiv}},
	author = {Kim, Jiho and Kwon, Yeonsu and Jo, Yohan and Choi, Edward},
	date = {2023-10-17},
	keywords = {finished, {KG}-based, needs training, notion, Retriever},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\MWDQABBL\\Kim et al. - 2023 - KG-GPT A General Framework for Reasoning on Knowledge Graphs Using Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\UA4ZAAT4\\2310.html:text/html},
}

@article{wang_pre-trained_2023,
	title = {Pre-Trained Language Models and Their Applications},
	volume = {25},
	issn = {2095-8099},
	url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
	doi = {10.1016/j.eng.2022.04.024},
	abstract = {Pre-trained language models have achieved striking success in natural language processing ({NLP}), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The {NLP} community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the {NLP} field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.},
	pages = {51--65},
	journaltitle = {Engineering},
	shortjournal = {Engineering},
	author = {Wang, Haifeng and Li, Jiwei and Wu, Hua and Hovy, Eduard and Sun, Yu},
	urldate = {2024-09-12},
	date = {2023-06-01},
	keywords = {notion},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\4DU26VXL\\Wang et al. - 2023 - Pre-Trained Language Models and Their Applications.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\EZWZC9YR\\S2095809922006324.html:text/html},
}

@misc{chung_scaling_2022,
	title = {Scaling Instruction-Finetuned Language Models},
	doi = {10.48550/arXiv.2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction finetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) finetuning on chain-of-thought data. We find that instruction finetuning with the above aspects dramatically improves performance on a variety of model classes ({PaLM}, T5, U-{PaLM}), prompting setups (zero-shot, few-shot, {CoT}), and evaluation benchmarks ({MMLU}, {BBH}, {TyDiQA}, {MGSM}, open-ended generation). For instance, Flan-{PaLM} 540B instruction-finetuned on 1.8K tasks outperforms {PALM} 540B by a large margin (+9.4\% on average). Flan-{PaLM} 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on five-shot {MMLU}. We also publicly release Flan-T5 checkpoints, which achieve strong few-shot performance even compared to much larger models, such as {PaLM} 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models.},
	number = {{arXiv}:2210.11416},
	publisher = {{arXiv}},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	date = {2022-12-06},
	eprinttype = {arxiv},
	eprint = {2210.11416 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\JMZ76WEY\\Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\4QT3ZA9C\\2210.html:text/html},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 Technical Report},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of {GPT}-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, {GPT}-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. {GPT}-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of {GPT}-4's performance based on models trained with no more than 1/1,000th the compute of {GPT}-4.},
	number = {{arXiv}:2303.08774},
	publisher = {{arXiv}},
	author = {{OpenAI} and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and {McGrew}, Bob and {McKinney}, Scott Mayer and {McLeavey}, Christine and {McMillan}, Paul and {McNeil}, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	date = {2024-03-04},
	eprinttype = {arxiv},
	eprint = {2303.08774 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\WGWX3FSX\\OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\I36VI4G8\\2303.html:text/html},
}

@misc{he_deberta_2021,
	title = {{DeBERTa}: Decoding-enhanced {BERT} with Disentangled Attention},
	doi = {10.48550/arXiv.2006.03654},
	shorttitle = {{DeBERTa}},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing ({NLP}) tasks. In this paper we propose a new model architecture {DeBERTa} (Decoding-enhanced {BERT} with disentangled attention) that improves the {BERT} and {RoBERTa} models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding ({NLU}) and natural langauge generation ({NLG}) downstream tasks. Compared to {RoBERTa}-Large, a {DeBERTa} model trained on half of the training data performs consistently better on a wide range of {NLP} tasks, achieving improvements on {MNLI} by +0.9\% (90.2\% vs. 91.1\%), on {SQuAD} v2.0 by +2.3\% (88.4\% vs. 90.7\%) and {RACE} by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up {DeBERTa} by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single {DeBERTa} model surpass the human performance on the {SuperGLUE} benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble {DeBERTa} model sits atop the {SuperGLUE} leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
	number = {{arXiv}:2006.03654},
	publisher = {{arXiv}},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	date = {2021-10-06},
	eprinttype = {arxiv},
	eprint = {2006.03654 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\3XBG2T49\\He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled Attention.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\JVYQ9TU6\\2006.html:text/html},
}

@book{barrasa_building_2023,
	location = {Beijing Boston Farnham Sebastopol Tokyo},
	title = {Building knowledge graphs: a practitioner's guide},
	isbn = {978-1-0981-2710-7},
	shorttitle = {Building knowledge graphs},
	pagetotal = {277},
	publisher = {O'Reilly},
	author = {Barrasa, Jesús and Webber, Jim},
	date = {2023},
	keywords = {notion},
}

@article{bouziane_question_2015,
	title = {Question Answering Systems: Survey and Trends},
	volume = {73},
	issn = {1877-0509},
	url = {https://www.sciencedirect.com/science/article/pii/S1877050915034663},
	doi = {10.1016/j.procs.2015.12.005},
	series = {International Conference on Advanced Wireless Information and Communication Technologies ({AWICT} 2015)},
	shorttitle = {Question Answering Systems},
	abstract = {The need to query information content available in various formats including structured and unstructured data (text in natural language, semi-structured Web documents, structured {RDF} data in the semantic Web, etc.) has become increasingly important. Thus, Question Answering Systems ({QAS}) are essential to satisfy this need. {QAS} aim at satisfying users who are looking to answer a specific question in natural language. In this paper we survey various {QAS}. We give also statistics and analysis. This can clear the way and help researchers to choose the appropriate solution to their issue. They can see the insufficiency, so that they can propose new systems for complex queries. They can also adapt or reuse {QAS} techniques for specific research issues.},
	pages = {366--375},
	journaltitle = {Procedia Computer Science},
	shortjournal = {Procedia Computer Science},
	author = {Bouziane, Abdelghani and Bouchiha, Djelloul and Doumi, Noureddine and Malki, Mimoun},
	urldate = {2024-09-12},
	date = {2015-01-01},
	keywords = {notion},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\IK66Z98G\\Bouziane et al. - 2015 - Question Answering Systems Survey and Trends.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\W5WT4T93\\S1877050915034663.html:text/html},
}

@inproceedings{berant_semantic_2013,
	location = {Seattle, Washington, {USA}},
	title = {Semantic Parsing on Freebase from Question-Answer Pairs},
	url = {https://aclanthology.org/D13-1160},
	eventtitle = {{EMNLP} 2013},
	pages = {1533--1544},
	booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Berant, Jonathan and Chou, Andrew and Frostig, Roy and Liang, Percy},
	editor = {Yarowsky, David and Baldwin, Timothy and Korhonen, Anna and Livescu, Karen and Bethard, Steven},
	urldate = {2024-09-14},
	date = {2013-10},
	keywords = {notion, no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\B2UF48NJ\\Berant et al. - 2013 - Semantic Parsing on Freebase from Question-Answer Pairs.pdf:application/pdf},
}

@inproceedings{yih_value_2016,
	location = {Berlin, Germany},
	title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
	doi = {10.18653/v1/P16-2033},
	eventtitle = {{ACL} 2016},
	pages = {201--206},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	editor = {Erk, Katrin and Smith, Noah A.},
	date = {2016-08},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HKUQXCIZ\\Yih et al. - 2016 - The Value of Semantic Parse Labeling for Knowledge Base Question Answering.pdf:application/pdf},
}

@inproceedings{bollacker_freebase_2008,
	location = {New York, {NY}, {USA}},
	title = {Freebase: a collaboratively created graph database for structuring human knowledge},
	isbn = {978-1-60558-102-6},
	doi = {10.1145/1376616.1376746},
	series = {{SIGMOD} '08},
	shorttitle = {Freebase},
	abstract = {Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an {HTTP}-based graph-query {API} using the Metaweb Query Language ({MQL}) as a data query and manipulation language. {MQL} provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.},
	pages = {1247--1250},
	booktitle = {Proceedings of the 2008 {ACM} {SIGMOD} international conference on Management of data},
	publisher = {Association for Computing Machinery},
	author = {Bollacker, Kurt and Evans, Colin and Paritosh, Praveen and Sturge, Tim and Taylor, Jamie},
	date = {2008-06-09},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\B7V367BS\\Bollacker et al. - 2008 - Freebase a collaboratively created graph database for structuring human knowledge.pdf:application/pdf},
}

@inproceedings{sen_knowledge_2023,
	location = {Toronto, Canada},
	title = {Knowledge Graph-augmented Language Models for Complex Question Answering},
	url = {https://aclanthology.org/2023.nlrse-1.1},
	doi = {10.18653/v1/2023.nlrse-1.1},
	abstract = {Large language models have shown impressive abilities to reason over input text, however, they are prone to hallucinations. On the other hand, end-to-end knowledge graph question answering ({KGQA}) models output responses grounded in facts, but they still struggle with complex reasoning, such as comparison or ordinal questions. In this paper, we propose a new method for complex question answering where we combine a knowledge graph retriever based on an end-to-end {KGQA} model with a language model that reasons over the retrieved facts to return an answer. We observe that augmenting language model prompts with retrieved {KG} facts improves performance over using a language model alone by an average of 83\%. In particular, we see improvements on complex questions requiring count, intersection, or multi-hop reasoning operations.},
	eventtitle = {{NLRSE} 2023},
	pages = {1--8},
	booktitle = {Proceedings of the 1st Workshop on Natural Language Reasoning and Structured Explanations ({NLRSE})},
	publisher = {Association for Computational Linguistics},
	author = {Sen, Priyanka and Mavadia, Sandeep and Saffari, Amir},
	editor = {Dalvi Mishra, Bhavana and Durrett, Greg and Jansen, Peter and Neves Ribeiro, Danilo and Wei, Jason},
	urldate = {2024-09-19},
	date = {2023-06},
	keywords = {notion, test},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GUY85BVJ\\Sen et al. - 2023 - Knowledge Graph-augmented Language Models for Complex Question Answering.pdf:application/pdf},
}

@misc{luo_reasoning_2024,
	title = {Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning},
	url = {http://arxiv.org/abs/2310.01061},
	doi = {10.48550/arXiv.2310.01061},
	shorttitle = {Reasoning on Graphs},
	abstract = {Large language models ({LLMs}) have demonstrated impressive reasoning abilities in complex tasks. However, they lack up-to-date knowledge and experience hallucinations during reasoning, which can lead to incorrect reasoning processes and diminish their performance and trustworthiness. Knowledge graphs ({KGs}), which capture vast amounts of facts in a structured format, offer a reliable source of knowledge for reasoning. Nevertheless, existing {KG}-based {LLM} reasoning methods only treat {KGs} as factual knowledge bases and overlook the importance of their structural information for reasoning. In this paper, we propose a novel method called reasoning on graphs ({RoG}) that synergizes {LLMs} with {KGs} to enable faithful and interpretable reasoning. Specifically, we present a planning-retrieval-reasoning framework, where {RoG} first generates relation paths grounded by {KGs} as faithful plans. These plans are then used to retrieve valid reasoning paths from the {KGs} for {LLMs} to conduct faithful reasoning. Furthermore, {RoG} not only distills knowledge from {KGs} to improve the reasoning ability of {LLMs} through training but also allows seamless integration with any arbitrary {LLMs} during inference. Extensive experiments on two benchmark {KGQA} datasets demonstrate that {RoG} achieves state-of-the-art performance on {KG} reasoning tasks and generates faithful and interpretable reasoning results.},
	number = {{arXiv}:2310.01061},
	publisher = {{arXiv}},
	author = {Luo, Linhao and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
	urldate = {2024-09-19},
	date = {2024-02-23},
	eprinttype = {arxiv},
	eprint = {2310.01061 [cs]},
	keywords = {notion, not related},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\FAYN5GWY\\Luo et al. - 2024 - Reasoning on Graphs Faithful and Interpretable Large Language Model Reasoning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\7YR85SG7\\2310.html:text/html},
}

@misc{tian_graph_2023,
	title = {Graph Neural Prompting with Large Language Models},
	url = {http://arxiv.org/abs/2309.15427},
	doi = {10.48550/arXiv.2309.15427},
	abstract = {Large language models ({LLMs}) have shown remarkable generalization capability with exceptional performance in various language modeling tasks. However, they still exhibit inherent limitations in precisely capturing and returning grounded knowledge. While existing work has explored utilizing knowledge graphs ({KGs}) to enhance language modeling via joint training and customized model architectures, applying this to {LLMs} is problematic owing to their large number of parameters and high computational cost. Therefore, how to enhance pre-trained {LLMs} using grounded knowledge, e.g., retrieval-augmented generation, remains an open question. In this work, we propose Graph Neural Prompting ({GNP}), a novel plug-and-play method to assist pre-trained {LLMs} in learning beneficial knowledge from {KGs}. {GNP} encompasses various designs, including a standard graph neural network encoder, a cross-modality pooling module, a domain projector, and a self-supervised link prediction objective. Extensive experiments on multiple datasets demonstrate the superiority of {GNP} on both commonsense and biomedical reasoning tasks across different {LLM} sizes and settings. Code is available at https://github.com/meettyj/{GNP}.},
	number = {{arXiv}:2309.15427},
	publisher = {{arXiv}},
	author = {Tian, Yijun and Song, Huan and Wang, Zichen and Wang, Haozhu and Hu, Ziqing and Wang, Fang and Chawla, Nitesh V. and Xu, Panpan},
	urldate = {2024-09-19},
	date = {2023-12-28},
	eprinttype = {arxiv},
	eprint = {2309.15427 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\FGP3VPCL\\Tian et al. - 2023 - Graph Neural Prompting with Large Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\PY2PLME5\\2309.html:text/html},
}

@misc{li_survey_2024,
	title = {A Survey of Graph Meets Large Language Model: Progress and Future Directions},
	doi = {10.48550/arXiv.2311.12399},
	shorttitle = {A Survey of Graph Meets Large Language Model},
	abstract = {Graph plays a significant role in representing and analyzing complex relationships in real-world applications such as citation networks, social networks, and biological data. Recently, Large Language Models ({LLMs}), which have achieved tremendous success in various domains, have also been leveraged in graph-related tasks to surpass traditional Graph Neural Networks ({GNNs}) based methods and yield state-of-the-art performance. In this survey, we first present a comprehensive review and analysis of existing methods that integrate {LLMs} with graphs. First of all, we propose a new taxonomy, which organizes existing methods into three categories based on the role (i.e., enhancer, predictor, and alignment component) played by {LLMs} in graph-related tasks. Then we systematically survey the representative methods along the three categories of the taxonomy. Finally, we discuss the remaining limitations of existing studies and highlight promising avenues for future research. The relevant papers are summarized and will be consistently updated at: https://github.com/{yhLeeee}/Awesome-{LLMs}-in-Graph-tasks.},
	number = {{arXiv}:2311.12399},
	publisher = {{arXiv}},
	author = {Li, Yuhan and Li, Zhixun and Wang, Peisong and Li, Jia and Sun, Xiangguo and Cheng, Hong and Yu, Jeffrey Xu},
	date = {2024-04-24},
	eprinttype = {arxiv},
	eprint = {2311.12399 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\2J8KR2KJ\\Li et al. - 2024 - A Survey of Graph Meets Large Language Model Progress and Future Directions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\YKGZCG5U\\2311.html:text/html},
}

@article{pan_integrating_2024,
	title = {Integrating Graphs with Large Language Models: Methods and Prospects},
	volume = {39},
	issn = {1541-1672, 1941-1294},
	url = {http://arxiv.org/abs/2310.05499},
	doi = {10.1109/MIS.2023.3332242},
	shorttitle = {Integrating Graphs with Large Language Models},
	abstract = {Large language models ({LLMs}) such as {GPT}-4 have emerged as frontrunners, showcasing unparalleled prowess in diverse applications, including answering queries, code generation, and more. Parallelly, graph-structured data, an intrinsic data type, is pervasive in real-world scenarios. Merging the capabilities of {LLMs} with graph-structured data has been a topic of keen interest. This paper bifurcates such integrations into two predominant categories. The first leverages {LLMs} for graph learning, where {LLMs} can not only augment existing graph algorithms but also stand as prediction models for various graph tasks. Conversely, the second category underscores the pivotal role of graphs in advancing {LLMs}. Mirroring human cognition, we solve complex tasks by adopting graphs in either reasoning or collaboration. Integrating with such structures can significantly boost the performance of {LLMs} in various complicated tasks. We also discuss and propose open questions for integrating {LLMs} with graph-structured data for the future direction of the field.},
	pages = {64--68},
	number = {1},
	journaltitle = {{IEEE} Intelligent Systems},
	shortjournal = {{IEEE} Intell. Syst.},
	author = {Pan, Shirui and Zheng, Yizhen and Liu, Yixin},
	urldate = {2024-09-19},
	date = {2024-01},
	eprinttype = {arxiv},
	eprint = {2310.05499 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\DD29RKL3\\Pan et al. - 2024 - Integrating Graphs with Large Language Models Methods and Prospects.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ZG947H2T\\2310.html:text/html},
}

@misc{zhang_graph_2023,
	title = {Graph Meets {LLMs}: Towards Large Graph Models},
	url = {http://arxiv.org/abs/2308.14522},
	doi = {10.48550/arXiv.2308.14522},
	shorttitle = {Graph Meets {LLMs}},
	abstract = {Large models have emerged as the most recent groundbreaking achievements in artificial intelligence, and particularly machine learning. However, when it comes to graphs, large models have not achieved the same level of success as in other fields, such as natural language processing and computer vision. In order to promote applying large models for graphs forward, we present a perspective paper to discuss the challenges and opportunities associated with developing large graph models. First, we discuss the desired characteristics of large graph models. Then, we present detailed discussions from three key perspectives: representation basis, graph data, and graph models. In each category, we provide a brief overview of recent advances and highlight the remaining challenges together with our visions. Finally, we discuss valuable applications of large graph models. We believe this perspective can encourage further investigations into large graph models, ultimately pushing us one step closer towards artificial general intelligence ({AGI}). We are the first to comprehensively study large graph models, to the best of our knowledge.},
	number = {{arXiv}:2308.14522},
	publisher = {{arXiv}},
	author = {Zhang, Ziwei and Li, Haoyang and Zhang, Zeyang and Qin, Yijian and Wang, Xin and Zhu, Wenwu},
	urldate = {2024-09-19},
	date = {2023-11-11},
	eprinttype = {arxiv},
	eprint = {2308.14522 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\GH7ZCL6Y\\Zhang et al. - 2023 - Graph Meets LLMs Towards Large Graph Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\PWBTZBZP\\2308.html:text/html},
}

@misc{wang_can_2024,
	title = {Can Language Models Solve Graph Problems in Natural Language?},
	url = {http://arxiv.org/abs/2305.10037},
	doi = {10.48550/arXiv.2305.10037},
	abstract = {Large language models ({LLMs}) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While {LLMs} have advanced the state-of-the-art on these tasks with structure implications, whether {LLMs} could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose {NLGraph} (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. {NLGraph} contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate {LLMs} ({GPT}-3/4) with various prompting approaches on the {NLGraph} benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) {LLMs} are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance {LLMs} in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of {LLMs} on {NLGraph} by 3.07\% to 16.85\% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The {NLGraph} benchmark and evaluation code are available at https://github.com/Arthur-Heng/{NLGraph}.},
	number = {{arXiv}:2305.10037},
	publisher = {{arXiv}},
	author = {Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia},
	urldate = {2024-09-19},
	date = {2024-01-05},
	eprinttype = {arxiv},
	eprint = {2305.10037 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\5UJ7KHVZ\\Wang et al. - 2024 - Can Language Models Solve Graph Problems in Natural Language.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\96JFPF4S\\2305.html:text/html},
}

@misc{bordes_large-scale_2015,
	title = {Large-scale Simple Question Answering with Memory Networks},
	url = {http://arxiv.org/abs/1506.02075},
	abstract = {Training large-scale question answering systems is complicated because training sources usually cover a small portion of the range of possible questions. This paper studies the impact of multitask and transfer learning for simple question answering; a setting for which the reasoning required to answer is quite easy, as long as one can retrieve the correct evidence given a question, which can be difficult in large-scale conditions. To this end, we introduce a new dataset of 100k questions that we use in conjunction with existing benchmarks. We conduct our study within the framework of Memory Networks (Weston et al., 2015) because this perspective allows us to eventually scale up to more complex reasoning, and show that Memory Networks can be successfully trained to achieve excellent performance.},
	number = {{arXiv}:1506.02075},
	publisher = {{arXiv}},
	author = {Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason},
	urldate = {2024-09-23},
	date = {2015-06-05},
	keywords = {finished, notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\KXD2CVA2\\Bordes et al. - 2015 - Large-scale Simple Question Answering with Memory Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NCZKK6ME\\1506.html:text/html},
}

@inproceedings{trivedi_lc-quad_2017,
	location = {Berlin, Heidelberg},
	title = {{LC}-{QuAD}: A Corpus for Complex Question Answering over Knowledge Graphs},
	isbn = {978-3-319-68203-7},
	doi = {10.1007/978-3-319-68204-4_22},
	shorttitle = {{LC}-{QuAD}},
	abstract = {Being able to access knowledge bases in an intuitive way has been an active area of research over the past years. In particular, several question answering ({QA}) approaches which allow to query {RDF} datasets in natural language have been developed as they allow end users to access knowledge without needing to learn the schema of a knowledge base and learn a formal query language. To foster this research area, several training datasets have been created, e.g. in the {QALD} (Question Answering over Linked Data) initiative. However, existing datasets are insufficient in terms of size, variety or complexity to apply and evaluate a range of machine learning based {QA} approaches for learning complex {SPARQL} queries. With the provision of the Large-Scale Complex Question Answering Dataset ({LC}-{QuAD}), we close this gap by providing a dataset with 5000 questions and their corresponding {SPARQL} queries over the {DBpedia} dataset. In this article, we describe the dataset creation process and how we ensure a high variety of questions, which should enable to assess the robustness and accuracy of the next generation of {QA} systems for knowledge graphs.},
	pages = {210--218},
	booktitle = {The Semantic Web – {ISWC} 2017: 16th International Semantic Web Conference, Vienna, Austria, October 21-25, 2017, Proceedings, Part {II}},
	publisher = {Springer-Verlag},
	author = {Trivedi, Priyansh and Maheshwari, Gaurav and Dubey, Mohnish and Lehmann, Jens},
	date = {2017-10-21},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\D9YEIAEG\\Trivedi et al. - 2017 - LC-QuAD A Corpus for Complex Question Answering over Knowledge Graphs.pdf:application/pdf},
}

@inproceedings{dubey_lc-quad_2019,
	location = {Cham},
	title = {{LC}-{QuAD} 2.0: A Large Dataset for Complex Question Answering over Wikidata and {DBpedia}},
	isbn = {978-3-030-30796-7},
	doi = {10.1007/978-3-030-30796-7_5},
	shorttitle = {{LC}-{QuAD} 2.0},
	abstract = {Providing machines with the capability of exploring knowledge graphs and answering natural language questions has been an active area of research over the past decade. In this direction translating natural language questions to formal queries has been one of the key approaches. To advance the research area, several datasets like {WebQuestions}, {QALD} and {LCQuAD} have been published in the past. The biggest data set available for complex questions ({LCQuAD}) over knowledge graphs contains five thousand questions. We now provide {LC}-{QuAD} 2.0 (Large-Scale Complex Question Answering Dataset) with 30,000 questions, their paraphrases and their corresponding {SPARQL} queries. {LC}-{QuAD} 2.0 is compatible with both Wikidata and {DBpedia} 2018 knowledge graphs. In this article, we explain how the dataset was created and the variety of questions available with examples. We further provide a statistical analysis of the dataset.},
	pages = {69--78},
	booktitle = {The Semantic Web – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Banerjee, Debayan and Abdelkawi, Abdelrahman and Lehmann, Jens},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	date = {2019},
	langid = {english},
	keywords = {notion, finished, related\_dataset},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ARFHIF8U\\Dubey et al. - 2019 - LC-QuAD 2.0 A Large Dataset for Complex Question Answering over Wikidata and DBpedia.pdf:application/pdf},
}

@misc{lee_liquid_2023,
	title = {{LIQUID}: A Framework for List Question Answering Dataset Generation},
	url = {http://arxiv.org/abs/2302.01691},
	doi = {10.48550/arXiv.2302.01691},
	shorttitle = {{LIQUID}},
	abstract = {Question answering ({QA}) models often rely on large-scale training datasets, which necessitates the development of a data generation framework to reduce the cost of manual annotations. Although several recent studies have aimed to generate synthetic questions with single-span answers, no study has been conducted on the creation of list questions with multiple, non-contiguous spans as answers. To address this gap, we propose {LIQUID}, an automated framework for generating list {QA} datasets from unlabeled corpora. We first convert a passage from Wikipedia or {PubMed} into a summary and extract named entities from the summarized text as candidate answers. This allows us to select answers that are semantically correlated in context and is, therefore, suitable for constructing list questions. We then create questions using an off-the-shelf question generator with the extracted entities and original passage. Finally, iterative filtering and answer expansion are performed to ensure the accuracy and completeness of the answers. Using our synthetic data, we significantly improve the performance of the previous best list {QA} models by exact-match F1 scores of 5.0 on {MultiSpanQA}, 1.9 on Quoref, and 2.8 averaged across three {BioASQ} benchmarks.},
	number = {{arXiv}:2302.01691},
	publisher = {{arXiv}},
	author = {Lee, Seongyun and Kim, Hyunjae and Kang, Jaewoo},
	urldate = {2024-09-24},
	date = {2023-02-06},
	eprinttype = {arxiv},
	eprint = {2302.01691 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\TNVM4PNB\\Lee et al. - 2023 - LIQUID A Framework for List Question Answering Dataset Generation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\E9YGGDIZ\\2302.html:text/html},
}

@article{saikh_scienceqa_2022,
	title = {{ScienceQA}: a novel resource for question answering on scholarly articles},
	volume = {23},
	issn = {1432-1300},
	doi = {10.1007/s00799-022-00329-y},
	abstract = {Machine Reading Comprehension ({MRC}) of a document is a challenging problem that requires discourse-level understanding. Information extraction from scholarly articles nowadays is a critical use case for researchers to understand the underlying research quickly and move forward, especially in this age of infodemic. {MRC} on research articles can also provide helpful information to the reviewers and editors. However, the main bottleneck in building such models is the availability of human-annotated data. In this paper, firstly, we introduce a dataset to facilitate question answering ({QA}) on scientific articles. We prepare the dataset in a semi-automated fashion having more than 100k human-annotated context–question–answer triples. Secondly, we implement one baseline {QA} model based on Bidirectional Encoder Representations from Transformers ({BERT}). Additionally, we implement two models: the first one is based on Science {BERT} ({SciBERT}), and the second is the combination of {SciBERT} and Bi-Directional Attention Flow (Bi-{DAF}). The best model (i.e., {SciBERT}) obtains an F1 score of 75.46\%. Our dataset is novel, and our work opens up a new avenue for scholarly document processing research by providing a benchmark {QA} dataset and standard baseline. We make our dataset and codes available here at https://github.com/{TanikSaikh}/Scientific-Question-Answering.},
	pages = {289--301},
	number = {3},
	journaltitle = {International Journal on Digital Libraries},
	shortjournal = {Int J Digit Libr},
	author = {Saikh, Tanik and Ghosal, Tirthankar and Mittal, Amish and Ekbal, Asif and Bhattacharyya, Pushpak},
	date = {2022-09-01},
	langid = {english},
	keywords = {no type information, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PQDSQULV\\Saikh et al. - 2022 - ScienceQA a novel resource for question answering on scholarly articles.pdf:application/pdf},
}

@misc{kim_factkg_2023,
	title = {{FactKG}: Fact Verification via Reasoning on Knowledge Graphs},
	url = {http://arxiv.org/abs/2305.06590},
	doi = {10.48550/arXiv.2305.06590},
	shorttitle = {{FactKG}},
	abstract = {In real world applications, knowledge graphs ({KG}) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, {KGs} have not been adequately utilized as a knowledge source. {KGs} can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A {KG} consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use {KGs}, we introduce a new dataset, {FactKG}: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, {FactKG} contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze {FactKG} over these reasoning types. We believe {FactKG} can advance both reliability and practicality in {KG}-based fact verification.},
	number = {{arXiv}:2305.06590},
	publisher = {{arXiv}},
	author = {Kim, Jiho and Park, Sungjin and Kwon, Yeonsu and Jo, Yohan and Thorne, James and Choi, Edward},
	urldate = {2024-09-24},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.06590 [cs]},
	keywords = {notion, needs training},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\KKJMXDUM\\Kim et al. - 2023 - FactKG Fact Verification via Reasoning on Knowledge Graphs.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2P2XM8WG\\2305.html:text/html},
}

@misc{yu_defense_2024,
	title = {In Defense of {RAG} in the Era of Long-Context Language Models},
	url = {http://arxiv.org/abs/2409.01666},
	doi = {10.48550/arXiv.2409.01666},
	abstract = {Overcoming the limited context limitations in early-generation {LLMs}, retrieval-augmented generation ({RAG}) has been a reliable solution for context-based answer generation in the past. Recently, the emergence of long-context {LLMs} allows the models to incorporate much longer text sequences, making {RAG} less attractive. Recent studies show that long-context {LLMs} significantly outperform {RAG} in long-context applications. Unlike the existing works favoring the long-context {LLM} over {RAG}, we argue that the extremely long context in {LLMs} suffers from a diminished focus on relevant information and leads to potential degradation in answer quality. This paper revisits the {RAG} in long-context answer generation. We propose an order-preserve retrieval-augmented generation ({OP}-{RAG}) mechanism, which significantly improves the performance of {RAG} for long-context question-answer applications. With {OP}-{RAG}, as the number of retrieved chunks increases, the answer quality initially rises, and then declines, forming an inverted U-shaped curve. There exist sweet points where {OP}-{RAG} could achieve higher answer quality with much less tokens than long-context {LLM} taking the whole context as input. Extensive experiments on public benchmark demonstrate the superiority of our {OP}-{RAG}.},
	number = {{arXiv}:2409.01666},
	publisher = {{arXiv}},
	author = {Yu, Tan and Xu, Anbang and Akkiraju, Rama},
	urldate = {2024-09-26},
	date = {2024-09-03},
	eprinttype = {arxiv},
	eprint = {2409.01666 [cs]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:C\:\\Users\\Marco\\Zotero\\storage\\QHCBJXRB\\Yu et al. - 2024 - In Defense of RAG in the Era of Long-Context Language Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\MY5JUNBL\\2409.html:text/html},
}

@inproceedings{karras_divide_2023,
	title = {Divide and Conquer the {EmpiRE}: A Community-Maintainable Knowledge Graph of Empirical Research in Requirements Engineering},
	doi = {10.1109/ESEM56168.2023.10304795},
	abstract = {[Background.] Empirical research in requirements engineering ({RE}) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its “current” state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph ({ORKG}) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in {RE} ({KG}-{EmpiRE}) whose data is openly available. Our long-term goal is to continuously maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. [Method.] We conduct a literature review using the {ORKG} to build and publish {KG}-{EmpiRE} which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020–2025. [Results.] From 570 papers of the {IEEE} International Requirements Engineering Conference (2000–2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The {ORKG} is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.},
	eventtitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	pages = {1--12},
	booktitle = {2023 {ACM}/{IEEE} International Symposium on Empirical Software Engineering and Measurement ({ESEM})},
	author = {Karras, Oliver and Wernlein, Felix and Klünder, Jil and Auer, Sören},
	date = {2023-10},
	keywords = {domain\_specific, finished, notion},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2SUXDABT\\Karras et al. - 2023 - Divide and Conquer the EmpiRE A Community-Maintainable Knowledge Graph of Empirical Research in Req.pdf:application/pdf},
}

@inproceedings{karras_kg-empire_2024,
	title = {{KG}-{EmpiRE}: A Community-Maintainable Knowledge Graph for a Sustainable Literature Review on the State and Evolution of Empirical Research in Requirements Engineering},
	url = {https://ieeexplore.ieee.org/document/10628483},
	doi = {10.1109/RE59067.2024.00063},
	shorttitle = {{KG}-{EmpiRE}},
	abstract = {In the last two decades, several researchers provided snapshots of the “current” state and evolution of empirical research in requirements engineering ({RE}) through literature reviews. However, these literature reviews were not sustainable, as none built on or updated previous works due to the unavailability of the extracted and analyzed data. {KG}-{EmpiRE} is a Knowledge Graph ({KG}) of empirical research in {RE} based on scientific data extracted from currently 680 papers published in the {IEEE} International Requirements Engineering Conference (1994-2022). {KG}-{EmpiRE} is maintained in the Open Research Knowledge Graph ({ORKG}), making all data openly and long-term available according to the {FAIR} data principles. Our long-term goal is to constantly maintain {KG}-{EmpiRE} with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in {RE}. Besides {KG}-{EmpiRE}, we provide its analysis with all supplementary materials in a repository. This repository contains all files with instructions for replicating and (re-)using the analysis locally or via executable environments and for repeating the research approach. Since its first release based on 199 papers (2014-2022), {KG}-{EmpiRE} and its analysis have been updated twice, currently covering over 650 papers. {KG}-{EmpiRE} and its analysis demonstrate how innovative infrastructures, such as the {ORKG}, can be leveraged to make data from literature reviews {FAIR}, openly available, and maintainable for the research community in the long term. In this way, we can enable replicable, (re-)usable, and thus sustainable literature reviews to ensure the quality, reliability, and timeliness of their research results.},
	eventtitle = {2024 {IEEE} 32nd International Requirements Engineering Conference ({RE})},
	pages = {500--501},
	booktitle = {2024 {IEEE} 32nd International Requirements Engineering Conference ({RE})},
	author = {Karras, Oliver},
	urldate = {2024-09-28},
	date = {2024-06},
	note = {{ISSN}: 2332-6441},
	keywords = {notion, finished},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EWVZI9H8\\Karras - 2024 - KG-EmpiRE A Community-Maintainable Knowledge Graph for a Sustainable Literature Review on the State.pdf:application/pdf},
}

@incollection{mendez_open_2020,
	location = {Cham},
	title = {Open Science in Software Engineering},
	isbn = {978-3-030-32489-6},
	url = {https://doi.org/10.1007/978-3-030-32489-6_17},
	abstract = {Open science describes the movement of making any research artifact available to the public and includes, but is not limited to, open access, open data, and open source. While open science is becoming generally accepted as a norm in other scientific disciplines, in software engineering, we are still struggling in adapting open science to the particularities of our discipline, rendering progress in our scientific community cumbersome. In this chapter, we reflect upon the essentials in open science for software engineering including what open science is, why we should engage in it, and how we should do it. We particularly draw from our experiences made as conference chairs implementing open science initiatives and as researchers actively engaging in open science to critically discuss challenges and pitfalls and to address more advanced topics such as how and under which conditions to share preprints, what infrastructure and licence model to cover, or how do it within the limitations of different reviewing models, such as double-blind reviewing. Our hope is to help establishing a common ground and to contribute to make open science a norm also in software engineering.},
	pages = {477--501},
	booktitle = {Contemporary Empirical Methods in Software Engineering},
	publisher = {Springer International Publishing},
	author = {Mendez, Daniel and Graziotin, Daniel and Wagner, Stefan and Seibold, Heidi},
	editor = {Felderer, Michael and Travassos, Guilherme Horta},
	urldate = {2024-09-28},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32489-6_17},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YVKRSPTY\\Mendez et al. - 2020 - Open Science in Software Engineering.pdf:application/pdf},
}

@article{wilkinson_fair_2016,
	title = {The {FAIR} Guiding Principles for scientific data management and stewardship},
	volume = {3},
	rights = {2016 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata201618},
	doi = {10.1038/sdata.2016.18},
	abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the {FAIR} Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the {FAIR} Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the {FAIR} Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
	pages = {160018},
	number = {1},
	journaltitle = {Scientific Data},
	shortjournal = {Sci Data},
	author = {Wilkinson, Mark D. and Dumontier, Michel and Aalbersberg, {IJsbrand} Jan and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and da Silva Santos, Luiz Bonino and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and ’t Hoen, Peter A. C. and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and van Schaik, Rene and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and van der Lei, Johan and van Mulligen, Erik and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
	urldate = {2024-09-28},
	date = {2016-03-15},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2344DKL2\\Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data management and stewardship.pdf:application/pdf},
}

@inproceedings{auer_dbpedia_2007,
	location = {Berlin, Heidelberg},
	title = {{DBpedia}: A Nucleus for a Web of Open Data},
	isbn = {978-3-540-76298-0},
	doi = {10.1007/978-3-540-76298-0_52},
	shorttitle = {{DBpedia}},
	abstract = {{DBpedia} is a community effort to extract structured information from Wikipedia and to make this information available on the Web. {DBpedia} allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the {DBpedia} datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the {DBpedia} community and show how website authors can facilitate {DBpedia} content within their sites. Finally, we present the current status of interlinking {DBpedia} with other open datasets on the Web and outline how {DBpedia} could serve as a nucleus for an emerging Web of open data.},
	pages = {722--735},
	booktitle = {The Semantic Web},
	publisher = {Springer},
	author = {Auer, Sören and Bizer, Christian and Kobilarov, Georgi and Lehmann, Jens and Cyganiak, Richard and Ives, Zachary},
	editor = {Aberer, Karl and Choi, Key-Sun and Noy, Natasha and Allemang, Dean and Lee, Kyung-Il and Nixon, Lyndon and Golbeck, Jennifer and Mika, Peter and Maynard, Diana and Mizoguchi, Riichiro and Schreiber, Guus and Cudré-Mauroux, Philippe},
	date = {2007},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SGW7NE5I\\Auer et al. - 2007 - DBpedia A Nucleus for a Web of Open Data.pdf:application/pdf},
}

@article{vrandecic_wikidata_2014,
	title = {Wikidata: a free collaborative knowledgebase},
	volume = {57},
	issn = {0001-0782},
	doi = {10.1145/2629489},
	shorttitle = {Wikidata},
	abstract = {This collaboratively edited knowledgebase provides a common source of data for Wikipedia, and everyone else.},
	pages = {78--85},
	number = {10},
	journaltitle = {Commun. {ACM}},
	author = {Vrandečić, Denny and Krötzsch, Markus},
	date = {2014-09-23},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DKGY8EQK\\Vrandečić und Krötzsch - 2014 - Wikidata a free collaborative knowledgebase.pdf:application/pdf},
}

@inproceedings{auer_towards_2018,
	title = {Towards a Knowledge Graph for Science},
	isbn = {978-1-4503-5489-9},
	doi = {10.1145/3227609.3227689},
	abstract = {The document-centric workflows in science have reached (or already exceeded) the limits of adequacy. This is emphasized by recent discussions on the increasing proliferation of scientific literature and the reproducibility crisis. This presents an opportunity to rethink the dominant paradigm of document-centric scholarly information communication and transform it into knowledge-based information flows by representing and expressing information through semantically rich, interlinked knowledge graphs. At the core of knowledge-based information flows is the creation and evolution of information models that establish a common understanding of information communicated between stakeholders as well as the integration of these technologies into the infrastructure and processes of search and information exchange in the research library of the future. By integrating these models into existing and new research infrastructure services, the information structures that are currently still implicit and deeply hidden in documents can be made explicit and directly usable. This has the potential to revolutionize scientific work as information and research results can be seamlessly interlinked with each other and better matched to complex information needs. Furthermore, research results become directly comparable and easier to reuse. As our main contribution, we propose the vision of a knowledge graph for science, present a possible infrastructure for such a knowledge graph as well as our early attempts towards an implementation of the infrastructure.},
	pages = {1--6},
	booktitle = {Proceedings of the 8th International Conference on Web Intelligence, Mining and Semantics},
	author = {Auer, Sören and Kovtun, Viktor and Prinz, Manuel and Kasprzik, Anna and Stocker, Markus and Vidal, Maria Esther},
	date = {2018-06-25},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\UUJYCHDA\\Auer et al. - 2018 - Towards a Knowledge Graph for Science.pdf:application/pdf},
}

@report{knublauch_shapes_2017,
	title = {Shapes constraint language ({SHACL})},
	url = {https://www.w3.org/TR/shacl/},
	institution = {W3C},
	author = {Knublauch, Holger and Kontokostas, Dimitris},
	date = {2017-07},
	keywords = {notion, 2018 aligned aligned-project group\_aksw kilt kontokostas},
}

@incollection{easterbrook_selecting_2008,
	location = {London},
	title = {Selecting Empirical Methods for Software Engineering Research},
	isbn = {978-1-84800-044-5},
	abstract = {Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.},
	pages = {285--311},
	booktitle = {Guide to Advanced Empirical Software Engineering},
	publisher = {Springer},
	author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
	editor = {Shull, Forrest and Singer, Janice and Sjøberg, Dag I. K.},
	date = {2008},
	langid = {english},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KC9B8WYM\\Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineering Research.pdf:application/pdf},
}

@inproceedings{shaw_writing_2003,
	title = {Writing good software engineering research papers},
	doi = {10.1109/ICSE.2003.1201262},
	abstract = {Software engineering researchers solve problems of several different kinds. To do so, they produce several different kinds of results, and they should develop appropriate evidence to validate these results. They often report their research in conference papers. I analyzed the abstracts of research papers submitted to {XSE} 2002 in order to identify the types of research reported in the submitted and accepted papers, and I observed the program committee discussions about which papers to accept. This report presents the research paradigms of the papers, common concerns of the program committee, and statistics on success rates. This information should help researchers design better research projects and write papers that present their results to best advantage.},
	eventtitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	pages = {726--736},
	booktitle = {25th International Conference on Software Engineering, 2003. Proceedings.},
	author = {Shaw, M.},
	date = {2003-05},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\N3KSK3ZY\\Shaw - 2003 - Writing good software engineering research papers.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\GVGKBD9L\\1201262.html:text/html},
}

@online{rajpurkar_squad_2016,
	title = {{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
	url = {https://arxiv.org/abs/1606.05250v3},
	shorttitle = {{SQuAD}},
	abstract = {We present the Stanford Question Answering Dataset ({SQuAD}), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
	titleaddon = {{arXiv}.org},
	author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
	urldate = {2024-10-05},
	date = {2016-06-16},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\57DRRYEE\\Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehension of Text.pdf:application/pdf},
}

@inproceedings{saikh_scholarlyread_2020,
	location = {Marseille, France},
	title = {{ScholarlyRead}: A New Dataset for Scientific Article Reading Comprehension},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.675},
	shorttitle = {{ScholarlyRead}},
	abstract = {We present {ScholarlyRead}, span-of-word-based scholarly articles' Reading Comprehension ({RC}) dataset with approximately 10K manually checked passage-question-answer instances. {ScholarlyRead} was constructed in semi-automatic way. We consider the articles from two popular journals of a reputed publishing house. Firstly, we generate questions from these articles in an automatic way. Generated questions are then manually checked by the human annotators. We propose a baseline model based on Bi-Directional Attention Flow ({BiDAF}) network that yields the F1 score of 37.31\%. The framework would be useful for building Question-Answering ({QA}) systems on scientific articles.},
	eventtitle = {{LREC} 2020},
	pages = {5498--5504},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Saikh, Tanik and Ekbal, Asif and Bhattacharyya, Pushpak},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2024-10-05},
	date = {2020-05},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\AH6RH9PT\\Saikh et al. - 2020 - ScholarlyRead A New Dataset for Scientific Article Reading Comprehension.pdf:application/pdf},
}

@article{manning_introduction_2009,
	title = {Introduction to Information Retrieval},
	author = {Manning, Christopher and Raghavan, Prabhakar and Schuetze, Hinrich},
	date = {2009},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\D4L2X35E\\Manning et al. - 2009 - Introduction to Information Retrieval.pdf:application/pdf},
}

@misc{fu_survey_2020,
	title = {A Survey on Complex Question Answering over Knowledge Base: Recent Advances and Challenges},
	doi = {10.48550/arXiv.2007.13069},
	shorttitle = {A Survey on Complex Question Answering over Knowledge Base},
	abstract = {Question Answering ({QA}) over Knowledge Base ({KB}) aims to automatically answer natural language questions via well-structured relation information between entities stored in knowledge bases. In order to make {KBQA} more applicable in actual scenarios, researchers have shifted their attention from simple questions to complex questions, which require more {KB} triples and constraint inference. In this paper, we introduce the recent advances in complex {QA}. Besides traditional methods relying on templates and rules, the research is categorized into a taxonomy that contains two main branches, namely Information Retrieval-based and Neural Semantic Parsing-based. After describing the methods of these branches, we analyze directions for future research and introduce the models proposed by the Alime team.},
	number = {{arXiv}:2007.13069},
	publisher = {{arXiv}},
	author = {Fu, Bin and Qiu, Yunqi and Tang, Chengguang and Li, Yang and Yu, Haiyang and Sun, Jian},
	date = {2020-07-26},
	eprinttype = {arxiv},
	eprint = {2007.13069},
	keywords = {notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\T25IURHD\\Fu et al. - 2020 - A Survey on Complex Question Answering over Knowledge Base Recent Advances and Challenges.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\PR9HBRZH\\2007.html:text/html},
}

@article{liu_question_2015,
	title = {Question Answering over Knowledge Bases},
	volume = {30},
	issn = {1941-1294},
	url = {https://ieeexplore.ieee.org/abstract/document/7243222},
	doi = {10.1109/MIS.2015.70},
	abstract = {Question answering over knowledge bases is a challenging task for next-generation search engines. The core of this task is to understand the meaning of questions and translate them into structured language-based queries. Previous research has focused on a specific knowledge base with a constrained domain, but with the increase in the size and domain of existing knowledge bases, fulfilling this aim is even more challenging. This article introduces the mainstream methods for question answering over knowledge bases, describing typical semantic meaning representation models and state-of-the-art systems for converting questions to predefined logical forms. It also puts a particular focus on the approaches for question answering over a large-scale knowledge base and multiple heterogeneous knowledge bases.},
	pages = {26--35},
	number = {5},
	journaltitle = {{IEEE} Intelligent Systems},
	author = {Liu, Kang and Zhao, Jun and He, Shizhu and Zhang, Yuanzhe},
	urldate = {2024-10-26},
	date = {2015-09},
	note = {Conference Name: {IEEE} Intelligent Systems},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\FAPV4IA6\\Liu et al. - 2015 - Question Answering over Knowledge Bases.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\NTHL7ESS\\7243222.html:text/html},
}

@misc{tran_comparative_2022,
	title = {A Comparative Study of Question Answering over Knowledge Bases},
	doi = {10.48550/arXiv.2211.08170},
	abstract = {Question answering over knowledge bases ({KBQA}) has become a popular approach to help users extract information from knowledge bases. Although several systems exist, choosing one suitable for a particular application scenario is difficult. In this article, we provide a comparative study of six representative {KBQA} systems on eight benchmark datasets. In that, we study various question types, properties, languages, and domains to provide insights on where existing systems struggle. On top of that, we propose an advanced mapping algorithm to aid existing models in achieving superior results. Moreover, we also develop a multilingual corpus {COVID}-{KGQA}, which encourages {COVID}-19 research and multilingualism for the diversity of future {AI}. Finally, we discuss the key findings and their implications as well as performance guidelines and some future improvements. Our source code is available at {\textbackslash}url\{https://github.com/tamlhp/kbqa\}.},
	number = {{arXiv}:2211.08170},
	publisher = {{arXiv}},
	author = {Tran, Khiem Vinh and Phan, Hao Phu and Quach, Khang Nguyen Duc and Nguyen, Ngan Luu-Thuy and Jo, Jun and Nguyen, Thanh Tam},
	date = {2022-11-15},
	keywords = {finished, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GYCPXHQW\\Tran et al. - 2022 - A Comparative Study of Question Answering over Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VG9KIM26\\2211.html:text/html},
}

@inproceedings{mikhailian_learning_2009,
	location = {Suntec, Singapore},
	title = {Learning foci for Question Answering over Topic Maps},
	url = {https://aclanthology.org/P09-2082},
	eventtitle = {{ACL}-{IJCNLP} 2009},
	pages = {325--328},
	booktitle = {Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers},
	publisher = {Association for Computational Linguistics},
	author = {Mikhailian, Alexander and Dalmas, Tiphaine and Pinchuk, Rani},
	editor = {Su, Keh-Yih and Su, Jian and Wiebe, Janyce and Li, Haizhou},
	urldate = {2024-10-27},
	date = {2009-08},
	keywords = {notion, finished, general},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XDL4SXLQ\\Mikhailian et al. - 2009 - Learning foci for Question Answering over Topic Maps.pdf:application/pdf},
}

@inproceedings{moldovan_structure_2000,
	location = {Hong Kong},
	title = {The Structure and Performance of an Open-Domain Question Answering System},
	doi = {10.3115/1075218.1075289},
	eventtitle = {{ACL} 2000},
	pages = {563--570},
	booktitle = {Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Moldovan, Dan and Harabagiu, Sanda and Pasca, Marius and Mihalcea, Rada and Girju, Roxana and Goodrum, Richard and Rus, Vasile},
	date = {2000-10},
	keywords = {finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\I9HD6KI6\\Moldovan et al. - 2000 - The Structure and Performance of an Open-Domain Question Answering System.pdf:application/pdf},
}

@article{diefenbach_core_2018,
	title = {Core techniques of question answering systems over knowledge bases: a survey},
	volume = {55},
	issn = {0219-3116},
	url = {https://doi.org/10.1007/s10115-017-1100-y},
	doi = {10.1007/s10115-017-1100-y},
	shorttitle = {Core techniques of question answering systems over knowledge bases},
	abstract = {The Semantic Web contains an enormous amount of information in the form of knowledge bases ({KB}). To make this information available, many question answering ({QA}) systems over {KBs} were created in the last years. Building a {QA} system over {KBs} is difficult because there are many different challenges to be solved. In order to address these challenges, {QA} systems generally combine techniques from natural language processing, information retrieval, machine learning and Semantic Web. The aim of this survey is to give an overview of the techniques used in current {QA} systems over {KBs}. We present the techniques used by the {QA} systems which were evaluated on a popular series of benchmarks: Question Answering over Linked Data. Techniques that solve the same task are first grouped together and then described. The advantages and disadvantages are discussed for each technique. This allows a direct comparison of similar techniques. Additionally, we point to techniques that are used over {WebQuestions} and {SimpleQuestions}, which are two other popular benchmarks for {QA} systems.},
	pages = {529--569},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Diefenbach, Dennis and Lopez, Vanessa and Singh, Kamal and Maret, Pierre},
	urldate = {2024-10-27},
	date = {2018-06-01},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\X5XD2HVG\\Diefenbach et al. - 2018 - Core techniques of question answering systems over knowledge bases a survey.pdf:application/pdf},
}

@article{pereira_systematic_2022,
	title = {Systematic review of question answering over knowledge bases},
	volume = {16},
	rights = {© 2021 The Authors. {IET} Software published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	issn = {1751-8814},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1049/sfw2.12028},
	doi = {10.1049/sfw2.12028},
	abstract = {Over the years, a growing number of semantic data repositories have been made available on the web. However, this has created new challenges in exploiting these resources efficiently. Querying services require knowledge beyond the typical user’s expertise, which is a critical issue in adopting semantic information solutions. Several proposals to overcome this difficulty have suggested using question answering ({QA}) systems to provide user-friendly interfaces and allow natural language use. Because question answering over knowledge bases ({KBQAs}) is a very active research topic, a comprehensive view of the field is essential. The purpose of this study was to conduct a systematic review of methods and systems for {KBQAs} to identify their main advantages and limitations. The inclusion criteria rationale was English full-text articles published since 2015 on methods and systems for {KBQAs}. Sixty-six articles were reviewed to describe their underlying reference architectures.},
	pages = {1--13},
	number = {1},
	journaltitle = {{IET} Software},
	author = {Pereira, Arnaldo and Trifan, Alina and Lopes, Rui Pedro and Oliveira, José Luís},
	urldate = {2024-10-27},
	date = {2022},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1049/sfw2.12028},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\VHLRRZFR\\Pereira et al. - 2022 - Systematic review of question answering over knowledge bases.pdf:application/pdf},
}

@inproceedings{riloff_rule-based_2000,
	title = {A Rule-based Question Answering System for Reading Comprehension Tests},
	doi = {10.3115/1117595.1117598},
	booktitle = {{ANLP}-{NAACL} 2000 Workshop: Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems},
	author = {Riloff, Ellen and Thelen, Michael},
	date = {2000},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M5HB3ATU\\Riloff and Thelen - 2000 - A Rule-based Question Answering System for Reading Comprehension Tests.pdf:application/pdf},
}

@inproceedings{singhal_att_1999,
	title = {{AT}\&T at {TREC}-8},
	abstract = {In 1999, {AT}\&T participated in the ad-hoc task and the Question Answering {QA}, Spoken Document Retrieval {SDR}, and Web tracks. Most of our eeort for {TREC}-8 focused on the {QA} and {SDR} tracks. Results from {SDR} track show that our document expansion techniques, presented in 99, are very eeective for speech retrieval. The results for question answering are also encouraging. Our system designed in a relatively short period for this task can the correct answer for about 45 of the user questions. This is specially good given the fact that our system extracts only a short phrase as an answer.},
	author = {Singhal, Amit and Abney, Steve and Bacchiani, Michiel and Collins, Michael and Hindle, Donald and Pereira, Fernando},
	date = {1999-11-01},
	keywords = {notion, finished, general},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BH2UE3PD\\Singhal et al. - 1999 - AT&T at TREC-8.pdf:application/pdf},
}

@inproceedings{li_learning_2002,
	title = {Learning Question Classifiers},
	doi = {10.3115/1072228.107237},
	booktitle = {{COLING} 2002: The 19th International Conference on Computational Linguistics},
	author = {Li, Xin and Roth, Dan},
	date = {2002},
	keywords = {finished, general, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RU9HI9XC\\Li and Roth - 2002 - Learning Question Classifiers.pdf:application/pdf},
}

@article{hu_answering_2018,
	title = {Answering Natural Language Questions by Subgraph Matching over Knowledge Graphs},
	volume = {30},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/8085196/?arnumber=8085196},
	doi = {10.1109/TKDE.2017.2766634},
	abstract = {{RDF} question/answering (Q/A) allows users to ask questions in natural languages over a knowledge base represented by {RDF}. To answer a natural language question, the existing work takes a two-stage approach: question understanding and query evaluation. Their focus is on question understanding to deal with the disambiguation of the natural language phrases. The most common technique is the joint disambiguation, which has the exponential search space. In this paper, we propose a systematic framework to answer natural language questions over {RDF} repository ({RDF} Q/A) from a graph data-driven perspective. We propose a semantic query graph to model the query intention in the natural language question in a structural way, based on which, {RDF} Q/A is reduced to subgraph matching problem. More importantly, we resolve the ambiguity of natural language questions at the time when matches of query are found. The cost of disambiguation is saved if there are no matching found. More specifically, we propose two different frameworks to build the semantic query graph, one is relation (edge)-first and the other one is node-first. We compare our method with some state-of-the-art {RDF} Q/A systems in the benchmark dataset. Extensive experiments confirm that our method not only improves the precision but also speeds up query performance greatly.},
	pages = {824--837},
	number = {5},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Hu, Sen and Zou, Lei and Yu, Jeffrey Xu and Wang, Haixun and Zhao, Dongyan},
	urldate = {2024-10-26},
	date = {2018-05},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\MQ6JK3VF\\Hu et al. - 2018 - Answering Natural Language Questions by Subgraph Matching over Knowledge Graphs.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\PPFK7U6F\\8085196.html:text/html},
}

@misc{noauthor_10th_2022,
	title = {10th Question Answering over Linked Data ({QALD}) Challenge},
	url = {https://www.nliwod.org/challenge},
	urldate = {2024-10-30},
	date = {2022},
	keywords = {notion, finished},
}

@inproceedings{wieringa_design_2009,
	location = {New York, {NY}, {USA}},
	title = {Design science as nested problem solving},
	isbn = {978-1-60558-408-9},
	url = {https://dl.acm.org/doi/10.1145/1555619.1555630},
	doi = {10.1145/1555619.1555630},
	series = {{DESRIST} '09},
	abstract = {Design science emphasizes the connection between knowledge and practice by showing that we can produce scientific knowledge by designing useful things. However, without further guidelines, aspiring design science researchers tend to identify practical problems with knowledge questions, which may lead to methodologically unsound research designs. To solve a practical problem, the real world is changed to suit human purposes, but to solve a knowledge problem, we acquire knowledge about the world without necessarily changing it. In design science, these two kinds of problems are mutually nested, but this nesting should not blind us for the fact that their problem-solving and solution justification methods are different. This paper analyzes the mutual nesting of practical problems and knowledge problems, derives some methodological guidelines from this for design science researchers, and gives an example of a design science project following this problem nesting.},
	pages = {1--12},
	booktitle = {Proceedings of the 4th International Conference on Design Science Research in Information Systems and Technology},
	publisher = {Association for Computing Machinery},
	author = {Wieringa, Roel},
	urldate = {2024-10-29},
	date = {2009-05-07},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\V9PK8CFF\\Wieringa - 2009 - Design science as nested problem solving.pdf:application/pdf},
}

@incollection{stol_guidelines_2020,
	location = {Cham},
	title = {Guidelines for Conducting Software Engineering Research},
	isbn = {978-3-030-32489-6},
	url = {https://doi.org/10.1007/978-3-030-32489-6_2},
	abstract = {This chapter presents a holistic overview of software engineering research strategies. It identifies the two main modes of research within the software engineering research field, namely knowledge-seeking and solution-seeking research—the Design Science model corresponding well with the latter. We present the {ABC} framework for research strategies as a model to structure knowledge-seeking research. The {ABC} represents three desirable aspects of research—generalizability over actors (A), precise control of behavior (B), and realism of context (C). Unfortunately, as our framework illustrates, these three aspects cannot be simultaneously maximized. We describe the two dimensions that provide the foundation of the {ABC} framework—generalizability and control, explain the four different types of settings in which software engineering research is conducted, and position eight archetypal research strategies within the {ABC} framework. We illustrate each strategy with examples, identify appropriate metaphors, and present an example of how the {ABC} framework can be used to design a research program.},
	pages = {27--62},
	booktitle = {Contemporary Empirical Methods in Software Engineering},
	publisher = {Springer International Publishing},
	author = {Stol, Klaas-Jan and Fitzgerald, Brian},
	editor = {Felderer, Michael and Travassos, Guilherme Horta},
	urldate = {2024-10-29},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32489-6_2},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BG9GWXNZ\\Stol and Fitzgerald - 2020 - Guidelines for Conducting Software Engineering Research.pdf:application/pdf},
}

@article{stol_abc_2018,
	title = {The {ABC} of Software Engineering Research},
	volume = {27},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3241743},
	doi = {10.1145/3241743},
	abstract = {A variety of research methods and techniques are available to {SE} researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the {ABC} framework for {SE} research, which offers a holistic view of eight archetypal research strategies. {ABC} refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The {ABC} framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key {SE} domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance {SE} research.},
	pages = {11:1--11:51},
	number = {3},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Stol, Klaas-Jan and Fitzgerald, Brian},
	urldate = {2024-10-29},
	date = {2018-09-17},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\F8CP8UHW\\Stol and Fitzgerald - 2018 - The ABC of Software Engineering Research.pdf:application/pdf},
}

@inproceedings{sjoberg_future_2007,
	title = {The Future of Empirical Methods in Software Engineering Research},
	doi = {10.1109/FOSE.2007.30},
	abstract = {We present the vision that for all fields of software engineering ({SE}), empirical research methods should enable the development of scientific knowledge about how useful different {SE} technologies are for different kinds of actors, performing different kinds of activities, on different kinds of systems. It is part of the vision that such scientific knowledge will guide the development of new {SE} technology and is a major input to important {SE} decisions in industry. Major challenges to the pursuit of this vision are: more {SE} research should be based on the use of empirical methods; the quality, including relevance, of the studies using such methods should be increased; there should be more and better synthesis of empirical evidence; and more theories should be built and tested. Means to meet these challenges include (1) increased competence regarding how to apply and combine alternative empirical methods, (2) tighter links between academia and industry, (3) the development of common research agendas with a focus on empirical methods, and (4) more resources for empirical research.},
	eventtitle = {Future of Software Engineering ({FOSE} '07)},
	pages = {358--378},
	booktitle = {Future of Software Engineering ({FOSE} '07)},
	author = {Sjoberg, Dag I. K. and Dyba, Tore and Jorgensen, Magne},
	date = {2007-05},
	keywords = {domain\_specific, finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\9SLHWRLK\\Sjoberg et al. - 2007 - The Future of Empirical Methods in Software Engineering Research.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\ILRQHH33\\4221632.html:text/html},
}

@inproceedings{wohlin_guidelines_2014,
	location = {New York, {NY}, {USA}},
	title = {Guidelines for snowballing in systematic literature studies and a replication in software engineering},
	isbn = {978-1-4503-2476-2},
	doi = {10.1145/2601248.2601268},
	series = {{EASE} '14},
	abstract = {Background: Systematic literature studies have become common in software engineering, and hence it is important to understand how to conduct them efficiently and reliably.Objective: This paper presents guidelines for conducting literature reviews using a snowballing approach, and they are illustrated and evaluated by replicating a published systematic literature review.Method: The guidelines are based on the experience from conducting several systematic literature reviews and experimenting with different approaches.Results: The guidelines for using snowballing as a way to search for relevant literature was successfully applied to a systematic literature review.Conclusions: It is concluded that using snowballing, as a first search strategy, may very well be a good alternative to the use of database searches.},
	pages = {1--10},
	booktitle = {Proceedings of the 18th International Conference on Evaluation and Assessment in Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Wohlin, Claes},
	urldate = {2024-11-10},
	date = {2014-05-13},
	keywords = {finished, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YM4Z7TPF\\Wohlin - 2014 - Guidelines for snowballing in systematic literature studies and a replication in software engineerin.pdf:application/pdf},
}

@article{dillon_classification_1984,
	title = {The Classification of Research Questions},
	volume = {54},
	doi = {10.3102/00346543054003327},
	abstract = {What are the kinds of questions that may be posed for research? A dozen schemes proposing to classify research questions are surveyed, analyzed, and applied to the understanding and practice of inquiry. The extent to which the various schemes account for questions found in educational journals is estimated. Some principles and issues are identified to stimulate work on the classification of research questions in education and other enterprises of inquiry. On the whole, little is known about the kinds of questions that may be posed for research.},
	pages = {327--361},
	number = {3},
	journaltitle = {Review of Educational Research},
	author = {Dillon, J. T.},
	date = {1984-09-01},
	langid = {english},
	keywords = {finished, notion},
	file = {SAGE PDF Full Text:C\:\\Users\\Marco\\Zotero\\storage\\P8PL8YLD\\Dillon - 1984 - The Classification of Research Questions.pdf:application/pdf},
}

@article{thuan_construction_2019,
	title = {Construction of Design Science Research Questions},
	volume = {44},
	issn = {1529-3181},
	doi = {10.17705/1CAIS.04420},
	number = {1},
	journaltitle = {Communications of the Association for Information Systems},
	author = {Thuan, Nguyen and Drechsler, Andreas and Antunes, Pedro},
	date = {2019-03-01},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\W3667CHP\\Thuan et al. - 2019 - Construction of Design Science Research Questions.pdf:application/pdf},
}

@misc{li_few-shot_2021,
	title = {Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models},
	url = {http://arxiv.org/abs/2106.01623},
	doi = {10.48550/arXiv.2106.01623},
	abstract = {This paper studies how to automatically generate a natural language text that describes the facts in knowledge graph ({KG}). Considering the few-shot setting, we leverage the excellent capacities of pretrained language models ({PLMs}) in language understanding and generation. We make three major technical contributions, namely representation alignment for bridging the semantic gap between {KG} encodings and {PLMs}, relation-biased {KG} linearization for deriving better input representations, and multi-task learning for learning the correspondence between {KG} and text. Extensive experiments on three benchmark datasets have demonstrated the effectiveness of our model on {KG}-to-text generation task. In particular, our model outperforms all comparison methods on both fully-supervised and few-shot settings. Our code and datasets are available at https://github.com/{RUCAIBox}/Few-Shot-{KG}2Text.},
	number = {{arXiv}:2106.01623},
	publisher = {{arXiv}},
	author = {Li, Junyi and Tang, Tianyi and Zhao, Wayne Xin and Wei, Zhicheng and Yuan, Nicholas Jing and Wen, Ji-Rong},
	urldate = {2024-11-10},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2106.01623},
	keywords = {notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\UU2TYGR6\\Li et al. - 2021 - Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\LJR6483R\\2106.html:text/html},
}

@misc{luo_graph-constrained_2024,
	title = {Graph-constrained Reasoning: Faithful Reasoning on Knowledge Graphs with Large Language Models},
	url = {http://arxiv.org/abs/2410.13080},
	doi = {10.48550/arXiv.2410.13080},
	shorttitle = {Graph-constrained Reasoning},
	abstract = {Large language models ({LLMs}) have demonstrated impressive reasoning abilities, but they still struggle with faithful reasoning due to knowledge gaps and hallucinations. To address these issues, knowledge graphs ({KGs}) have been utilized to enhance {LLM} reasoning through their structured knowledge. However, existing {KG}-enhanced methods, either retrieval-based or agent-based, encounter difficulties in accurately retrieving knowledge and efficiently traversing {KGs} at scale. In this work, we introduce graph-constrained reasoning ({GCR}), a novel framework that bridges structured knowledge in {KGs} with unstructured reasoning in {LLMs}. To eliminate hallucinations, {GCR} ensures faithful {KG}-grounded reasoning by integrating {KG} structure into the {LLM} decoding process through {KG}-Trie, a trie-based index that encodes {KG} reasoning paths. {KG}-Trie constrains the decoding process, allowing {LLMs} to directly reason on graphs and generate faithful reasoning paths grounded in {KGs}. Additionally, {GCR} leverages a lightweight {KG}-specialized {LLM} for graph-constrained reasoning alongside a powerful general {LLM} for inductive reasoning over multiple reasoning paths, resulting in accurate reasoning with zero reasoning hallucination. Extensive experiments on several {KGQA} benchmarks demonstrate that {GCR} achieves state-of-the-art performance and exhibits strong zero-shot generalizability to unseen {KGs} without additional training.},
	number = {{arXiv}:2410.13080},
	publisher = {{arXiv}},
	author = {Luo, Linhao and Zhao, Zicheng and Gong, Chen and Haffari, Gholamreza and Pan, Shirui},
	urldate = {2024-11-20},
	date = {2024-10-16},
	eprinttype = {arxiv},
	eprint = {2410.13080},
	keywords = {notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\WRUQP7Y6\\Luo et al. - 2024 - Graph-constrained Reasoning Faithful Reasoning on Knowledge Graphs with Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\IZQHMNWB\\2410.html:text/html},
}

@misc{yu_decaf_2023,
	title = {{DecAF}: Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases},
	url = {http://arxiv.org/abs/2210.00063},
	doi = {10.48550/arXiv.2210.00063},
	shorttitle = {{DecAF}},
	abstract = {Question answering over knowledge bases ({KBs}) aims to answer natural language questions with factual information such as entities and relations in {KBs}. Previous methods either generate logical forms that can be executed over {KBs} to obtain final answers or predict answers directly. Empirical results show that the former often produces more accurate answers, but it suffers from non-execution issues due to potential syntactic and semantic errors in the generated logical forms. In this work, we propose a novel framework {DecAF} that jointly generates both logical forms and direct answers, and then combines the merits of them to get the final answers. Moreover, different from most of the previous methods, {DecAF} is based on simple free-text retrieval without relying on any entity linking tools -- this simplification eases its adaptation to different datasets. {DecAF} achieves new state-of-the-art accuracy on {WebQSP}, {FreebaseQA}, and {GrailQA} benchmarks, while getting competitive results on the {ComplexWebQuestions} benchmark.},
	number = {{arXiv}:2210.00063},
	publisher = {{arXiv}},
	author = {Yu, Donghan and Zhang, Sheng and Ng, Patrick and Zhu, Henghui and Li, Alexander Hanbo and Wang, Jun and Hu, Yiqun and Wang, William and Wang, Zhiguo and Xiang, Bing},
	urldate = {2024-11-20},
	date = {2023-04-14},
	eprinttype = {arxiv},
	eprint = {2210.00063},
	keywords = {notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\W3WH6GUL\\Yu et al. - 2023 - DecAF Joint Decoding of Answers and Logical Forms for Question Answering over Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\PXMQW43I\\2210.html:text/html},
}

@unpublished{sack_knowledge_2024,
	title = {Knowledge (Graph) Driven Research Data Management in the Age of {LLMs}},
	url = {https://zenodo.org/records/14191610},
	abstract = {One of the difficulties in general research data management lies into the fact that different scientific disciplines interpret specific concepts differently. Moreover, even if you agree about the research object to be represented, there are various different ways and principles to follow. Ontologies might help to enable {FAIR} research data management, i.e. to make research data findable, accessible, interoperable, and reusable. However, in the age of large language model ({LLM}), the effort of ontology-driven development is questioned and reevaluated. This presentation will give an introduction into research data management ({RDM}) in general, and in knowledge graph-basewd {RDM} in particular. On the example of the German National Research Data Infrastructure programme, a knowledge graph-based {RDM} approach is presented, accompanied by first experiments in how {LLMs} can improve efficiency and quality of the ontological engineering lifecycle.},
	author = {Sack, Harald},
	urldate = {2024-11-20},
	date = {2024-11-20},
	doi = {10.5281/zenodo.14191610},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\LWDNQMRP\\Sack - 2024 - Knowledge (Graph) Driven Research Data Management in the Age of LLMs.pdf:application/pdf},
}

@inproceedings{li_kgmistral_2024,
	title = {{KGMistral}: Towards Boosting the Performance of Large Language Models for Question Answering with Knowledge Graph Integration},
	url = {https://openreview.net/forum?id=JzL0qm3YA8},
	shorttitle = {{KGMistral}},
	abstract = {In this paper, a novel question-answering ({QA}) approach named {KGMistral} is proposed, based on the Retrieval Augmented Generation ({RAG}) framework. Given the limitations of Large Language Models ({LLMs}) in generating accurate answers for domains not adequately covered by their training corpus, this work focuses on leveraging external domain-specific Knowledge Graphs ({KGs}) to enhance the performance of {LLMs}. Specifically, the study examines the benefits of using information from a {KG} to improve the {QA} performance of the Mistral model in the material science and engineering field. Experimental results indicate that {KGMistral} significantly enhances Mistral’s {QA} performance.},
	eventtitle = {Workshop on Deep Learning and Large Language Models for Knowledge Graphs},
	author = {Li, Mingze and Yang, Haoran and Liu, Zhaotai and Alam, Mirza Mohtashim and Ebrahim and Sack, Harald and Gesese, Genet Asefa},
	urldate = {2024-11-20},
	date = {2024-07-09},
	langid = {english},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ECGJK9LR\\Li et al. - 2024 - KGMistral Towards Boosting the Performance of Large Language Models for Question Answering with Kno.pdf:application/pdf},
}

@inproceedings{becker_sustainability_2015,
	title = {Sustainability Design and Software: The Karlskrona Manifesto},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/7202997},
	doi = {10.1109/ICSE.2015.179},
	shorttitle = {Sustainability Design and Software},
	abstract = {Sustainability has emerged as a broad concern for society. Many engineering disciplines have been grappling with challenges in how we sustain technical, social and ecological systems. In the software engineering community, for example, maintainability has been a concern for a long time. But too often, these issues are treated in isolation from one another. Misperceptions among practitioners and research communities persist, rooted in a lack of coherent understanding of sustainability, and how it relates to software systems research and practice. This article presents a cross-disciplinary initiative to create a common ground and a point of reference for the global community of research and practice in software and sustainability, to be used for effectively communicating key issues, goals, values and principles of sustainability design for software-intensive systems.The centrepiece of this effort is the Karlskrona Manifesto for Sustainability Design, a vehicle for a much needed conversation about sustainability within and beyond the software community, and an articulation of the fundamental principles underpinning design choices that affect sustainability. We describe the motivation for developing this manifesto, including some considerations of the genre of the manifesto as well as the dynamics of its creation. We illustrate the collaborative reflective writing process and present the current edition of the manifesto itself. We assess immediate implications and applications of the articulated principles, compare these to current practice, and suggest future steps.},
	eventtitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	pages = {467--476},
	booktitle = {2015 {IEEE}/{ACM} 37th {IEEE} International Conference on Software Engineering},
	author = {Becker, Christoph and Chitchyan, Ruzanna and Duboc, Leticia and Easterbrook, Steve and Penzenstadler, Birgit and Seyff, Norbert and Venters, Colin C.},
	urldate = {2024-11-25},
	date = {2015-05},
	note = {{ISSN}: 1558-1225},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\FTJ585EX\\Becker et al. - 2015 - Sustainability Design and Software The Karlskrona Manifesto.pdf:application/pdf},
}

@article{fandino_formulating_2019,
	title = {Formulating a good research question: Pearls and pitfalls},
	volume = {63},
	issn = {0019-5049},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6691636/},
	doi = {10.4103/ija.IJA_198_19},
	shorttitle = {Formulating a good research question},
	abstract = {The process of formulating a good research question can be challenging and frustrating. While a comprehensive literature review is compulsory, the researcher usually encounters methodological difficulties in the conduct of the study, particularly if the primary study question has not been adequately selected in accordance with the clinical dilemma that needs to be addressed. Therefore, optimising time and resources before embarking in the design of a clinical protocol can make an impact on the final results of the research project. Researchers have developed effective ways to convey the message of how to build a good research question that can be easily recalled under the acronyms of {PICOT} (population, intervention, comparator, outcome, and time frame) and {FINER} (feasible, interesting, novel, ethical, and relevant). In line with these concepts, this article highlights the main issues faced by clinicians, when developing a research question.},
	pages = {611--616},
	number = {8},
	journaltitle = {Indian Journal of Anaesthesia},
	shortjournal = {Indian J Anaesth},
	author = {Fandino, Wilson},
	urldate = {2024-11-26},
	date = {2019-08},
	pmid = {31462805},
	pmcid = {PMC6691636},
	keywords = {notion, no type information},
	file = {PubMed Central Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BUG64PVV\\Fandino - 2019 - Formulating a good research question Pearls and pitfalls.pdf:application/pdf},
}

@article{agee_developing_2009,
	title = {Developing qualitative research questions: a reflective process},
	volume = {22},
	issn = {0951-8398},
	url = {https://doi.org/10.1080/09518390902736512},
	doi = {10.1080/09518390902736512},
	shorttitle = {Developing qualitative research questions},
	abstract = {The reflective and interrogative processes required for developing effective qualitative research questions can give shape and direction to a study in ways that are often underestimated. Good research questions do not necessarily produce good research, but poorly conceived or constructed questions will likely create problems that affect all subsequent stages of a study. In qualitative studies, the ongoing process of questioning is an integral part of understanding the unfolding lives and perspectives of others. This article addresses both the development of initial research questions and how the processes of generating and refining questions are critical to the shaping of a qualitative study.},
	pages = {431--447},
	number = {4},
	journaltitle = {International Journal of Qualitative Studies in Education},
	author = {Agee, Jane},
	urldate = {2024-11-26},
	date = {2009-07-01},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/09518390902736512},
	keywords = {notion, no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SDZ5PV3Q\\Agee - 2009 - Developing qualitative research questions a reflective process.pdf:application/pdf},
}

@article{loni_survey_2011,
	title = {A Survey of State-of-the-Art Methods on Question Classification},
	url = {https://repository.tudelft.nl/record/uuid:8e57caa8-04fc-4fe2-b668-20767ab3db92},
	abstract = {The task of question classification ({QC}) is to predict the entity type of a question which is written in natural language. This is done by classifying the question to a category from a set of predefined categories. Question classification is an important component of question answering systems and it attracted a notable amount of research since the past decade. This paper gives a comprehensive overview of the state-of-the-art approaches in question classification and provides a detailed comparison of recent works on question classification and discussed about possible extensions to {QC} problem.},
	author = {Loni, B.},
	urldate = {2024-11-27},
	date = {2011},
	langid = {english},
	keywords = {notion, wiederholung},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EECAP3NS\\Loni - 2011 - A Survey of State-of-the-Art Methods on Question Classification.pdf:application/pdf},
}

@inproceedings{damiano_towards_2016,
	title = {Towards a Framework for Closed-Domain Question Answering in Italian},
	url = {https://ieeexplore.ieee.org/document/7907527},
	doi = {10.1109/SITIS.2016.100},
	abstract = {In the last years, Cognitive Systems are increasingly appearing, offering new ways for developing Question Answering solutions able to autonomously extract an answer for a question formulated in natural language. Currently, to the best of our knowledge, most of the available Question Answering solutions are designed for the English language and use {SQL}-like knowledge bases to provide factual answers to a natural language question. Starting from these considerations, this work presents a preliminary Question Answering framework for closed-domains, like Cultural Heritage. It has been expressly thought to extract factual answers from collections of documents by operating with the Italian language. Such a framework exploits a variety of {NLP} methods for the Italian language to help the understanding of user's questions and the extraction of precise answers from textual passages contained into documents. Moreover, Deep Learning techniques have been used to proficiently understand the topic of a question, whereas a rule-based approach relying on dictionaries has been applied for the annotation and indexing of collections of documents in Italian, enabling their usage into a state-of-the-art Information Retrieval engine. An experimental session has also been arranged, showing very promising preliminary results.},
	eventtitle = {2016 12th International Conference on Signal-Image Technology \& Internet-Based Systems ({SITIS})},
	pages = {604--611},
	booktitle = {2016 12th International Conference on Signal-Image Technology \& Internet-Based Systems ({SITIS})},
	author = {Damiano, Emanuele and Spinelli, Raffaele and Esposito, Massimo and De Pietro, Giuseppe},
	urldate = {2024-11-27},
	date = {2016-11},
	keywords = {notion, Cognitive Computing, Cultural differences, Data models, Italian Text, Knowledge based systems, Knowledge discovery, Machine learning, Natural languages, {NLP}, Pipelines, Question answering, Unstructured Information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JTEZYTDB\\Damiano et al. - 2016 - Towards a Framework for Closed-Domain Question Answering in Italian.pdf:application/pdf},
}

@article{madabushi_high_nodate,
	title = {High Accuracy Rule-based Question Classiﬁcation using Question Syntax and Semantics},
	abstract = {We present in this paper a purely rule-based system for Question Classiﬁcation which we divide into two parts: The ﬁrst is the extraction of relevant words from a question by use of its structure, and the second is the classiﬁcation of questions based on rules that associate these words to Concepts. We achieve an accuracy of 97.2\%, close to a 6 point improvement over the previous State of the Art of 91.6\%. Additionally, we believe that machine learning algorithms can be applied on top of this method to further improve accuracy.},
	author = {Madabushi, Harish Tayyar and Lee, Mark},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\5E6HNLGY\\Madabushi und Lee - High Accuracy Rule-based Question Classiﬁcation using Question Syntax and Semantics.pdf:application/pdf},
}

@article{mishra_survey_2016,
	title = {A survey on question answering systems with classification},
	volume = {28},
	issn = {1319-1578},
	url = {https://www.sciencedirect.com/science/article/pii/S1319157815000890},
	doi = {10.1016/j.jksuci.2014.10.007},
	abstract = {Question answering systems ({QASs}) generate answers of questions asked in natural languages. Early {QASs} were developed for restricted domains and have limited capabilities. Current {QASs} focus on types of questions generally asked by users, characteristics of data sources consulted, and forms of correct answers generated. Research in the area of {QASs} began in 1960s and since then, a large number of {QASs} have been developed. To identify the future scope of research in this area, the need of a comprehensive survey on {QASs} arises naturally. This paper surveys {QASs} and classifies them based on different criteria. We identify the current status of the research in the each category of {QASs}, and suggest future scope of the research.},
	pages = {345--361},
	number = {3},
	journaltitle = {Journal of King Saud University - Computer and Information Sciences},
	shortjournal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Mishra, Amit and Jain, Sanjay Kumar},
	urldate = {2024-11-27},
	date = {2016-07-01},
	keywords = {notion, Information retrieval, Natural language processing, Natural language understanding, Question answering system, Search engine, no type information},
	file = {ScienceDirect Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EWYKRSZT\\Mishra und Jain - 2016 - A survey on question answering systems with classification.pdf:application/pdf},
}

@inproceedings{chernov_linguistically_2015,
	location = {Vilnius, Lithuania},
	title = {Linguistically Motivated Question Classification},
	url = {https://aclanthology.org/W15-1809},
	eventtitle = {{NoDaLiDa} 2015},
	pages = {51--59},
	booktitle = {Proceedings of the 20th Nordic Conference of Computational Linguistics ({NODALIDA} 2015)},
	publisher = {Linköping University Electronic Press, Sweden},
	author = {Chernov, Alexandr and Petukhova, Volha and Klakow, Dietrich},
	editor = {Megyesi, Beáta},
	urldate = {2024-11-27},
	date = {2015-05},
	keywords = {notion, finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\29FG3BPG\\Chernov et al. - 2015 - Linguistically Motivated Question Classification.pdf:application/pdf},
}

@article{nguyen_ripple_2017,
	title = {Ripple Down Rules for question answering},
	volume = {8},
	issn = {1570-0844},
	doi = {10.3233/SW-150204},
	abstract = {Recent years have witnessed a new trend of building ontology-based question answering systems. These systems use semantic web information to produce more precise answers to users’ queries. However, these systems are mostly designed for English. In th},
	pages = {511--532},
	number = {4},
	journaltitle = {Semantic Web},
	author = {Nguyen, Dat Quoc and Pham, Son Bao},
	date = {2017-01-01},
	langid = {english},
	keywords = {finished, notion},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\DHZ8ALS4\\Nguyen et al. - 2017 - Ripple Down Rules for question answering.pdf:application/pdf},
}

@article{kaur_visual_2022,
	title = {Visual citation navigation of open education resources using Litmaps},
	volume = {39},
	rights = {https://www.emerald.com/insight/site-policies},
	issn = {0741-9058, 0741-9058},
	url = {https://www.emerald.com/insight/content/doi/10.1108/LHTN-01-2022-0012/full/html},
	doi = {10.1108/LHTN-01-2022-0012},
	abstract = {Purpose
              The purpose of this study is to visualize the key literature on the topic “Open Educational Resources” using the research discovery tool “Litmaps”.
            
            
              Design/methodology/approach
              Litmaps visual citation navigation, the ultimate science discovery platform, is used for the present study. It provides an interface for discovering scientific literature, explores the research landscape and discovers articles that are highly connected to maps. Litmaps provides quick-start options to import articles from reference manager, keyword search, {ORCID} {ID}, {DOI} or using a seed article. In this paper, “keyword search” and research strategy “Open Educational Resources” or “{OER}” are put to use.
            
            
              Findings
              The findings of the study revealed that Litmaps gives citations between articles over time visually. The map generated is dynamic as it is adjustable for making the map according to the researcher’s needs.
            
            
              Research limitations/implications
              Litmaps helps researchers in doing the literature review in a very brief and systematic way. It is helpful in finding the related or relevant studies through the seed paper/keyword search.
            
            
              Originality/value
              The study makes a useful contribution to the literature on this topic as one can independently find research topics and also compare topic overlapping. The study provides insights that help researchers in building citation maps and see connections between articles over time. The originality of the present paper lies in highlighting the importance of the research discovery tool Litmaps for the researchers as so far, to the best of the authors’ knowledge, no research has been taken place on using it.},
	pages = {7--11},
	number = {5},
	journaltitle = {Library Hi Tech News},
	shortjournal = {{LHTN}},
	author = {Kaur, Amanpreet and Gulati, Sarita and Sharma, Ritu and Sinhababu, Atasi and Chakravarty, Rupak},
	urldate = {2024-11-27},
	date = {2022-05-12},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\R6S7MLSH\\Kaur et al. - 2022 - Visual citation navigation of open education resources using Litmaps.pdf:application/pdf},
}

@incollection{gruninger_role_1995,
	location = {Boston, {MA}},
	title = {The Role of Competency Questions in Enterprise Engineering},
	isbn = {978-0-387-34847-6},
	url = {https://doi.org/10.1007/978-0-387-34847-6_3},
	abstract = {We present a logical framework for representing activities, states, time, and cost in an enterprise integration architecture. We define ontologies for these concepts in first-order logic and consider the problems of temporal projection and reasoning about the occurrence of actions. We characterize the ontology with the use of competency questions. The ontology must contain a necessary and sufficient set of axioms to represent and solve these questions. These questions not only characterize existing ontologies for enterprise engineering, but also drive the development of new ontologies that are required to solve the competency questions.},
	pages = {22--31},
	booktitle = {Benchmarking — Theory and Practice},
	publisher = {Springer {US}},
	author = {Grüninger, Michael and Fox, Mark S.},
	editor = {Rolstadås, Asbjørn},
	urldate = {2024-11-27},
	date = {1995},
	langid = {english},
	doi = {10.1007/978-0-387-34847-6_3},
	keywords = {notion, Enterprise Engineering, Enterprise Model, Quality Function Deployment, Situation Calculus, Temporal Projection, no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6VRYG3KW\\Grüninger und Fox - 1995 - The Role of Competency Questions in Enterprise Engineering.pdf:application/pdf},
}

@misc{hogan_knowledge_2021,
	title = {Knowledge Graphs},
	url = {http://arxiv.org/abs/2003.02320},
	doi = {10.48550/arXiv.2003.02320},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	number = {{arXiv}:2003.02320},
	publisher = {{arXiv}},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and Melo, Gerard de and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	urldate = {2024-11-27},
	date = {2021-09-11},
	eprinttype = {arxiv},
	eprint = {2003.02320},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\ECM3ERJ4\\Hogan et al. - 2021 - Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\P4XUQSCC\\2003.html:text/html},
}

@misc{jaradeh_question_2020,
	title = {Question Answering on Scholarly Knowledge Graphs},
	doi = {10.48550/arXiv.2006.01527},
	abstract = {Answering questions on scholarly knowledge comprising text and other artifacts is a vital part of any research life cycle. Querying scholarly knowledge and retrieving suitable answers is currently hardly possible due to the following primary reason: machine inactionable, ambiguous and unstructured content in publications. We present {JarvisQA}, a {BERT} based system to answer questions on tabular views of scholarly knowledge graphs. Such tables can be found in a variety of shapes in the scholarly literature (e.g., surveys, comparisons or results). Our system can retrieve direct answers to a variety of different questions asked on tabular data in articles. Furthermore, we present a preliminary dataset of related tables and a corresponding set of natural language questions. This dataset is used as a benchmark for our system and can be reused by others. Additionally, {JarvisQA} is evaluated on two datasets against other baselines and shows an improvement of two to three folds in performance compared to related methods.},
	number = {{arXiv}:2006.01527},
	publisher = {{arXiv}},
	author = {Jaradeh, Mohamad Yaser and Stocker, Markus and Auer, Sören},
	date = {2020-06-02},
	eprinttype = {arxiv},
	eprint = {2006.01527},
	keywords = {finished, notion, related\_dataset},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\6H5I4YVV\\Jaradeh et al. - 2020 - Question Answering on Scholarly Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\P8HTJ8QY\\2006.html:text/html},
}

@inproceedings{su_generating_2016,
	location = {Austin, Texas},
	title = {On Generating Characteristic-rich Question Sets for {QA} Evaluation},
	url = {https://aclanthology.org/D16-1054},
	doi = {10.18653/v1/D16-1054},
	eventtitle = {{EMNLP} 2016},
	pages = {562--572},
	booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Su, Yu and Sun, Huan and Sadler, Brian and Srivatsa, Mudhakar and Gür, Izzeddin and Yan, Zenghui and Yan, Xifeng},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	urldate = {2024-11-27},
	date = {2016-11},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\U3ZR2WVU\\Su et al. - 2016 - On Generating Characteristic-rich Question Sets for QA Evaluation.pdf:application/pdf},
}

@misc{gu_beyond_2021,
	title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
	url = {http://arxiv.org/abs/2011.07743},
	doi = {10.48550/arXiv.2011.07743},
	shorttitle = {Beyond I.I.D.},
	abstract = {Existing studies on question answering on knowledge bases ({KBQA}) mainly operate with the standard i.i.d assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d may be neither reasonably achievable nor desirable on large-scale {KBs} because 1) true user distribution is hard to capture and 2) randomly sample training examples from the enormous space would be highly data-inefficient. Instead, we suggest that {KBQA} models should have three levels of built-in generalization: i.i.d, compositional, and zero-shot. To facilitate the development of {KBQA} models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, {GrailQA}, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel {BERT}-based {KBQA} model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like {BERT} in the generalization of {KBQA}.},
	number = {{arXiv}:2011.07743},
	publisher = {{arXiv}},
	author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
	urldate = {2024-11-27},
	date = {2021-02-22},
	eprinttype = {arxiv},
	eprint = {2011.07743},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\B3SUVJH6\\Gu et al. - 2021 - Beyond I.I.D. Three Levels of Generalization for Question Answering on Knowledge Bases.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8WMLDLX2\\2011.html:text/html},
}

@inproceedings{cao_kqa_2022,
	location = {Dublin, Ireland},
	title = {{KQA} Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base},
	url = {https://aclanthology.org/2022.acl-long.422},
	doi = {10.18653/v1/2022.acl-long.422},
	shorttitle = {{KQA} Pro},
	abstract = {Complex question answering over knowledge base (Complex {KBQA}) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation, etc. Existing benchmarks have some shortcomings that limit the development of Complex {KBQA}: 1) they only provide {QA} pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce {KQA} Pro, a dataset for Complex {KBQA} including around 120K diverse natural language questions. We introduce a compositional and interpretable programming language {KoPL} to represent the reasoning process of complex questions. For each question, we provide the corresponding {KoPL} program and {SPARQL} query, so that {KQA} Pro can serve for both {KBQA} and semantic parsing tasks. Experimental results show that state-of-the-art {KBQA} methods cannot achieve promising results on {KQA} Pro as on current datasets, which suggests that {KQA} Pro is challenging and Complex {KBQA} requires further research efforts. We also treat {KQA} Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex {KBQA}. Our codes and datasets can be obtained from https://github.com/shijx12/{KQAPro}\_Baselines.},
	eventtitle = {{ACL} 2022},
	pages = {6101--6119},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Shulin and Shi, Jiaxin and Pan, Liangming and Nie, Lunyiu and Xiang, Yutong and Hou, Lei and Li, Juanzi and He, Bin and Zhang, Hanwang},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-11-27},
	date = {2022-05},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BD2EU9VN\\Cao et al. - 2022 - KQA Pro A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledg.pdf:application/pdf},
}

@misc{keysers_measuring_2020,
	title = {Measuring Compositional Generalization: A Comprehensive Method on Realistic Data},
	url = {http://arxiv.org/abs/1912.09713},
	doi = {10.48550/arXiv.1912.09713},
	shorttitle = {Measuring Compositional Generalization},
	abstract = {State-of-the-art machine learning methods exhibit limited compositional generalization. At the same time, there is a lack of realistic benchmarks that comprehensively measure this ability, which makes it challenging to find and evaluate improvements. We introduce a novel method to systematically construct such benchmarks by maximizing compound divergence while guaranteeing a small atom divergence between train and test sets, and we quantitatively compare this method to other approaches for creating compositional generalization benchmarks. We present a large and realistic natural language question answering dataset that is constructed according to this method, and we use it to analyze the compositional generalization ability of three machine learning architectures. We find that they fail to generalize compositionally and that there is a surprisingly strong negative correlation between compound divergence and accuracy. We also demonstrate how our method can be used to create new compositionality benchmarks on top of the existing {SCAN} dataset, which confirms these findings.},
	number = {{arXiv}:1912.09713},
	publisher = {{arXiv}},
	author = {Keysers, Daniel and Schärli, Nathanael and Scales, Nathan and Buisman, Hylke and Furrer, Daniel and Kashubin, Sergii and Momchev, Nikola and Sinopalnikov, Danila and Stafiniak, Lukasz and Tihon, Tibor and Tsarkov, Dmitry and Wang, Xiao and Zee, Marc van and Bousquet, Olivier},
	urldate = {2024-11-27},
	date = {2020-06-25},
	eprinttype = {arxiv},
	eprint = {1912.09713},
	keywords = {notion, Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\L2D43AHD\\Keysers et al. - 2020 - Measuring Compositional Generalization A Comprehensive Method on Realistic Data.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\UWQ8NN5P\\1912.html:text/html},
}

@misc{feng_knowledge_2023,
	title = {Knowledge Solver: Teaching {LLMs} to Search for Domain Knowledge from Knowledge Graphs},
	doi = {10.48550/arXiv.2309.03118},
	shorttitle = {Knowledge Solver},
	abstract = {Large language models ({LLMs}), such as {ChatGPT} and {GPT}-4, are versatile and can solve different tasks due to their emergent ability and generalizability. However, {LLMs} sometimes lack domain-specific knowledge to perform tasks, which would also cause hallucination during inference. In some previous works, additional modules like graph neural networks ({GNNs}) are trained on retrieved knowledge from external knowledge bases, aiming to mitigate the problem of lacking domain-specific knowledge. However, incorporating additional modules: 1) would need retraining additional modules when encountering novel domains; 2) would become a bottleneck since {LLMs}' strong abilities are not fully utilized for retrieval. In this paper, we propose a paradigm, termed Knowledge Solver ({KSL}), to teach {LLMs} to search for essential knowledge from external knowledge bases by harnessing their own strong generalizability. Specifically, we design a simple yet effective prompt to transform retrieval into a multi-hop decision sequence, which empowers {LLMs} with searching knowledge ability in zero-shot manner. Additionally, {KSL} is able to provide complete retrieval paths and therefore increase explainability of {LLMs}' reasoning processes. We conduct experiments on three datasets: {CommonsenseQA}, {OpenbookQA}, and {MedQA}-{USMLE}, and found that our approach improves {LLM} baseline performance by a relatively large margin.},
	number = {{arXiv}:2309.03118},
	publisher = {{arXiv}},
	author = {Feng, Chao and Zhang, Xinyu and Fei, Zichu},
	date = {2023-09-06},
	eprinttype = {arxiv},
	eprint = {2309.03118},
	keywords = {Computer Science - Computation and Language, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BLS4NYKX\\Feng et al. - 2023 - Knowledge Solver Teaching LLMs to Search for Domain Knowledge from Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\E9JI25BK\\2309.html:text/html},
}

@misc{choudhary_complex_2024,
	title = {Complex Logical Reasoning over Knowledge Graphs using Large Language Models},
	url = {http://arxiv.org/abs/2305.01157},
	doi = {10.48550/arXiv.2305.01157},
	abstract = {Reasoning over knowledge graphs ({KGs}) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs ({LARK}), that formulates complex {KG} reasoning as a combination of contextual {KG} search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models ({LLM}), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art {KG} reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying {LLM}, enabling the integration of the latest advancements in {LLMs} for logical reasoning over {KGs}. Our work presents a new direction for addressing the challenges of complex {KG} reasoning and paves the way for future research in this area.},
	number = {{arXiv}:2305.01157},
	publisher = {{arXiv}},
	author = {Choudhary, Nurendra and Reddy, Chandan K.},
	urldate = {2024-11-29},
	date = {2024-03-31},
	eprinttype = {arxiv},
	eprint = {2305.01157},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Logic in Computer Science},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2WHDS8VM\\Choudhary und Reddy - 2024 - Complex Logical Reasoning over Knowledge Graphs using Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\HUE2X7YI\\2305.html:text/html},
}

@article{ratan_formulation_2019,
	title = {Formulation of Research Question – Stepwise Approach},
	volume = {24},
	doi = {10.4103/jiaps.JIAPS_76_18},
	abstract = {Formulation of research question ({RQ}) is an essentiality before starting any research. It aims to explore an existing uncertainty in an area of concern and points to a need for deliberate investigation. It is, therefore, pertinent to formulate a good {RQ}. The present paper aims to discuss the process of formulation of {RQ} with stepwise approach. The characteristics of good {RQ} are expressed by acronym “{FINERMAPS}” expanded as feasible, interesting, novel, ethical, relevant, manageable, appropriate, potential value, publishability, and systematic. A {RQ} can address different formats depending on the aspect to be evaluated. Based on this, there can be different types of {RQ} such as based on the existence of the phenomenon, description and classification, composition, relationship, comparative, and causality. To develop a {RQ}, one needs to begin by identifying the subject of interest and then do preliminary research on that subject. The researcher then defines what still needs to be known in that particular subject and assesses the implied questions. After narrowing the focus and scope of the research subject, researcher frames a {RQ} and then evaluates it. Thus, conception to formulation of {RQ} is very systematic process and has to be performed meticulously as research guided by such question can have wider impact in the field of social and health research by leading to formulation of policies for the benefit of larger population.},
	pages = {15},
	journaltitle = {Journal of Indian Association of Pediatric Surgeons},
	shortjournal = {Journal of Indian Association of Pediatric Surgeons},
	author = {Ratan, {SimmiK} and Anand, Tanu and Ratan, John},
	date = {2019-01-01},
	keywords = {notion, finished},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7SRKUYPL\\Ratan et al. - 2019 - Formulation of Research Question – Stepwise Approach.pdf:application/pdf},
}

@misc{guo_lightrag_2024,
	title = {{LightRAG}: Simple and Fast Retrieval-Augmented Generation},
	doi = {10.48550/arXiv.2410.05779},
	shorttitle = {{LightRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) systems enhance large language models ({LLMs}) by integrating external knowledge sources, enabling more accurate and contextually relevant responses tailored to user needs. However, existing {RAG} systems have significant limitations, including reliance on flat data representations and inadequate contextual awareness, which can lead to fragmented answers that fail to capture complex inter-dependencies. To address these challenges, we propose {LightRAG}, which incorporates graph structures into text indexing and retrieval processes. This innovative framework employs a dual-level retrieval system that enhances comprehensive information retrieval from both low-level and high-level knowledge discovery. Additionally, the integration of graph structures with vector representations facilitates efficient retrieval of related entities and their relationships, significantly improving response times while maintaining contextual relevance. This capability is further enhanced by an incremental update algorithm that ensures the timely integration of new data, allowing the system to remain effective and responsive in rapidly changing data environments. Extensive experimental validation demonstrates considerable improvements in retrieval accuracy and efficiency compared to existing approaches. We have made our {LightRAG} open-source and available at the link: https://github.com/{HKUDS}/{LightRAG}.},
	number = {{arXiv}:2410.05779},
	publisher = {{arXiv}},
	author = {Guo, Zirui and Xia, Lianghao and Yu, Yanhua and Ao, Tu and Huang, Chao},
	date = {2024-11-07},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, finished, no training, not for existing kg, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PHY27SAJ\\Guo et al. - 2024 - LightRAG Simple and Fast Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\UKKNAZS4\\2410.html:text/html},
}

@misc{wang_modern_2022,
	title = {Modern Question Answering Datasets and Benchmarks: A Survey},
	url = {http://arxiv.org/abs/2206.15030},
	doi = {10.48550/arXiv.2206.15030},
	shorttitle = {Modern Question Answering Datasets and Benchmarks},
	abstract = {Question Answering ({QA}) is one of the most important natural language processing ({NLP}) tasks. It aims using {NLP} technologies to generate a corresponding answer to a given question based on the massive unstructured corpus. With the development of deep learning, more and more challenging {QA} datasets are being proposed, and lots of new methods for solving them are also emerging. In this paper, we investigate influential {QA} datasets that have been released in the era of deep learning. Specifically, we begin with introducing two of the most common {QA} tasks - textual question answer and visual question answering - separately, covering the most representative datasets, and then give some current challenges of {QA} research.},
	number = {{arXiv}:2206.15030},
	publisher = {{arXiv}},
	author = {Wang, Zhen},
	urldate = {2024-12-09},
	date = {2022-06-30},
	eprinttype = {arxiv},
	eprint = {2206.15030 [cs]},
	keywords = {notion, Computer Science - Computation and Language, not related},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YN4FWW34\\Wang - 2022 - Modern Question Answering Datasets and Benchmarks A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3Q5LRYHP\\2206.html:text/html},
}

@misc{liang_scemqa_2024,
	title = {{SceMQA}: A Scientific College Entrance Level Multimodal Question Answering Benchmark},
	url = {http://arxiv.org/abs/2402.05138},
	doi = {10.48550/arXiv.2402.05138},
	shorttitle = {{SceMQA}},
	abstract = {The paper introduces {SceMQA}, a novel benchmark for scientific multimodal question answering at the college entrance level. It addresses a critical educational phase often overlooked in existing benchmarks, spanning high school to pre-college levels. {SceMQA} focuses on core science subjects including Mathematics, Physics, Chemistry, and Biology. It features a blend of multiple-choice and free-response formats, ensuring a comprehensive evaluation of {AI} models' abilities. Additionally, our benchmark provides specific knowledge points for each problem and detailed explanations for each answer. {SceMQA} also uniquely presents problems with identical contexts but varied questions to facilitate a more thorough and accurate assessment of reasoning capabilities. In the experiment, we evaluate both open-source and close-source state-of-the-art Multimodal Large Language Models ({MLLMs}), across various experimental settings. The results show that further research and development are needed in developing more capable {MLLM}, as highlighted by only 50\% to 60\% accuracy achieved by the strongest models. Our benchmark and analysis will be available at https://scemqa.github.io/},
	number = {{arXiv}:2402.05138},
	publisher = {{arXiv}},
	author = {Liang, Zhenwen and Guo, Kehan and Liu, Gang and Guo, Taicheng and Zhou, Yujun and Yang, Tianyu and Jiao, Jiajun and Pi, Renjie and Zhang, Jipeng and Zhang, Xiangliang},
	urldate = {2024-12-09},
	date = {2024-02-06},
	eprinttype = {arxiv},
	eprint = {2402.05138 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, not related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GK23ULAL\\Liang et al. - 2024 - SceMQA A Scientific College Entrance Level Multimodal Question Answering Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\M67H6CV5\\2402.html:text/html},
}

@misc{lukovnikov_pretrained_2020,
	title = {Pretrained Transformers for Simple Question Answering over Knowledge Graphs},
	url = {http://arxiv.org/abs/2001.11985},
	doi = {10.48550/arXiv.2001.11985},
	abstract = {Answering simple questions over knowledge graphs is a well-studied problem in question answering. Previous approaches for this task built on recurrent and convolutional neural network based architectures that use pretrained word embeddings. It was recently shown that finetuning pretrained transformer networks (e.g. {BERT}) can outperform previous approaches on various natural language processing tasks. In this work, we investigate how well {BERT} performs on {SimpleQuestions} and provide an evaluation of both {BERT} and {BiLSTM}-based models in datasparse scenarios.},
	number = {{arXiv}:2001.11985},
	publisher = {{arXiv}},
	author = {Lukovnikov, D. and Fischer, A. and Lehmann, J.},
	urldate = {2024-12-09},
	date = {2020-01-31},
	eprinttype = {arxiv},
	eprint = {2001.11985 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\NU7XXCVX\\Lukovnikov et al. - 2020 - Pretrained Transformers for Simple Question Answering over Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\77TN93QU\\2001.html:text/html},
}

@misc{hu_empirical_2023,
	title = {An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2303.10368},
	doi = {10.48550/arXiv.2303.10368},
	abstract = {Large-scale pre-trained language models ({PLMs}) such as {BERT} have recently achieved great success and become a milestone in natural language processing ({NLP}). It is now the consensus of the {NLP} community to adopt {PLMs} as the backbone for downstream tasks. In recent works on knowledge graph question answering ({KGQA}), {BERT} or its variants have become necessary in their {KGQA} models. However, there is still a lack of comprehensive research and comparison of the performance of different {PLMs} in {KGQA}. To this end, we summarize two basic {KGQA} frameworks based on {PLMs} without additional neural network modules to compare the performance of nine {PLMs} in terms of accuracy and efficiency. In addition, we present three benchmarks for larger-scale {KGs} based on the popular {SimpleQuestions} benchmark to investigate the scalability of {PLMs}. We carefully analyze the results of all {PLMs}-based {KGQA} basic frameworks on these benchmarks and two other popular datasets, {WebQuestionSP} and {FreebaseQA}, and find that knowledge distillation techniques and knowledge enhancement methods in {PLMs} are promising for {KGQA}. Furthermore, we test {ChatGPT}, which has drawn a great deal of attention in the {NLP} community, demonstrating its impressive capabilities and limitations in zero-shot {KGQA}. We have released the code and benchmarks to promote the use of {PLMs} on {KGQA}.},
	number = {{arXiv}:2303.10368},
	publisher = {{arXiv}},
	author = {Hu, Nan and Wu, Yike and Qi, Guilin and Min, Dehai and Chen, Jiaoyan and Pan, Jeff Z. and Ali, Zafar},
	urldate = {2024-12-09},
	date = {2023-03-18},
	eprinttype = {arxiv},
	eprint = {2303.10368 [cs]},
	keywords = {notion, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\78W7SJMI\\Hu et al. - 2023 - An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VFEMV3HY\\2303.html:text/html},
}

@inproceedings{luo_bert-based_2020,
	title = {A {BERT}-based Approach with Relation-aware Attention for Knowledge Base Question Answering},
	url = {https://ieeexplore.ieee.org/document/9207186/?arnumber=9207186},
	doi = {10.1109/IJCNN48605.2020.9207186},
	abstract = {Knowledge Base Question Answering ({KBQA}), which uses the facts in the knowledge base ({KB}) to answer natural language questions, has received extensive attention in recent years. The existing works mainly focus on the modeling method and neglect the relations between questions and {KB} facts, which might restrict the further improvements of the performance. To address this problem, this paper proposes a {BERT}-based approach for single-relation question answering ({SR}-{QA}), which consists of two models, entity linking and relation detection. For entity linking, we adopt pre-trained {BERT} and a heuristic algorithm to reduce the noise in the candidate facts. For relation detection, the existing approaches usually model the question and the candidate fact respectively before calculate their semantic similarity, which might lose part of the original interaction information between them. To work around this problem, a {BERT}-based model with relation-aware attention is proposed. We construct the question-fact pair as the input of pre-trained {BERT} to preserves the original interactive information. To bridge the semantic gap between the questions and the {KB} facts, we also use a relation-aware attention network to enhance the representation of candidates. The experimental results show that our entity linking model achieves new state-of-the-art results and our complete approach also achieves state-of-the-art accuracy of 80.9\% on the {SimpleQuestions} dataset.},
	eventtitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	pages = {1--8},
	booktitle = {2020 International Joint Conference on Neural Networks ({IJCNN})},
	author = {Luo, Da and Su, Jindian and Yu, Shanshan},
	urldate = {2024-12-09},
	date = {2020-07},
	note = {{ISSN}: 2161-4407},
	keywords = {notion, Knowledge discovery, attention mechanism, {BERT}, Bit error rate, Deep learning, Feature extraction, Heuristic algorithms, knowledge base question answering, Labeling, Semantics, Task analysis},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\UB5G2C4H\\Luo et al. - 2020 - A BERT-based Approach with Relation-aware Attention for Knowledge Base Question Answering.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\EQHNTJVE\\9207186.html:text/html},
}

@inproceedings{zhang_subgraph_2022,
	title = {Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering},
	url = {http://arxiv.org/abs/2202.13296},
	doi = {10.18653/v1/2022.acl-long.396},
	abstract = {Recent works on knowledge base question answering ({KBQA}) retrieve subgraphs for easier reasoning. A desired subgraph is crucial as a small one may exclude the answer but a large one might introduce more noises. However, the existing retrieval is either heuristic or interwoven with the reasoning, causing reasoning on the partial subgraphs, which increases the reasoning bias when the intermediate supervision is missing. This paper proposes a trainable subgraph retriever ({SR}) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented {KBQA} model. Extensive experiments demonstrate {SR} achieves significantly better retrieval and {QA} performance than existing retrieval methods. Via weakly supervised pre-training as well as the end-to-end fine-tuning, {SRl} achieves new state-of-the-art performance when combined with {NSM}, a subgraph-oriented reasoner, for embedding-based {KBQA} methods.},
	pages = {5773--5784},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	author = {Zhang, Jing and Zhang, Xiaokang and Yu, Jifan and Tang, Jian and Tang, Jie and Li, Cuiping and Chen, Hong},
	urldate = {2024-12-09},
	date = {2022},
	eprinttype = {arxiv},
	eprint = {2202.13296 [cs]},
	keywords = {notion, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\HS53Y36M\\Zhang et al. - 2022 - Subgraph Retrieval Enhanced Model for Multi-hop Knowledge Base Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\MWRFUL99\\2202.html:text/html},
}

@misc{zeng_agenttuning_2023,
	title = {{AgentTuning}: Enabling Generalized Agent Abilities for {LLMs}},
	url = {http://arxiv.org/abs/2310.12823},
	doi = {10.48550/arXiv.2310.12823},
	shorttitle = {{AgentTuning}},
	abstract = {Open large language models ({LLMs}) with great performance in various tasks have significantly advanced the development of {LLMs}. However, they are far inferior to commercial models such as {ChatGPT} and {GPT}-4 when acting as agents to tackle complex tasks in the real world. These agent tasks employ {LLMs} as the central controller responsible for planning, memorization, and tool utilization, necessitating both fine-grained prompting methods and robust {LLMs} to achieve satisfactory performance. Though many prompting methods have been proposed to complete particular agent tasks, there is lack of research focusing on improving the agent capabilities of {LLMs} themselves without compromising their general abilities. In this work, we present {AgentTuning}, a simple and general method to enhance the agent abilities of {LLMs} while maintaining their general {LLM} capabilities. We construct {AgentInstruct}, a lightweight instruction-tuning dataset containing high-quality interaction trajectories. We employ a hybrid instruction-tuning strategy by combining {AgentInstruct} with open-source instructions from general domains. {AgentTuning} is used to instruction-tune the Llama 2 series, resulting in {AgentLM}. Our evaluations show that {AgentTuning} enables {LLMs}' agent capabilities without compromising general abilities. The {AgentLM}-70B is comparable to {GPT}-3.5-turbo on unseen agent tasks, demonstrating generalized agent capabilities. We open source the {AgentInstruct} and {AgentLM}-7B, 13B, and 70B models at https://github.com/{THUDM}/{AgentTuning}, serving open and powerful alternatives to commercial {LLMs} for agent tasks.},
	number = {{arXiv}:2310.12823},
	publisher = {{arXiv}},
	author = {Zeng, Aohan and Liu, Mingdao and Lu, Rui and Wang, Bowen and Liu, Xiao and Dong, Yuxiao and Tang, Jie},
	urldate = {2024-12-09},
	date = {2023-10-22},
	eprinttype = {arxiv},
	eprint = {2310.12823 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\2FKE2I64\\Zeng et al. - 2023 - AgentTuning Enabling Generalized Agent Abilities for LLMs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\UJ4QHPPY\\2310.html:text/html},
}

@misc{liu_knowledge_2024,
	title = {Knowledge Graph-Enhanced Large Language Models via Path Selection},
	url = {http://arxiv.org/abs/2406.13862},
	doi = {10.48550/arXiv.2406.13862},
	abstract = {Large Language Models ({LLMs}) have shown unprecedented performance in various real-world applications. However, they are known to generate factually inaccurate outputs, a.k.a. the hallucination problem. In recent years, incorporating external knowledge extracted from Knowledge Graphs ({KGs}) has become a promising strategy to improve the factual accuracy of {LLM}-generated outputs. Nevertheless, most existing explorations rely on {LLMs} themselves to perform {KG} knowledge extraction, which is highly inflexible as {LLMs} can only provide binary judgment on whether a certain knowledge (e.g., a knowledge path in {KG}) should be used. In addition, {LLMs} tend to pick only knowledge with direct semantic relationship with the input text, while potentially useful knowledge with indirect semantics can be ignored. In this work, we propose a principled framework {KELP} with three stages to handle the above problems. Specifically, {KELP} is able to achieve finer granularity of flexible knowledge extraction by generating scores for knowledge paths with input texts via latent semantic matching. Meanwhile, knowledge paths with indirect semantic relationships with the input text can also be considered via trained encoding between the selected paths in {KG} and the input text. Experiments on real-world datasets validate the effectiveness of {KELP}.},
	number = {{arXiv}:2406.13862},
	publisher = {{arXiv}},
	author = {Liu, Haochen and Wang, Song and Zhu, Yaochen and Dong, Yushun and Li, Jundong},
	urldate = {2024-12-17},
	date = {2024-06-19},
	eprinttype = {arxiv},
	eprint = {2406.13862 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\4J63JSVU\\Liu et al. - 2024 - Knowledge Graph-Enhanced Large Language Models via Path Selection.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2X57HNWH\\2406.html:text/html},
}

@misc{li_decoding_2024,
	title = {Decoding on Graphs: Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Formed Chains},
	url = {http://arxiv.org/abs/2410.18415},
	doi = {10.48550/arXiv.2410.18415},
	shorttitle = {Decoding on Graphs},
	abstract = {Knowledge Graphs ({KGs}) can serve as reliable knowledge sources for question answering ({QA}) due to their structured representation of knowledge. Existing research on the utilization of {KG} for large language models ({LLMs}) prevalently relies on subgraph retriever or iterative prompting, overlooking the potential synergy of {LLMs}' step-wise reasoning capabilities and {KGs}' structural nature. In this paper, we present {DoG} (Decoding on Graphs), a novel framework that facilitates a deep synergy between {LLMs} and {KGs}. We first define a concept, well-formed chain, which consists of a sequence of interrelated fact triplets on the {KGs}, starting from question entities and leading to answers. We argue that this concept can serve as a principle for making faithful and sound reasoning for {KGQA}. To enable {LLMs} to generate well-formed chains, we propose graph-aware constrained decoding, in which a constraint derived from the topology of the {KG} regulates the decoding process of the {LLMs}. This constrained decoding method ensures the generation of well-formed chains while making full use of the step-wise reasoning capabilities of {LLMs}. Based on the above, {DoG}, a training-free approach, is able to provide faithful and sound reasoning trajectories grounded on the {KGs}. Experiments across various {KGQA} tasks with different background {KGs} demonstrate that {DoG} achieves superior and robust performance. {DoG} also shows general applicability with various open-source {LLMs}.},
	number = {{arXiv}:2410.18415},
	publisher = {{arXiv}},
	author = {Li, Kun and Zhang, Tianhua and Wu, Xixin and Luo, Hongyin and Glass, James and Meng, Helen},
	urldate = {2024-12-17},
	date = {2024-10-24},
	eprinttype = {arxiv},
	eprint = {2410.18415 [cs]},
	keywords = {notion, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\LACWM8JH\\Li et al. - 2024 - Decoding on Graphs Faithful and Sound Reasoning on Knowledge Graphs through Generation of Well-Form.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8MBTQ3LE\\2410.html:text/html},
}

@misc{zhang_question-guided_2024,
	title = {Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2410.01401},
	doi = {10.48550/arXiv.2410.01401},
	abstract = {Knowledge graph question answering ({KGQA}) involves answering natural language questions by leveraging structured information stored in a knowledge graph. Typically, {KGQA} initially retrieve a targeted subgraph from a large-scale knowledge graph, which serves as the basis for reasoning models to address queries. However, the retrieved subgraph inevitably brings distraction information for knowledge utilization, impeding the model's ability to perform accurate reasoning. To address this issue, we propose a Question-guided Knowledge Graph Re-scoring method (Q-{KGR}) to eliminate noisy pathways for the input question, thereby focusing specifically on pertinent factual knowledge. Moreover, we introduce Knowformer, a parameter-efficient method for injecting the re-scored knowledge graph into large language models to enhance their ability to perform factual reasoning. Extensive experiments on multiple {KGQA} benchmarks demonstrate the superiority of our method over existing systems.},
	number = {{arXiv}:2410.01401},
	publisher = {{arXiv}},
	author = {Zhang, Yu and Chen, Kehai and Bai, Xuefeng and kang, zhao and Guo, Quanjiang and Zhang, Min},
	urldate = {2024-12-17},
	date = {2024-10-02},
	eprinttype = {arxiv},
	eprint = {2410.01401 [cs]},
	keywords = {notion, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\SYCUU9AB\\Zhang et al. - 2024 - Question-guided Knowledge Graph Re-scoring and Injection for Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\PKKJ5PU9\\2410.html:text/html},
}

@misc{ma_think--graph_2024,
	title = {Think-on-Graph 2.0: Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval Augmented Generation},
	doi = {10.48550/arXiv.2407.10805},
	shorttitle = {Think-on-Graph 2.0},
	abstract = {Retrieval-augmented generation ({RAG}) has improved large language models ({LLMs}) by using knowledge retrieval to overcome knowledge deficiencies. However, current {RAG} methods often fall short of ensuring the depth and completeness of retrieved information, which is necessary for complex reasoning tasks. In this work, we introduce Think-on-Graph 2.0 ({ToG}-2), a hybrid {RAG} framework that iteratively retrieves information from both unstructured and structured knowledge sources in a tight-coupling manner. Specifically, {ToG}-2 leverages knowledge graphs ({KGs}) to link documents via entities, facilitating deep and knowledge-guided context retrieval. Simultaneously, it utilizes documents as entity contexts to achieve precise and efficient graph retrieval. {ToG}-2 alternates between graph retrieval and context retrieval to search for in-depth clues relevant to the question, enabling {LLMs} to generate answers. We conduct a series of well-designed experiments to highlight the following advantages of {ToG}-2: 1) {ToG}-2 tightly couples the processes of context retrieval and graph retrieval, deepening context retrieval via the {KG} while enabling reliable graph retrieval based on contexts; 2) it achieves deep and faithful reasoning in {LLMs} through an iterative knowledge retrieval process of collaboration between contexts and the {KG}; and 3) {ToG}-2 is training-free and plug-and-play compatible with various {LLMs}. Extensive experiments demonstrate that {ToG}-2 achieves overall state-of-the-art ({SOTA}) performance on 6 out of 7 knowledge-intensive datasets with {GPT}-3.5, and can elevate the performance of smaller models (e.g., {LLAMA}-2-13B) to the level of {GPT}-3.5's direct reasoning. The source code is available on https://github.com/{IDEA}-{FinAI}/{ToG}-2.},
	number = {{arXiv}:2407.10805},
	publisher = {{arXiv}},
	author = {Ma, Shengjie and Xu, Chengjin and Jiang, Xuhui and Li, Muzhi and Qu, Huaren and Yang, Cehao and Mao, Jiaxin and Guo, Jian},
	date = {2024-12-09},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\AS7NP69B\\Ma et al. - 2024 - Think-on-Graph 2.0 Deep and Faithful Large Language Model Reasoning with Knowledge-guided Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\BP9H4JV9\\2407.html:text/html},
}

@misc{wang_keqing_2023,
	title = {keqing: knowledge-based question answering is a nature chain-of-thought mentor of {LLM}},
	url = {http://arxiv.org/abs/2401.00426},
	doi = {10.48550/arXiv.2401.00426},
	shorttitle = {keqing},
	abstract = {Large language models ({LLMs}) have exhibited remarkable performance on various natural language processing ({NLP}) tasks, especially for question answering. However, in the face of problems beyond the scope of knowledge, these {LLMs} tend to talk nonsense with a straight face, where the potential solution could be incorporating an Information Retrieval ({IR}) module and generating response based on these retrieved knowledge. In this paper, we present a novel framework to assist {LLMs}, such as {ChatGPT}, to retrieve question-related structured information on the knowledge graph, and demonstrate that Knowledge-based question answering (Keqing) could be a nature Chain-of-Thought ({CoT}) mentor to guide the {LLM} to sequentially find the answer entities of a complex question through interpretable logical chains. Specifically, the workflow of Keqing will execute decomposing a complex question according to predefined templates, retrieving candidate entities on knowledge graph, reasoning answers of sub-questions, and finally generating response with reasoning paths, which greatly improves the reliability of {LLM}'s response. The experimental results on {KBQA} datasets show that Keqing can achieve competitive performance and illustrate the logic of answering each question.},
	number = {{arXiv}:2401.00426},
	publisher = {{arXiv}},
	author = {Wang, Chaojie and Xu, Yishi and Peng, Zhong and Zhang, Chenxi and Chen, Bo and Wang, Xinrun and Feng, Lei and An, Bo},
	urldate = {2024-12-17},
	date = {2023-12-31},
	eprinttype = {arxiv},
	eprint = {2401.00426 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\KZN9SQIM\\Wang et al. - 2023 - keqing knowledge-based question answering is a nature chain-of-thought mentor of LLM.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\TM29MTIJ\\2401.html:text/html},
}

@misc{mavromatis_gnn-rag_2024,
	title = {{GNN}-{RAG}: Graph Neural Retrieval for Large Language Model Reasoning},
	url = {http://arxiv.org/abs/2405.20139},
	doi = {10.48550/arXiv.2405.20139},
	shorttitle = {{GNN}-{RAG}},
	abstract = {Knowledge Graphs ({KGs}) represent human-crafted factual knowledge in the form of triplets (head, relation, tail), which collectively form a graph. Question Answering over {KGs} ({KGQA}) is the task of answering natural questions grounding the reasoning to the information provided by the {KG}. Large Language Models ({LLMs}) are the state-of-the-art models for {QA} tasks due to their remarkable ability to understand natural language. On the other hand, Graph Neural Networks ({GNNs}) have been widely used for {KGQA} as they can handle the complex graph information stored in the {KG}. In this work, we introduce {GNN}-{RAG}, a novel method for combining language understanding abilities of {LLMs} with the reasoning abilities of {GNNs} in a retrieval-augmented generation ({RAG}) style. First, a {GNN} reasons over a dense {KG} subgraph to retrieve answer candidates for a given question. Second, the shortest paths in the {KG} that connect question entities and answer candidates are extracted to represent {KG} reasoning paths. The extracted paths are verbalized and given as input for {LLM} reasoning with {RAG}. In our {GNN}-{RAG} framework, the {GNN} acts as a dense subgraph reasoner to extract useful graph information, while the {LLM} leverages its natural language processing ability for ultimate {KGQA}. Furthermore, we develop a retrieval augmentation ({RA}) technique to further boost {KGQA} performance with {GNN}-{RAG}. Experimental results show that {GNN}-{RAG} achieves state-of-the-art performance in two widely used {KGQA} benchmarks ({WebQSP} and {CWQ}), outperforming or matching {GPT}-4 performance with a 7B tuned {LLM}. In addition, {GNN}-{RAG} excels on multi-hop and multi-entity questions outperforming competing approaches by 8.9--15.5\% points at answer F1.},
	number = {{arXiv}:2405.20139},
	publisher = {{arXiv}},
	author = {Mavromatis, Costas and Karypis, George},
	urldate = {2024-12-17},
	date = {2024-05-30},
	eprinttype = {arxiv},
	eprint = {2405.20139 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\IURF995D\\Mavromatis and Karypis - 2024 - GNN-RAG Graph Neural Retrieval for Large Language Model Reasoning.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\IZ9Z7JL8\\2405.html:text/html},
}

@misc{jin_graph_2024,
	title = {Graph Chain-of-Thought: Augmenting Large Language Models by Reasoning on Graphs},
	doi = {10.48550/arXiv.2404.07103},
	shorttitle = {Graph Chain-of-Thought},
	abstract = {Large language models ({LLMs}), while exhibiting exceptional performance, suffer from hallucinations, especially on knowledge-intensive tasks. Existing works propose to augment {LLMs} with individual text units retrieved from external knowledge corpora to alleviate the issue. However, in many domains, texts are interconnected (e.g., academic papers in a bibliographic graph are linked by citations and co-authorships) which form a (text-attributed) graph. The knowledge in such graphs is encoded not only in single texts/nodes but also in their associated connections. To facilitate the research of augmenting {LLMs} with graphs, we manually construct a Graph Reasoning Benchmark dataset called {GRBench}, containing 1,740 questions that can be answered with the knowledge from 10 domain graphs. Then, we propose a simple and effective framework called Graph Chain-of-thought (Graph-{CoT}) to augment {LLMs} with graphs by encouraging {LLMs} to reason on the graph iteratively. Each Graph-{CoT} iteration consists of three sub-steps: {LLM} reasoning, {LLM}-graph interaction, and graph execution. We conduct systematic experiments with three {LLM} backbones on {GRBench}, where Graph-{CoT} outperforms the baselines consistently. The code is available at https://github.com/{PeterGriffinJin}/Graph-{CoT}.},
	number = {{arXiv}:2404.07103},
	publisher = {{arXiv}},
	author = {Jin, Bowen and Xie, Chulin and Zhang, Jiawei and Roy, Kashob Kumar and Zhang, Yu and Li, Zheng and Li, Ruirui and Tang, Xianfeng and Wang, Suhang and Meng, Yu and Han, Jiawei},
	date = {2024-10-03},
	eprinttype = {arxiv},
	eprint = {2404.07103 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Machine Learning, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\58YAHLPI\\Jin et al. - 2024 - Graph Chain-of-Thought Augmenting Large Language Models by Reasoning on Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9HSNZRHY\\2404.html:text/html},
}

@software{noauthor_zjukgkg-llm-papers_2024,
	title = {zjukg/{KG}-{LLM}-Papers},
	rights = {{MIT}},
	url = {https://github.com/zjukg/KG-LLM-Papers},
	abstract = {[Paper List] Papers integrating knowledge graphs ({KGs}) and large language models ({LLMs})},
	publisher = {{ZJUKG}},
	urldate = {2024-12-17},
	date = {2024-12-17},
	note = {original-date: 2023-06-12T03:26:14Z},
	keywords = {notion, awesome, awesome-kg, awesome-llm, awsome-list, commonsense, gpt, knowledge, knowledge-graph, language-models, large-language-models, llm, nlp, paper-list, prompt, survey},
}

@misc{peng_graph_2024,
	title = {Graph Retrieval-Augmented Generation: A Survey},
	doi = {10.48550/arXiv.2408.08921},
	shorttitle = {Graph Retrieval-Augmented Generation},
	abstract = {Recently, Retrieval-Augmented Generation ({RAG}) has achieved remarkable success in addressing the challenges of Large Language Models ({LLMs}) without necessitating retraining. By referencing an external knowledge base, {RAG} refines {LLM} outputs, effectively mitigating issues such as ``hallucination'', lack of domain-specific knowledge, and outdated information. However, the complex structure of relationships among different entities in databases presents challenges for {RAG} systems. In response, {GraphRAG} leverages structural information across entities to enable more precise and comprehensive retrieval, capturing relational knowledge and facilitating more accurate, context-aware responses. Given the novelty and potential of {GraphRAG}, a systematic review of current technologies is imperative. This paper provides the first comprehensive overview of {GraphRAG} methodologies. We formalize the {GraphRAG} workflow, encompassing Graph-Based Indexing, Graph-Guided Retrieval, and Graph-Enhanced Generation. We then outline the core technologies and training methods at each stage. Additionally, we examine downstream tasks, application domains, evaluation methodologies, and industrial use cases of {GraphRAG}. Finally, we explore future research directions to inspire further inquiries and advance progress in the field. In order to track recent progress in this field, we set up a repository at {\textbackslash}url\{https://github.com/pengboci/{GraphRAG}-Survey\}.},
	number = {{arXiv}:2408.08921},
	publisher = {{arXiv}},
	author = {Peng, Boci and Zhu, Yun and Liu, Yongchao and Bo, Xiaohe and Shi, Haizhou and Hong, Chuntao and Zhang, Yan and Tang, Siliang},
	date = {2024-09-10},
	eprinttype = {arxiv},
	eprint = {2408.08921 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5IFFSVZR\\Peng et al. - 2024 - Graph Retrieval-Augmented Generation A Survey.pdf:application/pdf;Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GJ3MELB7\\Peng et al. - 2024 - Graph Retrieval-Augmented Generation A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\RBU4WCSY\\2408.html:text/html},
}

@article{guo_knowledgenavigator_2024,
	title = {{KnowledgeNavigator}: Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph},
	volume = {10},
	issn = {2199-4536, 2198-6053},
	url = {http://arxiv.org/abs/2312.15880},
	doi = {10.1007/s40747-024-01527-8},
	shorttitle = {{KnowledgeNavigator}},
	abstract = {Large language model ({LLM}) has achieved outstanding performance on various downstream tasks with its powerful natural language understanding and zero-shot capability, but {LLM} still suffers from knowledge limitation. Especially in scenarios that require long logical chains or complex reasoning, the hallucination and knowledge limitation of {LLM} limit its performance in question answering ({QA}). In this paper, we propose a novel framework {KnowledgeNavigator} to address these challenges by efficiently and accurately retrieving external knowledge from knowledge graph and using it as a key factor to enhance {LLM} reasoning. Specifically, {KnowledgeNavigator} first mines and enhances the potential constraints of the given question to guide the reasoning. Then it retrieves and filters external knowledge that supports answering through iterative reasoning on knowledge graph with the guidance of {LLM} and the question. Finally, {KnowledgeNavigator} constructs the structured knowledge into effective prompts that are friendly to {LLM} to help its reasoning. We evaluate {KnowledgeNavigator} on multiple public {KGQA} benchmarks, the experiments show the framework has great effectiveness and generalization, outperforming previous knowledge graph enhanced {LLM} methods and is comparable to the fully supervised models.},
	pages = {7063--7076},
	number = {5},
	journaltitle = {Complex \& Intelligent Systems},
	shortjournal = {Complex Intell. Syst.},
	author = {Guo, Tiezheng and Yang, Qingwen and Wang, Chen and Liu, Yanyi and Li, Pan and Tang, Jiawei and Li, Dapeng and Wen, Yingyou},
	urldate = {2024-12-20},
	date = {2024-10},
	eprinttype = {arxiv},
	eprint = {2312.15880 [cs]},
	keywords = {notion, Computer Science - Computation and Language, training-based},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\RN8BR39X\\Guo et al. - 2024 - KnowledgeNavigator Leveraging Large Language Models for Enhanced Reasoning over Knowledge Graph.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ELCJVQW6\\2312.html:text/html},
}

@misc{wu_retrieval-augmented_2024,
	title = {Retrieval-Augmented Generation for Natural Language Processing: A Survey},
	doi = {10.48550/arXiv.2407.13193},
	shorttitle = {Retrieval-Augmented Generation for Natural Language Processing},
	abstract = {Large language models ({LLMs}) have demonstrated great success in various fields, benefiting from their huge amount of parameters that store knowledge. However, {LLMs} still suffer from several key issues, such as hallucination problems, knowledge update issues, and lacking domain-specific expertise. The appearance of retrieval-augmented generation ({RAG}), which leverages an external knowledge database to augment {LLMs}, makes up those drawbacks of {LLMs}. This paper reviews all significant techniques of {RAG}, especially in the retriever and the retrieval fusions. Besides, tutorial codes are provided for implementing the representative techniques in {RAG}. This paper further discusses the {RAG} training, including {RAG} with/without datastore update. Then, we introduce the application of {RAG} in representative natural language processing tasks and industrial scenarios. Finally, this paper discusses the future directions and challenges of {RAG} for promoting its development.},
	number = {{arXiv}:2407.13193},
	publisher = {{arXiv}},
	author = {Wu, Shangyu and Xiong, Ying and Cui, Yufei and Wu, Haolun and Chen, Can and Yuan, Ye and Huang, Lianming and Liu, Xue and Kuo, Tei-Wei and Guan, Nan and Xue, Chun Jason},
	urldate = {2024-12-23},
	date = {2024-07-19},
	eprinttype = {arxiv},
	eprint = {2407.13193 [cs]},
	keywords = {Computer Science - Computation and Language, notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YVTHKVGJ\\Wu et al. - 2024 - Retrieval-Augmented Generation for Natural Language Processing A Survey.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\LP57Z7SM\\2407.html:text/html},
}

@misc{gupta_comprehensive_2024,
	title = {A Comprehensive Survey of Retrieval-Augmented Generation ({RAG}): Evolution, Current Landscape and Future Directions},
	url = {http://arxiv.org/abs/2410.12837},
	doi = {10.48550/arXiv.2410.12837},
	shorttitle = {A Comprehensive Survey of Retrieval-Augmented Generation ({RAG})},
	abstract = {This paper presents a comprehensive study of Retrieval-Augmented Generation ({RAG}), tracing its evolution from foundational concepts to the current state of the art. {RAG} combines retrieval mechanisms with generative language models to enhance the accuracy of outputs, addressing key limitations of {LLMs}. The study explores the basic architecture of {RAG}, focusing on how retrieval and generation are integrated to handle knowledge-intensive tasks. A detailed review of the significant technological advancements in {RAG} is provided, including key innovations in retrieval-augmented language models and applications across various domains such as question-answering, summarization, and knowledge-based tasks. Recent research breakthroughs are discussed, highlighting novel methods for improving retrieval efficiency. Furthermore, the paper examines ongoing challenges such as scalability, bias, and ethical concerns in deployment. Future research directions are proposed, focusing on improving the robustness of {RAG} models, expanding the scope of application of {RAG} models, and addressing societal implications. This survey aims to serve as a foundational resource for researchers and practitioners in understanding the potential of {RAG} and its trajectory in natural language processing.},
	number = {{arXiv}:2410.12837},
	publisher = {{arXiv}},
	author = {Gupta, Shailja and Ranjan, Rajesh and Singh, Surya Narayan},
	urldate = {2024-12-23},
	date = {2024-10-03},
	eprinttype = {arxiv},
	eprint = {2410.12837 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RWKXHHYS\\Gupta et al. - 2024 - A Comprehensive Survey of Retrieval-Augmented Generation (RAG) Evolution, Current Landscape and Fut.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\U9FMHT8V\\2410.html:text/html},
}

@misc{wang_domainrag_2024,
	title = {{DomainRAG}: A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2406.05654},
	doi = {10.48550/arXiv.2406.05654},
	shorttitle = {{DomainRAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) offers a promising solution to address various limitations of Large Language Models ({LLMs}), such as hallucination and difficulties in keeping up with real-time updates. This approach is particularly critical in expert and domain-specific applications where {LLMs} struggle to cover expert knowledge. Therefore, evaluating {RAG} models in such scenarios is crucial, yet current studies often rely on general knowledge sources like Wikipedia to assess the models' abilities in solving common-sense problems. In this paper, we evaluated {LLMs} by {RAG} settings in a domain-specific context, college enrollment. We identified six required abilities for {RAG} models, including the ability in conversational {RAG}, analyzing structural information, faithfulness to external knowledge, denoising, solving time-sensitive problems, and understanding multi-document interactions. Each ability has an associated dataset with shared corpora to evaluate the {RAG} models' performance. We evaluated popular {LLMs} such as Llama, Baichuan, {ChatGLM}, and {GPT} models. Experimental results indicate that existing closed-book {LLMs} struggle with domain-specific questions, highlighting the need for {RAG} models to solve expert problems. Moreover, there is room for {RAG} models to improve their abilities in comprehending conversational history, analyzing structural information, denoising, processing multi-document interactions, and faithfulness in expert knowledge. We expect future studies could solve these problems better.},
	number = {{arXiv}:2406.05654},
	publisher = {{arXiv}},
	author = {Wang, Shuting and Liu, Jiongnan and Song, Shiren and Cheng, Jiehan and Fu, Yuqi and Guo, Peidong and Fang, Kun and Zhu, Yutao and Dou, Zhicheng},
	urldate = {2024-12-23},
	date = {2024-06-17},
	eprinttype = {arxiv},
	eprint = {2406.05654 [cs]},
	keywords = {notion, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\R5KIHLS4\\Wang et al. - 2024 - DomainRAG A Chinese Benchmark for Evaluating Domain-specific Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NVVMYBVS\\2406.html:text/html},
}

@misc{hu_rag_2024,
	title = {{RAG} and {RAU}: A Survey on Retrieval-Augmented Language Model in Natural Language Processing},
	url = {http://arxiv.org/abs/2404.19543},
	doi = {10.48550/arXiv.2404.19543},
	shorttitle = {{RAG} and {RAU}},
	abstract = {Large Language Models ({LLMs}) have catalyzed significant advancements in Natural Language Processing ({NLP}), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with {LLMs}, substantially enhancing their performance across {NLP} tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models ({RALMs}), both Retrieval-Augmented Generation ({RAG}) and Retrieval-Augmented Understanding ({RAU}), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of {RALMs}, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. {RALMs} demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of {RALMs}, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of {RALMs}, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into {RALMs}, their potential, and the avenues for their future development in {NLP}. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/{RALM}\_Survey.},
	number = {{arXiv}:2404.19543},
	publisher = {{arXiv}},
	author = {Hu, Yucheng and Lu, Yuxing},
	urldate = {2024-12-23},
	date = {2024-04-30},
	eprinttype = {arxiv},
	eprint = {2404.19543 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\C7CDXIS9\\Hu und Lu - 2024 - RAG and RAU A Survey on Retrieval-Augmented Language Model in Natural Language Processing.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\25XGCBW5\\2404.html:text/html},
}

@inproceedings{hahner_classification_2023,
	location = {Cham},
	title = {A Classification of Software-Architectural Uncertainty Regarding Confidentiality},
	isbn = {978-3-031-36840-0},
	doi = {10.1007/978-3-031-36840-0_8},
	abstract = {In our connected world, ensuring and demonstrating the confidentiality of exchanged data becomes increasingly critical for software systems. However, especially in early system design, uncertainty exists about the software architecture itself and the software’s execution environment. This does not only impede early confidentiality analysis but can also cause data breaches due to the lack of awareness of the impact of uncertainty. Classifying uncertainty helps in understanding its impact and in choosing proper analysis and mitigation strategies. There already exist multiple taxonomies, e.g., from the domain of self-adaptive systems. However, they do not fit the abstraction of software architecture and do not focus on security-related quality properties like confidentiality.},
	pages = {139--160},
	booktitle = {E-Business and Telecommunications},
	publisher = {Springer Nature Switzerland},
	author = {Hahner, Sebastian and Seifermann, Stephan and Heinrich, Robert and Reussner, Ralf},
	editor = {Samarati, Pierangela and van Sinderen, Marten and Vimercati, Sabrina De Capitani di and Wijnhoven, Fons},
	date = {2023},
	langid = {english},
	keywords = {notion, Confidentiality, Software architecture, Uncertainty},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\9MLKIRPH\\Hahner et al. - 2023 - A Classification of Software-Architectural Uncertainty Regarding Confidentiality.pdf:application/pdf},
}

@inproceedings{procko_graph_2024,
	title = {Graph Retrieval-Augmented Generation for Large Language Models: A Survey},
	doi = {10.1109/AIxSET62544.2024.00030},
	shorttitle = {Graph Retrieval-Augmented Generation for Large Language Models},
	abstract = {Large Language Models ({LLMs}) demonstrate general knowledge, but they suffer when specifically needed knowledge is not present in their training set. Two approaches to ameliorating this, without retraining, are 1) prompt engineering and 2) Retrieval-Augmented Generation ({RAG}). {RAG} is a form of prompt engineering, insofar as relevant lexical snippets retrieved from {RAG} corpora are vectorized and aggregated with prompts. However, {RAG} documents are often noisy, i.e., while relevant to a given prompt, they can contain much other information that obfuscates the desired snippet. If the purpose of pretraining a {LLM} on massive and general corpora is to engender a generally applicable model, {RAG} is not: it is a means of {LLM} optimization, and as such, {RAG} document selection must be precise, not general. For expert tasks, it is imperative that a {RAG} corpus be as noise-free as possible, in much the same way a good prompt should be free of irrelevant text. Knowledge Graphs ({KGs}) provide a concise means of representing domain knowledge free of noisy information. This paper surveys work incorporating {KGs} with {LLM} {RAG}, intending to equip scientists with a better understanding of this novel research area for future work.},
	eventtitle = {2024 Conference on {AI}, Science, Engineering, and Technology ({AIxSET})},
	pages = {166--169},
	booktitle = {2024 Conference on {AI}, Science, Engineering, and Technology ({AIxSET})},
	author = {Procko, Tyler Thomas and Ochoa, Omar},
	urldate = {2025-01-07},
	date = {2024-09},
	keywords = {fine-tuning, {GPT}, knowledge graphs, Knowledge graphs, Large language models, {LLM}, Noise, Noise measurement, notion, Optimization, Prompt engineering, {RAG}, Reliability, Surveys, Training, Uncertainty},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3JU2BUU4\\Procko und Ochoa - 2024 - Graph Retrieval-Augmented Generation for Large Language Models A Survey.pdf:application/pdf},
}

@misc{li_simple_2024,
	title = {Simple is Effective: The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieval-Augmented Generation},
	url = {http://arxiv.org/abs/2410.20724},
	doi = {10.48550/arXiv.2410.20724},
	shorttitle = {Simple is Effective},
	abstract = {Large Language Models ({LLMs}) demonstrate strong reasoning abilities but face limitations such as hallucinations and outdated knowledge. Knowledge Graph ({KG})-based Retrieval-Augmented Generation ({RAG}) addresses these issues by grounding {LLM} outputs in structured external knowledge from {KGs}. However, current {KG}-based {RAG} frameworks still struggle to optimize the trade-off between retrieval effectiveness and efficiency in identifying a suitable amount of relevant graph information for the {LLM} to digest. We introduce {SubgraphRAG}, extending the {KG}-based {RAG} framework that retrieves subgraphs and leverages {LLMs} for reasoning and answer prediction. Our approach innovatively integrates a lightweight multilayer perceptron with a parallel triple-scoring mechanism for efficient and flexible subgraph retrieval while encoding directional structural distances to enhance retrieval effectiveness. The size of retrieved subgraphs can be flexibly adjusted to match the query's need and the downstream {LLM}'s capabilities. This design strikes a balance between model complexity and reasoning power, enabling scalable and generalizable retrieval processes. Notably, based on our retrieved subgraphs, smaller {LLMs} like Llama3.1-8B-Instruct deliver competitive results with explainable reasoning, while larger models like {GPT}-4o achieve state-of-the-art accuracy compared with previous baselines -- all without fine-tuning. Extensive evaluations on the {WebQSP} and {CWQ} benchmarks highlight {SubgraphRAG}'s strengths in efficiency, accuracy, and reliability by reducing hallucinations and improving response grounding.},
	number = {{arXiv}:2410.20724},
	publisher = {{arXiv}},
	author = {Li, Mufei and Miao, Siqi and Li, Pan},
	urldate = {2025-01-07},
	date = {2024-11-11},
	eprinttype = {arxiv},
	eprint = {2410.20724 [cs]},
	keywords = {notion, Computer Science - Machine Learning, Computer Science - Computation and Language, needs training},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZDCUNA58\\Li et al. - 2024 - Simple is Effective The Roles of Graphs and Large Language Models in Knowledge-Graph-Based Retrieva.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\5GXPJGEE\\2410.html:text/html},
}

@misc{white_prompt_2023,
	title = {A Prompt Pattern Catalog to Enhance Prompt Engineering with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models ({LLMs}), such as {ChatGPT}. Prompts are instructions given to an {LLM} to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an {LLM}. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with {LLMs}. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with {LLMs}. This paper provides the following contributions to research on prompt engineering that apply {LLMs} to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of {LLM} conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	number = {{arXiv}:2302.11382},
	publisher = {{arXiv}},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	urldate = {2025-01-08},
	date = {2023-02-21},
	eprinttype = {arxiv},
	eprint = {2302.11382 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\4JXWWVXA\\White et al. - 2023 - A Prompt Pattern Catalog to Enhance Prompt Engineering with ChatGPT.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NCEYJXZY\\2302.html:text/html},
}

@misc{taye_understanding_2010,
	title = {Understanding Semantic Web and Ontologies: Theory and Applications},
	url = {http://arxiv.org/abs/1006.4567},
	doi = {10.48550/arXiv.1006.4567},
	shorttitle = {Understanding Semantic Web and Ontologies},
	abstract = {Semantic Web is actually an extension of the current one in that it represents information more meaningfully for humans and computers alike. It enables the description of contents and services in machine-readable form, and enables annotating, discovering, publishing, advertising and composing services to be automated. It was developed based on Ontology, which is considered as the backbone of the Semantic Web. In other words, the current Web is transformed from being machine-readable to machine-understandable. In fact, Ontology is a key technique with which to annotate semantics and provide a common, comprehensible foundation for resources on the Semantic Web. Moreover, Ontology can provide a common vocabulary, a grammar for publishing data, and can supply a semantic description of data which can be used to preserve the Ontologies and keep them ready for inference. This paper provides basic concepts of web services and the Semantic Web, defines the structure and the main applications of ontology, and provides many relevant terms are explained in order to provide a basic understanding of ontologies.},
	number = {{arXiv}:1006.4567},
	publisher = {{arXiv}},
	author = {Taye, Mohammad Mustafa},
	urldate = {2025-01-08},
	date = {2010-06-23},
	eprinttype = {arxiv},
	eprint = {1006.4567 [cs]},
	keywords = {notion, Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\T3H9VBZ5\\Taye - 2010 - Understanding Semantic Web and Ontologies Theory and Applications.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\CMJQ2DHM\\1006.html:text/html},
}

@online{wood_rdf_2014,
	title = {{RDF} 1.1 Concepts and Abstract Syntax},
	url = {http://www.w3.org/TR/2014/REC-rdf11-concepts-20140225/},
	type = {W3C Recommendation},
	author = {Wood, David and Lanthaler, Markus and Cyganiak, Richard},
	urldate = {2015-03-15},
	date = {2014-02},
	note = {Publisher: W3C},
	keywords = {notion, {RDF} ontologies semantic-web},
}

@article{paulheim_knowledge_2016,
	title = {Knowledge graph refinement: A survey of approaches and evaluation methods},
	volume = {8},
	issn = {22104968, 15700844},
	url = {https://journals.sagepub.com/doi/full/10.3233/SW-160218},
	doi = {10.3233/SW-160218},
	shorttitle = {Knowledge graph refinement},
	abstract = {In the recent years, different Web knowledge graphs, both free and commercial, have been created. While Google coined the term “Knowledge Graph” in 2012, there are also a few openly available knowledge graphs, with {DBpedia}, {YAGO}, and Freebase being among the most prominent ones. Those graphs are often constructed from semi-structured knowledge, such as Wikipedia, or harvested from the web with a combination of statistical and linguistic methods. The result are large-scale knowledge graphs that try to make a good trade-off between completeness and correctness. In order to further increase the utility of such knowledge graphs, various reﬁnement methods have been proposed, which try to infer and add missing knowledge to the graph, or identify erroneous pieces of information. In this article, we provide a survey of such knowledge graph reﬁnement approaches, with a dual look at both the methods being proposed as well as the evaluation methodologies used.},
	pages = {489--508},
	number = {3},
	journaltitle = {Semantic Web},
	shortjournal = {{SW}},
	author = {Paulheim, Heiko},
	editor = {Cimiano, Philipp},
	urldate = {2025-01-08},
	date = {2016-12-06},
	langid = {english},
	keywords = {notion},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\TATFDL5Z\\Paulheim - 2016 - Knowledge graph refinement A survey of approaches and evaluation methods.pdf:application/pdf},
}

@article{hogan_knowledge_2022,
	title = {Knowledge Graphs},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	doi = {10.1145/3447772},
	abstract = {In this paper we provide a comprehensive introduction to knowledge graphs, which have recently garnered significant attention from both industry and academia in scenarios that require exploiting diverse, dynamic, large-scale collections of data. After some opening remarks, we motivate and contrast various graph-based data models and query languages that are used for knowledge graphs. We discuss the roles of schema, identity, and context in knowledge graphs. We explain how knowledge can be represented and extracted using a combination of deductive and inductive techniques. We summarise methods for the creation, enrichment, quality assessment, refinement, and publication of knowledge graphs. We provide an overview of prominent open knowledge graphs and enterprise knowledge graphs, their applications, and how they use the aforementioned techniques. We conclude with high-level future research directions for knowledge graphs.},
	pages = {1--37},
	number = {4},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Hogan, Aidan and Blomqvist, Eva and Cochez, Michael and d'Amato, Claudia and Melo, Gerard de and Gutierrez, Claudio and Gayo, José Emilio Labra and Kirrane, Sabrina and Neumaier, Sebastian and Polleres, Axel and Navigli, Roberto and Ngomo, Axel-Cyrille Ngonga and Rashid, Sabbir M. and Rula, Anisa and Schmelzeisen, Lukas and Sequeda, Juan and Staab, Steffen and Zimmermann, Antoine},
	date = {2022-05-31},
	eprinttype = {arxiv},
	eprint = {2003.02320 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning, notion},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\INMDTWHV\\Hogan et al. - 2022 - Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\YHNJIX77\\2003.html:text/html},
}

@misc{vahdati_unveiling_2018,
	title = {Unveiling Scholarly Communities over Knowledge Graphs},
	url = {http://arxiv.org/abs/1807.06816},
	doi = {10.48550/arXiv.1807.06816},
	abstract = {Knowledge graphs represent the meaning of properties of real-world entities and relationships among them in a natural way. Exploiting semantics encoded in knowledge graphs enables the implementation of knowledge-driven tasks such as semantic retrieval, query processing, and question answering, as well as solutions to knowledge discovery tasks including pattern discovery and link prediction. In this paper, we tackle the problem of knowledge discovery in scholarly knowledge graphs, i.e., graphs that integrate scholarly data, and present Korona, a knowledge-driven framework able to unveil scholarly communities for the prediction of scholarly networks. Korona implements a graph partition approach and relies on semantic similarity measures to determine relatedness between scholarly entities. As a proof of concept, we built a scholarly knowledge graph with data from researchers, conferences, and papers of the Semantic Web area, and apply Korona to uncover co-authorship networks. Results observed from our empirical evaluation suggest that exploiting semantics in scholarly knowledge graphs enables the identification of previously unknown relations between researchers. By extending the ontology, these observations can be generalized to other scholarly entities, e.g., articles or institutions, for the prediction of other scholarly patterns, e.g., co-citations or academic collaboration.},
	number = {{arXiv}:1807.06816},
	publisher = {{arXiv}},
	author = {Vahdati, Sahar and Palma, Guillermo and Nath, Rahul Jyoti and Lange, Christoph and Auer, Sören and Vidal, Maria-Esther},
	urldate = {2025-01-08},
	date = {2018-07-18},
	eprinttype = {arxiv},
	eprint = {1807.06816 [cs]},
	keywords = {notion, Computer Science - Digital Libraries, Physics - Physics and Society},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\5C6LVAU3\\Vahdati et al. - 2018 - Unveiling Scholarly Communities over Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\5J3VH8TT\\1807.html:text/html},
}

@article{runeson_guidelines_2009,
	title = {Guidelines for conducting and reporting case study research in software engineering},
	volume = {14},
	issn = {1573-7616},
	doi = {10.1007/s10664-008-9102-8},
	abstract = {Case study is a suitable research methodology for software engineering research since it studies contemporary phenomena in its natural context. However, the understanding of what constitutes a case study varies, and hence the quality of the resulting studies. This paper aims at providing an introduction to case study methodology and guidelines for researchers conducting case studies and readers studying reports of such studies. The content is based on the authors’ own experience from conducting and reading case studies. The terminology and guidelines are compiled from different methodology handbooks in other research domains, in particular social science and information systems, and adapted to the needs in software engineering. We present recommended practices for software engineering case studies as well as empirically derived and evaluated checklists for researchers and readers of case study research.},
	pages = {131--164},
	number = {2},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Runeson, Per and Höst, Martin},
	date = {2009-04-01},
	langid = {english},
	keywords = {Case study, Checklists, Guidelines, notion, Research methodology},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\FY7V2K7W\\Runeson und Höst - 2009 - Guidelines for conducting and reporting case study research in software engineering.pdf:application/pdf},
}

@incollection{vom_brocke_introduction_2020,
	location = {Cham},
	title = {Introduction to Design Science Research},
	isbn = {978-3-030-46781-4},
	url = {https://doi.org/10.1007/978-3-030-46781-4_1},
	abstract = {Design Science Research ({DSR}) is a problem-solving paradigm that seeks to enhance human knowledge via the creation of innovative artifacts. Simply stated, {DSR} seeks to enhance technology and science knowledge bases via the creation of innovative artifacts that solve problems and improve the environment in which they are instantiated. The results of {DSR} include both the newly designed artifacts and design knowledge ({DK}) that provides a fuller understanding via design theories of why the artifacts enhance (or, disrupt) the relevant application contexts. The goal of this introduction chapter is to provide a brief survey of {DSR} concepts for better understanding of the following chapters that present {DSR} case studies.},
	pages = {1--13},
	booktitle = {Design Science Research. Cases},
	publisher = {Springer International Publishing},
	author = {vom Brocke, Jan and Hevner, Alan and Maedche, Alexander},
	editor = {vom Brocke, Jan and Hevner, Alan and Maedche, Alexander},
	urldate = {2025-01-08},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-46781-4_1},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\WYGPCY7E\\vom Brocke et al. - 2020 - Introduction to Design Science Research.pdf:application/pdf},
}

@report{benner-wickner_leitfaden_2020,
	title = {Leitfaden für die Nutzung von Design Science Research in Abschlussarbeiten},
	rights = {http://www.econstor.eu/dspace/Nutzungsbedingungen},
	url = {https://www.econstor.eu/handle/10419/229136},
	abstract = {A common topic when performing research in technical disciplines is to design some kind of artefact, such as a model, an information system, or a method for performing a certain task. To address this topic in a systematic and scientific way, Design Science Research ({DSR}) has established itself as an appropriate research method. There are many different variations of {DSR}, on different levels of complexity and detail, but all are based on an iterative approach of designing the desired artefact and evaluating it. This makes it difficult for many students to get an understanding of {DSR} in order to use it for their thesis (on bachelor or master level). The goal of the current paper therefore is to provide a short introduction to {DSR} for these students, based on the {DSR} variant described by (Österle et al., 2010) which is considered fairly basic and easy to understand and use. We do not try to introduce any new approach to {DSR} but provide a short introduction to this existing variant of {DSR}.},
	number = {2/2020},
	institution = {{IUBH} Discussion Papers - {IT} \& Engineering},
	type = {Working Paper},
	author = {Benner-Wickner, Marian and Kneuper, Ralf and Schlömer, Inga},
	urldate = {2025-01-08},
	date = {2020},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YJR37ZUE\\Benner-Wickner et al. - 2020 - Leitfaden für die Nutzung von Design Science Research in Abschlussarbeiten.pdf:application/pdf},
}

@article{robson_real_2002,
	title = {Real World Research : A Resource for Social Scientists and Practitioner-Researchers / C. Robson.},
	author = {Robson, Colin},
	date = {2002-01},
	keywords = {notion},
}

@article{lethbridge_studying_2005,
	title = {Studying Software Engineers: Data Collection Techniques for Software Field Studies},
	volume = {10},
	issn = {1573-7616},
	url = {https://doi.org/10.1007/s10664-005-1290-x},
	doi = {10.1007/s10664-005-1290-x},
	shorttitle = {Studying Software Engineers},
	abstract = {Software engineering is an intensively people-oriented activity, yet too little is known about how designers, maintainers, requirements analysts and all other types of software engineers perform their work. In order to improve software engineering tools and practice, it is therefore essential to conduct field studies, i.e. to study real practitioners as they solve real problems. To do so effectively, however, requires an understanding of the techniques most suited to each type of field study task. In this paper, we provide a taxonomy of techniques, focusing on those for data collection. The taxonomy is organized according to the degree of human intervention each requires. For each technique, we provide examples from the literature, an analysis of some of its advantages and disadvantages, and a discussion of how to use it effectively. We also briefly talk about field study design in general, and data analysis.},
	pages = {311--341},
	number = {3},
	journaltitle = {Empirical Software Engineering},
	shortjournal = {Empir Software Eng},
	author = {Lethbridge, Timothy C. and Sim, Susan Elliott and Singer, Janice},
	urldate = {2025-01-09},
	date = {2005-07-01},
	langid = {english},
	keywords = {notion, empirical software engineering, Field studies, work practices},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YN4MLDI8\\Lethbridge et al. - 2005 - Studying Software Engineers Data Collection Techniques for Software Field Studies.pdf:application/pdf},
}

@article{wohlin_guiding_2021,
	title = {Guiding the selection of research methodology in industry–academia collaboration in software engineering},
	volume = {140},
	issn = {0950-5849},
	url = {https://www.sciencedirect.com/science/article/pii/S0950584921001361},
	doi = {10.1016/j.infsof.2021.106678},
	abstract = {Background:
The literature concerning research methodologies and methods has increased in software engineering in the last decade. However, there is limited guidance on selecting an appropriate research methodology for a given research study or project.
Objective:
Based on a selection of research methodologies suitable for software engineering research in collaboration between industry and academia, we present, discuss and compare the methodologies aiming to provide guidance on which research methodology to choose in a given situation to ensure successful industry–academia collaboration in research.
Method:
Three research methodologies were chosen for two main reasons. Design Science and Action Research were selected for their usage in software engineering. We also chose a model emanating from software engineering, i.e., the Technology Transfer Model. An overview of each methodology is provided. It is followed by a discussion and an illustration concerning their use in industry–academia collaborative research. The three methodologies are then compared using a set of criteria as a basis for our guidance.
Results:
The discussion and comparison of the three research methodologies revealed general similarities and distinct differences. All three research methodologies are easily mapped to the general research process describe–solve–practice, while the main driver behind the formulation of the research methodologies is different. Thus, we guide in selecting a research methodology given the primary research objective for a given research study or project in collaboration between industry and academia.
Conclusions:
We observe that the three research methodologies have different main objectives and differ in some characteristics, although still having a lot in common. We conclude that it is vital to make an informed decision concerning which research methodology to use. The presentation and comparison aim to guide selecting an appropriate research methodology when conducting research in collaboration between industry and academia.},
	pages = {106678},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Wohlin, Claes and Runeson, Per},
	urldate = {2025-01-09},
	date = {2021-12-01},
	keywords = {notion, Research methodology, Action Research, Design Science, Industry–academia collaboration, Selecting research methodology, Technology Transfer Model},
	file = {Volltext:C\:\\Users\\Marco\\Zotero\\storage\\FADWUMTR\\Wohlin und Runeson - 2021 - Guiding the selection of research methodology in industry–academia collaboration in software enginee.pdf:application/pdf},
}

@incollection{runeson_design_2020,
	location = {Cham},
	title = {The Design Science Paradigm as a Frame for Empirical Software Engineering},
	isbn = {978-3-030-32489-6},
	url = {https://doi.org/10.1007/978-3-030-32489-6_5},
	abstract = {Software engineering research aims to help improve real-world practice. With the adoption of empirical software engineering research methods, the understanding of real-world needs and validation of solution proposals have evolved. However, the philosophical perspective on what constitutes theoretical knowledge and research contributions in software engineering is less discussed in the community. In this chapter, we use the design science paradigm as a frame for articulating and communicating prescriptive software engineering research contributions. Design science embraces problem conceptualization, solution (or artifact) design, and validation of solution proposals, with recommendations for practice phrased as technological rules. Design science is used in related research areas, particularly information systems and management theory. We elaborate the constructs of design science for software engineering, relate them to different conceptualizations of design science, and provide examples of possible research methods. We outline how the assessment of research contributions, industry–academia communication, and theoretical knowledge building may be supported by the design science paradigm. Finally, we provide examples of software engineering research presented through a design science lens.},
	pages = {127--147},
	booktitle = {Contemporary Empirical Methods in Software Engineering},
	publisher = {Springer International Publishing},
	author = {Runeson, Per and Engström, Emelie and Storey, Margaret-Anne},
	editor = {Felderer, Michael and Travassos, Guilherme Horta},
	urldate = {2025-01-09},
	date = {2020},
	langid = {english},
	doi = {10.1007/978-3-030-32489-6_5},
	keywords = {notion},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\58QCMJL8\\Runeson et al. - 2020 - The Design Science Paradigm as a Frame for Empirical Software Engineering.pdf:application/pdf},
}

@book{wieringa_design_2014,
	location = {Berlin, Heidelberg},
	title = {Design Science Methodology for Information Systems and Software Engineering},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-43838-1 978-3-662-43839-8},
	url = {https://link.springer.com/10.1007/978-3-662-43839-8},
	publisher = {Springer},
	author = {Wieringa, Roel J.},
	urldate = {2025-01-09},
	date = {2014},
	langid = {english},
	doi = {10.1007/978-3-662-43839-8},
	keywords = {notion, case studies, design science, empirical studies, evaluation, experimentation, requirements engineering, research methodologies, software design engineering, validation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\UDFZZR5Q\\Wieringa - 2014 - Design Science Methodology for Information Systems and Software Engineering.pdf:application/pdf},
}

@inproceedings{ali_open_2023,
	title = {Open research knowledge graph for structuring scholarly contributions using transformers},
	url = {https://ieeexplore.ieee.org/document/10089637},
	doi = {10.1109/ICACS55311.2023.10089637},
	abstract = {More research papers are being published now than have ever been at any point in history. It is becoming increasingly difficult for the researchers to keep up with the papers that are being published in even a very narrow domain. This study proposes to build an open research knowledge graph ({ORKG}) that shows the scholarly contributions of the published papers. The paper makes use of natural language processing techniques and state-of-the-art deep learning models to achieve this task. The system generates a knowledge graph after performing four main steps including sentence classification, phrase extraction, triple formation (and classification) and finally, knowledge graph generation. Different state-of-the-art deep learning models such as {RoBERTa} have been used for classification and phrase extraction tasks whereas triple formation was performed using different heuristics. Finally, a knowledge graph is generated through which an end-user can identify the scholarly contributions in scholarly article. Experimental results are compared against other systems and show encouraging results.},
	eventtitle = {2023 4th International Conference on Advancements in Computational Sciences ({ICACS})},
	pages = {1--8},
	booktitle = {2023 4th International Conference on Advancements in Computational Sciences ({ICACS})},
	author = {Ali, Mehboob and Malik, Abdullah and Bashir, Maryam},
	urldate = {2025-01-09},
	date = {2023-02},
	keywords = {Natural language processing, {BERT}, Deep learning, Task analysis, Knowledge graphs, History, knowledge graph, {LSTM}, Transformers},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ED8PZU7B\\Ali et al. - 2023 - Open research knowledge graph for structuring scholarly contributions using transformers.pdf:application/pdf},
}

@inproceedings{wang_evaluating_2024,
	location = {Honolulu {HI} {USA}},
	title = {Evaluating Large Language Models on Academic Literature Understanding and Review: An Empirical Study among Early-stage Scholars},
	isbn = {979-8-4007-0330-0},
	url = {https://dl.acm.org/doi/10.1145/3613904.3641917},
	doi = {10.1145/3613904.3641917},
	shorttitle = {Evaluating Large Language Models on Academic Literature Understanding and Review},
	eventtitle = {{CHI} '24: {CHI} Conference on Human Factors in Computing Systems},
	pages = {1--18},
	booktitle = {Proceedings of the {CHI} Conference on Human Factors in Computing Systems},
	publisher = {{ACM}},
	author = {Wang, Jiyao and Hu, Haolong and Wang, Zuyuan and Yan, Song and Sheng, Youyu and He, Dengbo},
	urldate = {2025-01-09},
	date = {2024-05-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RCKZJETD\\Wang et al. - 2024 - Evaluating Large Language Models on Academic Literature Understanding and Review An Empirical Study.pdf:application/pdf},
}

@article{mishra_use_2024,
	title = {Use of large language models as artificial intelligence tools in academic research and publishing among global clinical researchers},
	volume = {14},
	rights = {2024 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-024-81370-6},
	doi = {10.1038/s41598-024-81370-6},
	abstract = {With breakthroughs in Natural Language Processing and Artificial Intelligence ({AI}), the usage of Large Language Models ({LLMs}) in academic research has increased tremendously. Models such as Generative Pre-trained Transformer ({GPT}) are used by researchers in literature review, abstract screening, and manuscript drafting. However, these models also present the attendant challenge of providing ethically questionable scientific information. Our study provides a snapshot of global researchers’ perception of current trends and future impacts of {LLMs} in research. Using a cross-sectional design, we surveyed 226 medical and paramedical researchers from 59 countries across 65 specialties, trained in the Global Clinical Scholars’ Research Training certificate program of Harvard Medical School between 2020 and 2024. Majority (57.5\%) of these participants practiced in an academic setting with a median of 7 (2,18) {PubMed} Indexed published articles. 198 respondents (87.6\%) were aware of {LLMs} and those who were aware had higher number of publications (p {\textless} 0.001). 18.7\% of the respondents who were aware (n = 37) had previously used {LLMs} in publications especially for grammatical errors and formatting (64.9\%); however, most (40.5\%) did not acknowledge its use in their papers. 50.8\% of aware respondents (n = 95) predicted an overall positive future impact of {LLMs} while 32.6\% were unsure of its scope. 52\% of aware respondents (n = 102) believed that {LLMs} would have a major impact in areas such as grammatical errors and formatting (66.3\%), revision and editing (57.2\%), writing (57.2\%) and literature review (54.2\%). 58.1\% of aware respondents were opined that journals should allow for use of {AI} in research and 78.3\% believed that regulations should be put in place to avoid its abuse. Seeing the perception of researchers towards {LLMs} and the significant association between awareness of {LLMs} and number of published works, we emphasize the importance of developing comprehensive guidelines and ethical framework to govern the use of {AI} in academic research and address the current challenges.},
	pages = {31672},
	number = {1},
	journaltitle = {Scientific Reports},
	shortjournal = {Sci Rep},
	author = {Mishra, Tanisha and Sutanto, Edward and Rossanti, Rini and Pant, Nayana and Ashraf, Anum and Raut, Akshay and Uwabareze, Germaine and Oluwatomiwa, Ajayi and Zeeshan, Bushra},
	urldate = {2025-01-09},
	date = {2024-12-30},
	langid = {english},
	note = {Publisher: Nature Publishing Group},
	keywords = {Health care, Medical research},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\WS6QJ8HR\\Mishra et al. - 2024 - Use of large language models as artificial intelligence tools in academic research and publishing am.pdf:application/pdf},
}

@article{han_automating_2024,
	title = {Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview},
	volume = {14},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/14/19/9103},
	doi = {10.3390/app14199103},
	shorttitle = {Automating Systematic Literature Reviews with Retrieval-Augmented Generation},
	abstract = {This study examines Retrieval-Augmented Generation ({RAG}) in large language models ({LLMs}) and their significant application for undertaking systematic literature reviews ({SLRs}). {RAG}-based {LLMs} can potentially automate tasks like data extraction, summarization, and trend identification. However, while {LLMs} are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. {RAG} mitigates these limitations by integrating {LLMs}’ generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the {RAG} framework—retrieval, augmentation, and generation. We then discuss applications of {RAG}-based {LLMs} to {SLR} automation and highlight future research topics, including integration of domain-specific {LLMs}, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of {RAG}-based {LLMs} for automating {SRLs}, which covers four stages of {SLR} process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between {LLM} selection, training strategies, {RAG} techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.},
	pages = {9103},
	number = {19},
	journaltitle = {Applied Sciences},
	author = {Han, Binglan and Susnjak, Teo and Mathrani, Anuradha},
	urldate = {2025-01-09},
	date = {2024-01},
	langid = {english},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {large language models, retrieval-augmented generation, systematic literature review},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\W99WX8JG\\Han et al. - 2024 - Automating Systematic Literature Reviews with Retrieval-Augmented Generation A Comprehensive Overvi.pdf:application/pdf},
}

@misc{sanmartin_kg-rag_2024,
	title = {{KG}-{RAG}: Bridging the Gap Between Knowledge and Creativity},
	url = {http://arxiv.org/abs/2405.12035},
	doi = {10.48550/arXiv.2405.12035},
	shorttitle = {{KG}-{RAG}},
	abstract = {Ensuring factual accuracy while maintaining the creative capabilities of Large Language Model Agents ({LMAs}) poses significant challenges in the development of intelligent agent systems. {LMAs} face prevalent issues such as information hallucinations, catastrophic forgetting, and limitations in processing long contexts when dealing with knowledge-intensive tasks. This paper introduces a {KG}-{RAG} (Knowledge Graph-Retrieval Augmented Generation) pipeline, a novel framework designed to enhance the knowledge capabilities of {LMAs} by integrating structured Knowledge Graphs ({KGs}) with the functionalities of {LLMs}, thereby significantly reducing the reliance on the latent knowledge of {LLMs}. The {KG}-{RAG} pipeline constructs a {KG} from unstructured text and then performs information retrieval over the newly created graph to perform {KGQA} (Knowledge Graph Question Answering). The retrieval methodology leverages a novel algorithm called Chain of Explorations ({CoE}) which benefits from {LLMs} reasoning to explore nodes and relationships within the {KG} sequentially. Preliminary experiments on the {ComplexWebQuestions} dataset demonstrate notable improvements in the reduction of hallucinated content and suggest a promising path toward developing intelligent systems adept at handling knowledge-intensive tasks.},
	number = {{arXiv}:2405.12035},
	publisher = {{arXiv}},
	author = {Sanmartin, Diego},
	urldate = {2025-01-11},
	date = {2024-05-20},
	eprinttype = {arxiv},
	eprint = {2405.12035 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XA5LBC4J\\Sanmartin - 2024 - KG-RAG Bridging the Gap Between Knowledge and Creativity.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2DL8NYWW\\2405.html:text/html},
}

@inproceedings{ramos_using_2003,
	title = {Using {TF}-{IDF} to Determine Word Relevance in Document Queries},
	url = {https://www.semanticscholar.org/paper/Using-TF-IDF-to-Determine-Word-Relevance-in-Queries-Ramos/b3bf6373ff41a115197cb5b30e57830c16130c2c},
	abstract = {In this paper, we examine the results of applying Term Frequency Inverse Document Frequency ({TF}-{IDF}) to determine what words in a corpus of documents might be more favorable to use in a query. As the term implies, {TF}-{IDF} calculates values for each word in a document through an inverse proportion of the frequency of the word in a particular document to the percentage of documents the word appears in. Words with high {TF}-{IDF} numbers imply a strong relationship with the document they appear in, suggesting that if that word were to appear in a query, the document could be of interest to the user. We provide evidence that this simple algorithm efficiently categorizes relevant words that can enhance query retrieval.},
	author = {Ramos, J. E.},
	urldate = {2025-01-11},
	date = {2003},
}

@article{robertson_probabilistic_2009,
	title = {The Probabilistic Relevance Framework: {BM}25 and Beyond},
	volume = {3},
	issn = {1554-0669, 1554-0677},
	url = {http://www.nowpublishers.com/article/Details/INR-019},
	doi = {10.1561/1500000019},
	shorttitle = {The Probabilistic Relevance Framework},
	abstract = {The Probabilistic Relevance Framework ({PRF}) is a formal framework for document retrieval, grounded in work done in the 1970—1980s, which led to the development of one of the most successful text-retrieval algorithms, {BM}25. In recent years, research in the {PRF} has yielded new retrieval models capable of taking into account document meta-data (especially structure and link-graph information). Again, this has led to one of the most successful Web-search and corporate-search algorithms, {BM}25F. This work presents the {PRF} from a conceptual point of view, describing the probabilistic modelling assumptions behind the framework and the different ranking algorithms that result from its application: the binary independence model, relevance feedback models, {BM}25 and {BM}25F. It also discusses the relation between the {PRF} and other statistical models for {IR}, and covers some related topics, such as the use of non-textual features, and parameter optimisation for models with free parameters.},
	pages = {333--389},
	number = {4},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	shortjournal = {{FNT} in Information Retrieval},
	author = {Robertson, Stephen and Zaragoza, Hugo},
	urldate = {2025-01-11},
	date = {2009},
	langid = {english},
}

@misc{douze_faiss_2024,
	title = {The Faiss library},
	doi = {10.48550/arXiv.2401.08281},
	abstract = {Vector databases typically manage large collections of embedding vectors. Currently, {AI} applications are growing rapidly, and so is the number of embeddings that need to be stored and indexed. The Faiss library is dedicated to vector similarity search, a core functionality of vector databases. Faiss is a toolkit of indexing methods and related primitives used to search, cluster, compress and transform vectors. This paper describes the trade-off space of vector search and the design principles of Faiss in terms of structure, approach to optimization and interfacing. We benchmark key features of the library and discuss a few selected applications to highlight its broad applicability.},
	number = {{arXiv}:2401.08281},
	publisher = {{arXiv}},
	author = {Douze, Matthijs and Guzhva, Alexandr and Deng, Chengqi and Johnson, Jeff and Szilvasy, Gergely and Mazaré, Pierre-Emmanuel and Lomeli, Maria and Hosseini, Lucas and Jégou, Hervé},
	date = {2024-09-06},
	eprinttype = {arxiv},
	eprint = {2401.08281 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Software Engineering},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\FISWHME8\\Douze et al. - 2024 - The Faiss library.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VTEUHH4I\\2401.html:text/html},
}

@article{besta_graph_2024,
	title = {Graph of Thoughts: Solving Elaborate Problems with Large Language Models},
	volume = {38},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/2308.09687},
	doi = {10.1609/aaai.v38i16.29720},
	shorttitle = {Graph of Thoughts},
	abstract = {We introduce Graph of Thoughts ({GoT}): a framework that advances prompting capabilities in large language models ({LLMs}) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts ({ToT}). The key idea and primary advantage of {GoT} is the ability to model the information generated by an {LLM} as an arbitrary graph, where units of information ("{LLM} thoughts") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary {LLM} thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that {GoT} offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62\% over {ToT}, while simultaneously reducing costs by {\textgreater}31\%. We ensure that {GoT} is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the {LLM} reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.},
	pages = {17682--17690},
	number = {16},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	shortjournal = {{AAAI}},
	author = {Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Podstawski, Michal and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Niewiadomski, Hubert and Nyczyk, Piotr and Hoefler, Torsten},
	urldate = {2025-01-11},
	date = {2024-03-24},
	eprinttype = {arxiv},
	eprint = {2308.09687 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\2GKAJAW3\\Besta et al. - 2024 - Graph of Thoughts Solving Elaborate Problems with Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\GFZ779SG\\2308.html:text/html},
}

@misc{lanchantin_learning_2023,
	title = {Learning to Reason and Memorize with Self-Notes},
	url = {http://arxiv.org/abs/2305.00833},
	doi = {10.48550/arXiv.2305.00833},
	abstract = {Large language models have been shown to struggle with multi-step reasoning, and do not retain previous reasoning steps for future use. We propose a simple method for solving both of these problems by allowing the model to take Self-Notes. Unlike recent chain-of-thought or scratchpad approaches, the model can deviate from the input context at any time to explicitly think and write down its thoughts. This allows the model to perform reasoning on the fly as it reads the context and even integrate previous reasoning steps, thus enhancing its memory with useful information and enabling multi-step reasoning. Experiments across a wide variety of tasks demonstrate that our method can outperform chain-of-thought and scratchpad methods by taking Self-Notes that interleave the input text.},
	number = {{arXiv}:2305.00833},
	publisher = {{arXiv}},
	author = {Lanchantin, Jack and Toshniwal, Shubham and Weston, Jason and Szlam, Arthur and Sukhbaatar, Sainbayar},
	urldate = {2025-01-11},
	date = {2023-10-31},
	eprinttype = {arxiv},
	eprint = {2305.00833 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\SXLQAQBY\\Lanchantin et al. - 2023 - Learning to Reason and Memorize with Self-Notes.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\J7QCZS54\\2305.html:text/html},
}

@article{ibrahim_survey_2024,
	title = {A survey on augmenting knowledge graphs ({KGs}) with large language models ({LLMs}): models, evaluation metrics, benchmarks, and challenges},
	volume = {4},
	issn = {2731-0809},
	doi = {10.1007/s44163-024-00175-8},
	shorttitle = {A survey on augmenting knowledge graphs ({KGs}) with large language models ({LLMs})},
	abstract = {Integrating Large Language Models ({LLMs}) with Knowledge Graphs ({KGs}) enhances the interpretability and performance of {AI} systems. This research comprehensively analyzes this integration, classifying approaches into three fundamental paradigms: {KG}-augmented {LLMs}, {LLM}-augmented {KGs}, and synergized frameworks. The evaluation examines each paradigm’s methodology, strengths, drawbacks, and practical applications in real-life scenarios. The findings highlight the substantial impact of these integrations in fundamentally improving real-time data analysis, efficient decision-making, and promoting innovation across various domains. In this paper, we also describe essential evaluation metrics and benchmarks for assessing the performance of these integrations, addressing challenges like scalability and computational overhead, and providing potential solutions. This comprehensive analysis underscores the profound impact of these integrations on improving real-time data analysis, enhancing decision-making efficiency, and fostering innovation across various domains.},
	pages = {76},
	number = {1},
	journaltitle = {Discover Artificial Intelligence},
	shortjournal = {Discov Artif Intell},
	author = {Ibrahim, Nourhan and Aboulela, Samar and Ibrahim, Ahmed and Kashef, Rasha},
	date = {2024-11-04},
	langid = {english},
	keywords = {Artificial Intelligence, Deep learning ({DL}), Evaluation metrics, finished, Knowledge graphs ({KGs}), Large language models ({LLMs}), Retrieval augmentation generation ({RAG})},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EYLY5ULD\\Ibrahim et al. - 2024 - A survey on augmenting knowledge graphs (KGs) with large language models (LLMs) models, evaluation.pdf:application/pdf},
}

@misc{baek_direct_2023,
	title = {Direct Fact Retrieval from Knowledge Graphs without Entity Linking},
	doi = {10.48550/arXiv.2305.12416},
	abstract = {There has been a surge of interest in utilizing Knowledge Graphs ({KGs}) for various natural language processing/understanding tasks. The conventional mechanism to retrieve facts in {KGs} usually involves three steps: entity span detection, entity disambiguation, and relation classification. However, this approach requires additional labels for training each of the three subcomponents in addition to pairs of input texts and facts, and also may accumulate errors propagated from failures in previous steps. To tackle these limitations, we propose a simple knowledge retrieval framework, which directly retrieves facts from the {KGs} given the input text based on their representational similarities, which we refer to as Direct Fact Retrieval ({DiFaR}). Specifically, we first embed all facts in {KGs} onto a dense embedding space by using a language model trained by only pairs of input texts and facts, and then provide the nearest facts in response to the input text. Since the fact, consisting of only two entities and one relation, has little context to encode, we propose to further refine ranks of top-k retrieved facts with a reranker that contextualizes the input text and the fact jointly. We validate our {DiFaR} framework on multiple fact retrieval tasks, showing that it significantly outperforms relevant baselines that use the three-step approach.},
	number = {{arXiv}:2305.12416},
	publisher = {{arXiv}},
	author = {Baek, Jinheon and Aji, Alham Fikri and Lehmann, Jens and Hwang, Sung Ju},
	date = {2023-05-21},
	keywords = {Computer Science - Information Retrieval, finished, related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\DCYRP7A7\\Baek et al. - 2023 - Direct Fact Retrieval from Knowledge Graphs without Entity Linking.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2T2D56XF\\2305.html:text/html},
}

@inproceedings{lehmann_large_2024,
	location = {Cham},
	title = {Large Language Models for Scientific Question Answering: An Extensive Analysis of the {SciQA} Benchmark},
	isbn = {978-3-031-60626-7},
	doi = {10.1007/978-3-031-60626-7_11},
	shorttitle = {Large Language Models for Scientific Question Answering},
	abstract = {The {SciQA} benchmark for scientific question answering aims to represent a challenging task for next-generation question-answering systems on which vanilla large language models fail. In this article, we provide an analysis of the performance of language models on this benchmark including prompting and fine-tuning techniques to adapt them to the {SciQA} task. We show that both fine-tuning and prompting techniques with intelligent few-shot selection allow us to obtain excellent results on the {SciQA} benchmark. We discuss the valuable lessons and common error categories, and outline their implications on how to optimise large language models for question answering over knowledge graphs.},
	pages = {199--217},
	booktitle = {The Semantic Web},
	publisher = {Springer Nature Switzerland},
	author = {Lehmann, Jens and Meloni, Antonello and Motta, Enrico and Osborne, Francesco and Recupero, Diego Reforgiato and Salatino, Angelo Antonio and Vahdati, Sahar},
	editor = {Meroño Peñuela, Albert and Dimou, Anastasia and Troncy, Raphaël and Hartig, Olaf and Acosta, Maribel and Alam, Mehwish and Paulheim, Heiko and Lisena, Pasquale},
	date = {2024},
	langid = {english},
	keywords = {Question answering, Knowledge graphs, Few-shot learning, Fine-tuning, Language models.},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XGV6PN7E\\Lehmann et al. - 2024 - Large Language Models for Scientific Question Answering An Extensive Analysis of the SciQA Benchmar.pdf:application/pdf},
}

@article{taffa_leveraging_2023,
	title = {Leveraging {LLMs} in Scholarly Knowledge Graph Question Answering},
	rights = {Creative Commons Attribution 4.0 International},
	doi = {10.48550/ARXIV.2311.09841},
	abstract = {This paper presents a scholarly Knowledge Graph Question Answering ({KGQA}) that answers bibliographic natural language questions by leveraging a large language model ({LLM}) in a few-shot manner. The model initially identifies the top-n similar training questions related to a given test question via a {BERT}-based sentence encoder and retrieves their corresponding {SPARQL}. Using the top-n similar question-{SPARQL} pairs as an example and the test question creates a prompt. Then pass the prompt to the {LLM} and generate a {SPARQL}. Finally, runs the {SPARQL} against the underlying {KG} - {ORKG} (Open Research {KG}) endpoint and returns an answer. Our system achieves an F1 score of 99.0\%, on {SciQA} - one of the Scholarly-{QALD}-23 challenge benchmarks.},
	author = {Taffa, Tilahun Abedissa and Usbeck, Ricardo},
	date = {2023},
	keywords = {Artificial Intelligence (cs.{AI}), Computation and Language (cs.{CL}), Databases (cs.{DB}), {FOS}: Computer and information sciences, Machine Learning (cs.{LG}), no type information},
}

@book{wohlin_experimentation_2024,
	location = {Berlin, Heidelberg},
	title = {Experimentation in Software Engineering},
	rights = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-662-69306-3},
	publisher = {Springer},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	date = {2024},
	langid = {english},
	doi = {10.1007/978-3-662-69306-3},
	keywords = {case study design, empirical software engineering, experiment analysis, experiment design, experiment process, experimentation, measurement},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\TQUGR5G5\\Wohlin et al. - 2024 - Experimentation in Software Engineering.pdf:application/pdf},
}

@misc{liu_recall_2023,
	title = {{RECALL}: A Benchmark for {LLMs} Robustness against External Counterfactual Knowledge},
	url = {http://arxiv.org/abs/2311.08147},
	doi = {10.48550/arXiv.2311.08147},
	shorttitle = {{RECALL}},
	abstract = {{LLMs} and {AI} chatbots have improved people's efficiency in various fields. However, the necessary knowledge for answering the question may be beyond the models' knowledge boundaries. To mitigate this issue, many researchers try to introduce external knowledge, such as knowledge graphs and Internet contents, into {LLMs} for up-to-date information. However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response. Thus there is a pressing need for {LLMs} to possess the ability to distinguish reliable information from external knowledge. Therefore, to evaluate the ability of {LLMs} to discern the reliability of external knowledge, we create a benchmark from existing knowledge bases. Our benchmark consists of two tasks, Question Answering and Text Generation, and for each task, we provide models with a context containing counterfactual information. Evaluation results show that existing {LLMs} are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.},
	number = {{arXiv}:2311.08147},
	publisher = {{arXiv}},
	author = {Liu, Yi and Huang, Lianzhe and Li, Shicheng and Chen, Sishuo and Zhou, Hao and Meng, Fandong and Zhou, Jie and Sun, Xu},
	urldate = {2025-01-13},
	date = {2023-11-14},
	eprinttype = {arxiv},
	eprint = {2311.08147 [cs]},
	keywords = {finished, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\I44LAHST\\Liu et al. - 2023 - RECALL A Benchmark for LLMs Robustness against External Counterfactual Knowledge.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\47TMX7MD\\2311.html:text/html},
}

@misc{tang_multihop-rag_2024,
	title = {{MultiHop}-{RAG}: Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries},
	doi = {10.48550/arXiv.2401.15391},
	shorttitle = {{MultiHop}-{RAG}},
	abstract = {Retrieval-augmented generation ({RAG}) augments large language models ({LLM}) by retrieving relevant knowledge, showing promising potential in mitigating {LLM} hallucinations and enhancing response quality, thereby facilitating the great adoption of {LLMs} in practice. However, we find that existing {RAG} systems are inadequate in answering multi-hop queries, which require retrieving and reasoning over multiple pieces of supporting evidence. Furthermore, to our knowledge, no existing {RAG} benchmarking dataset focuses on multi-hop queries. In this paper, we develop a novel dataset, {MultiHop}-{RAG}, which consists of a knowledge base, a large collection of multi-hop queries, their ground-truth answers, and the associated supporting evidence. We detail the procedure of building the dataset, utilizing an English news article dataset as the underlying {RAG} knowledge base. We demonstrate the benchmarking utility of {MultiHop}-{RAG} in two experiments. The first experiment compares different embedding models for retrieving evidence for multi-hop queries. In the second experiment, we examine the capabilities of various state-of-the-art {LLMs}, including {GPT}-4, {PaLM}, and Llama2-70B, in reasoning and answering multi-hop queries given the evidence. Both experiments reveal that existing {RAG} methods perform unsatisfactorily in retrieving and answering multi-hop queries. We hope {MultiHop}-{RAG} will be a valuable resource for the community in developing effective {RAG} systems, thereby facilitating greater adoption of {LLMs} in practice. The {MultiHop}-{RAG} and implemented {RAG} system is publicly available at https://github.com/yixuantt/{MultiHop}-{RAG}/.},
	number = {{arXiv}:2401.15391},
	publisher = {{arXiv}},
	author = {Tang, Yixuan and Yang, Yi},
	date = {2024-01-27},
	keywords = {Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\8BDKYEW6\\Tang und Yang - 2024 - MultiHop-RAG Benchmarking Retrieval-Augmented Generation for Multi-Hop Queries.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3UVYK7YI\\2401.html:text/html},
}

@inproceedings{gienapp_evaluating_2024,
	title = {Evaluating Generative Ad Hoc Information Retrieval},
	url = {http://arxiv.org/abs/2311.04694},
	doi = {10.1145/3626772.3657849},
	abstract = {Recent advances in large language models have enabled the development of viable generative retrieval systems. Instead of a traditional document ranking, generative retrieval systems often directly return a grounded generated text as a response to a query. Quantifying the utility of the textual responses is essential for appropriately evaluating such generative ad hoc retrieval. Yet, the established evaluation methodology for ranking-based ad hoc retrieval is not suited for the reliable and reproducible evaluation of generated responses. To lay a foundation for developing new evaluation methods for generative retrieval systems, we survey the relevant literature from the fields of information retrieval and natural language processing, identify search tasks and system architectures in generative retrieval, develop a new user model, and study its operationalization.},
	pages = {1916--1929},
	booktitle = {Proceedings of the 47th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	author = {Gienapp, Lukas and Scells, Harrisen and Deckers, Niklas and Bevendorff, Janek and Wang, Shuai and Kiesel, Johannes and Syed, Shahbaz and Fröbe, Maik and Zuccon, Guido and Stein, Benno and Hagen, Matthias and Potthast, Martin},
	urldate = {2025-01-13},
	date = {2024-07-10},
	eprinttype = {arxiv},
	eprint = {2311.04694 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\YVDT6JJE\\Gienapp et al. - 2024 - Evaluating Generative Ad Hoc Information Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\EYEHYEEI\\2311.html:text/html},
}

@misc{chen_benchmarking_2023,
	title = {Benchmarking Large Language Models in Retrieval-Augmented Generation},
	doi = {10.48550/arXiv.2309.01431},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a promising approach for mitigating the hallucination of large language models ({LLMs}). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of {RAG} for different {LLMs}. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for {RAG}, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark ({RGB}), a new corpus for {RAG} evaluation in both English and Chinese. {RGB} divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative {LLMs} on {RGB} to diagnose the challenges of current {LLMs} when applying {RAG}. Evaluation reveals that while {LLMs} exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply {RAG} to {LLMs}.},
	number = {{arXiv}:2309.01431},
	publisher = {{arXiv}},
	author = {Chen, Jiawei and Lin, Hongyu and Han, Xianpei and Sun, Le},
	date = {2023-12-20},
	eprinttype = {arxiv},
	eprint = {2309.01431 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\8QRZPBLT\\Chen et al. - 2023 - Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\LPI97YG4\\2309.html:text/html},
}

@misc{lyu_crud-rag_2024,
	title = {{CRUD}-{RAG}: A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Models},
	url = {http://arxiv.org/abs/2401.17043},
	doi = {10.48550/arXiv.2401.17043},
	shorttitle = {{CRUD}-{RAG}},
	abstract = {Retrieval-Augmented Generation ({RAG}) is a technique that enhances the capabilities of large language models ({LLMs}) by incorporating external knowledge sources. This method addresses common {LLM} limitations, including outdated information and the tendency to produce inaccurate "hallucinated" content. However, the evaluation of {RAG} systems is challenging, as existing benchmarks are limited in scope and diversity. Most of the current benchmarks predominantly assess question-answering applications, overlooking the broader spectrum of situations where {RAG} could prove advantageous. Moreover, they only evaluate the performance of the {LLM} component of the {RAG} pipeline in the experiments, and neglect the influence of the retrieval component and the external knowledge database. To address these issues, this paper constructs a large-scale and more comprehensive benchmark, and evaluates all the components of {RAG} systems in various {RAG} application scenarios. Specifically, we have categorized the range of {RAG} applications into four distinct types-Create, Read, Update, and Delete ({CRUD}), each representing a unique use case. "Create" refers to scenarios requiring the generation of original, varied content. "Read" involves responding to intricate questions in knowledge-intensive situations. "Update" focuses on revising and rectifying inaccuracies or inconsistencies in pre-existing texts. "Delete" pertains to the task of summarizing extensive texts into more concise forms. For each of these {CRUD} categories, we have developed comprehensive datasets to evaluate the performance of {RAG} systems. We also analyze the effects of various components of the {RAG} system, such as the retriever, the context length, the knowledge base construction, and the {LLM}. Finally, we provide useful insights for optimizing the {RAG} technology for different scenarios.},
	number = {{arXiv}:2401.17043},
	publisher = {{arXiv}},
	author = {Lyu, Yuanjie and Li, Zhiyu and Niu, Simin and Xiong, Feiyu and Tang, Bo and Wang, Wenjin and Wu, Hao and Liu, Huanyong and Xu, Tong and Chen, Enhong},
	urldate = {2025-01-13},
	date = {2024-07-15},
	eprinttype = {arxiv},
	eprint = {2401.17043 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\FTMUMUG9\\Lyu et al. - 2024 - CRUD-RAG A Comprehensive Chinese Benchmark for Retrieval-Augmented Generation of Large Language Mod.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ZAT68FYY\\2401.html:text/html},
}

@inproceedings{papineni_bleu_2001,
	location = {Philadelphia, Pennsylvania},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	doi = {10.3115/1073083.1073135},
	shorttitle = {{BLEU}},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	eventtitle = {the 40th Annual Meeting},
	pages = {311},
	booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics  - {ACL} '02},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	date = {2001},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DWSZ97V6\\Papineni et al. - 2001 - BLEU a method for automatic evaluation of machine translation.pdf:application/pdf},
}

@misc{rosset_knowledge-aware_2021,
	title = {Knowledge-Aware Language Model Pretraining},
	url = {http://arxiv.org/abs/2007.00655},
	doi = {10.48550/arXiv.2007.00655},
	abstract = {How much knowledge do pretrained language models hold? Recent research observed that pretrained transformers are adept at modeling semantics but it is unclear to what degree they grasp human knowledge, or how to ensure they do so. In this paper we incorporate knowledge-awareness in language model pretraining without changing the transformer architecture, inserting explicit knowledge layers, or adding external storage of semantic information. Rather, we simply signal the existence of entities to the input of the transformer in pretraining, with an entity-extended tokenizer; and at the output, with an additional entity prediction task. Our experiments show that solely by adding these entity signals in pretraining, significantly more knowledge is packed into the transformer parameters: we observe improved language modeling accuracy, factual correctness in {LAMA} knowledge probing tasks, and semantics in the hidden representations through edge probing.We also show that our knowledge-aware language model ({KALM}) can serve as a drop-in replacement for {GPT}-2 models, significantly improving downstream tasks like zero-shot question-answering with no task-related training.},
	number = {{arXiv}:2007.00655},
	publisher = {{arXiv}},
	author = {Rosset, Corby and Xiong, Chenyan and Phan, Minh and Song, Xia and Bennett, Paul and Tiwary, Saurabh},
	urldate = {2025-01-14},
	date = {2021-02-04},
	eprinttype = {arxiv},
	eprint = {2007.00655 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\NNTADWME\\Rosset et al. - 2021 - Knowledge-Aware Language Model Pretraining.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\YRTWE7Q2\\2007.html:text/html},
}

@misc{taffa_hybrid-squad_2024,
	title = {Hybrid-{SQuAD}: Hybrid Scholarly Question Answering Dataset},
	doi = {10.48550/arXiv.2412.02788},
	shorttitle = {Hybrid-{SQuAD}},
	abstract = {Existing Scholarly Question Answering ({QA}) methods typically target homogeneous data sources, relying solely on either text or Knowledge Graphs ({KGs}). However, scholarly information often spans heterogeneous sources, necessitating the development of {QA} systems that integrate information from multiple heterogeneous data sources. To address this challenge, we introduce Hybrid-{SQuAD} (Hybrid Scholarly Question Answering Dataset), a novel large-scale {QA} dataset designed to facilitate answering questions incorporating both text and {KG} facts. The dataset consists of 10.5K question-answer pairs generated by a large language model, leveraging the {KGs} {DBLP} and {SemOpenAlex} alongside corresponding text from Wikipedia. In addition, we propose a {RAG}-based baseline hybrid {QA} model, achieving an exact match score of 69.65 on the Hybrid-{SQuAD} test set.},
	number = {{arXiv}:2412.02788},
	publisher = {{arXiv}},
	author = {Taffa, Tilahun Abedissa and Banerjee, Debayan and Assabie, Yaregal and Usbeck, Ricardo},
	date = {2024-12-05},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF6CAJHK\\Taffa et al. - 2024 - Hybrid-SQuAD Hybrid Scholarly Question Answering Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9MZ7S7VM\\2412.html:text/html},
}

@misc{giglou_scholarly_2024,
	title = {Scholarly Question Answering using Large Language Models in the {NFDI}4DataScience Gateway},
	url = {http://arxiv.org/abs/2406.07257},
	doi = {10.48550/arXiv.2406.07257},
	abstract = {This paper introduces a scholarly Question Answering ({QA}) system on top of the {NFDI}4DataScience Gateway, employing a Retrieval Augmented Generation-based ({RAG}) approach. The {NFDI}4DS Gateway, as a foundational framework, offers a unified and intuitive interface for querying various scientific databases using federated search. The {RAG}-based scholarly {QA}, powered by a Large Language Model ({LLM}), facilitates dynamic interaction with search results, enhancing filtering capabilities and fostering a conversational engagement with the Gateway search. The effectiveness of both the Gateway and the scholarly {QA} system is demonstrated through experimental analysis.},
	number = {{arXiv}:2406.07257},
	publisher = {{arXiv}},
	author = {Giglou, Hamed Babaei and Taffa, Tilahun Abedissa and Abdullah, Rana and Usmanova, Aida and Usbeck, Ricardo and D'Souza, Jennifer and Auer, Sören},
	urldate = {2025-01-18},
	date = {2024-06-11},
	eprinttype = {arxiv},
	eprint = {2406.07257 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HKAKIZPQ\\Giglou et al. - 2024 - Scholarly Question Answering using Large Language Models in the NFDI4DataScience Gateway.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ZFSB6VMH\\2406.html:text/html},
}

@inproceedings{sompel_all_2009,
	title = {All aboard: toward a machine-friendly scholarly communication system},
	shorttitle = {All aboard},
	abstract = {Semantic Scholar extracted view of "All aboard: toward a machine-friendly scholarly communication system" by H. Sompel et al.},
	eventtitle = {The Fourth Paradigm},
	author = {Sompel, H. and Lagoze, C.},
	date = {2009},
	file = {4th_paradigm_book_part4_sompel_lagoze.pdf:C\:\\Users\\Marco\\Zotero\\storage\\8CS5BNNC\\4th_paradigm_book_part4_sompel_lagoze.pdf:application/pdf},
}

@inproceedings{bosman_scholarly_2017,
	title = {The Scholarly Commons - principles and practices to guide research communication},
	rights = {https://creativecommons.org/licenses/by/4.0/legalcode},
	doi = {10.31219/osf.io/6c2xt},
	abstract = {Despite all available technology and despite major disruptions that the internet brought about in many sectors of modern life, scholarly communication has only seen change at glacial pace. Many useful, laudable tools and services are being developed to solve specific issues for particular domain groups. However, the question of how these efforts fit together remains largely unaddressed. If we have alternative models for all parts of the system, will that result in a coherent system? Will it be interoperable? Will it appeal to people as viable alternative? Will it be open and participatory for all?The solution we propose is that of a scholarly commons: a set of principles and rules for the community of researchers and other stakeholders to ascribe to, the practices based on those principles, and the common pool of resources around which the principles and practices revolve. The tenets of the scholarly commons are that research and knowledge should be freely available to all who wish to use or reuse it (open, {FAIR} and citable), participation in the production and use of knowledge should be open to all who wish to participate, and there should be no systemic barriers and disincentives to prevent either such free use or open participation.In this paper, we outline the backgrounds of the idea of the scholarly commons and the various considerations that play a role in defining it. We share the principles of the scholarly commons and the degrees of freedom interpreting those principles, and consider the broader landscape of ideas and charters that the scholarly commons fits into. Finally, we present a call for action to involve like-minded people in the discussion on how to bring such a commons to fruition, and what this would mean for different communities within science and scholarship.},
	author = {Bosman, Jeroen and Bruno, Ian and Chapman, Chris and Greshake Tzovaras, Bastian and Jacobs, Nate and Kramer, Bianca and Martone, Maryann Elizabeth and Murphy, Fiona Louise Mary and O'Donnell, Daniel Paul and Bar-Sinai, Michael and Hagstrom, Stephanie and Utley, Joshua Abel and Veksler, Lusia},
	date = {2017-09-15},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\ABDLXJWY\\Bosman et al. - 2017 - The Scholarly Commons - principles and practices to guide research communication.pdf:application/pdf},
}

@article{bornmann_growth_2021,
	title = {Growth rates of modern science: a latent piecewise growth curve approach to model publication numbers from established and new literature databases},
	volume = {8},
	rights = {2021 The Author(s)},
	issn = {2662-9992},
	doi = {10.1057/s41599-021-00903-w},
	shorttitle = {Growth rates of modern science},
	abstract = {Growth of science is a prevalent issue in science of science studies. In recent years, two new bibliographic databases have been introduced, which can be used to study growth processes in science from centuries back: Dimensions from Digital Science and Microsoft Academic. In this study, we used publication data from these new databases and added publication data from two established databases (Web of Science from Clarivate Analytics and Scopus from Elsevier) to investigate scientific growth processes from the beginning of the modern science system until today. We estimated regression models that included simultaneously the publication counts from the four databases. The results of the unrestricted growth of science calculations show that the overall growth rate amounts to 4.10\% with a doubling time of 17.3 years. As the comparison of various segmented regression models in the current study revealed, models with four or five segments fit the publication data best. We demonstrated that these segments with different growth rates can be interpreted very well, since they are related to either phases of economic (e.g., industrialization) and/or political developments (e.g., Second World War). In this study, we additionally analyzed scientific growth in two broad fields (Physical and Technical Sciences as well as Life Sciences) and the relationship of scientific and economic growth in {UK}. The comparison between the two fields revealed only slight differences. The comparison of the British economic and scientific growth rates showed that the economic growth rate is slightly lower than the scientific growth rate.},
	pages = {1--15},
	number = {1},
	journaltitle = {Humanities and Social Sciences Communications},
	shortjournal = {Humanit Soc Sci Commun},
	author = {Bornmann, Lutz and Haunschild, Robin and Mutz, Rüdiger},
	date = {2021-10-07},
	langid = {english},
	keywords = {Science, Sociology, technology and society},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\P3CXEQWC\\Bornmann et al. - 2021 - Growth rates of modern science a latent piecewise growth curve approach to model publication number.pdf:application/pdf},
}

@article{burton_scholix_2017,
	title = {The Scholix Framework for Interoperability in Data-Literature Information Exchange},
	volume = {23},
	issn = {1082-9873},
	doi = {10.1045/january2017-burton},
	abstract = {The Scholix Framework ({SCHOlarly} {LInk} {eXchange}) is a high level interoperability framework for exchanging information about the links between scholarly literature and data, as well as between datasets. Over the past decade, publishers, data centers, and indexing services have agreed on and implemented numerous bilateral agreements to establish bidirectional links between research data and the scholarly literature. However, because of the considerable differences inherent to these many agreements, there is very limited interoperability between the various solutions. This situation is fueling systemic inefficiencies and limiting the value of these, separated, sets of links. Scholix, a framework proposed by the {RDA}/{WDS} Publishing Data Services working group, envisions a universal interlinking service and proposes the technical guidelines of a multi-hub interoperability framework. Hubs are natural collection and aggregation points for data-literature information from their respective communities. Relevant hubs for the communities of data centers, repositories, and journals include {DataCite}, {OpenAIRE}, and Crossref, respectively. The framework respects existing community-specific practices while enabling interoperability among the hubs through a common conceptual model, an information model and open exchange protocols. The proposed framework will make research data, and the related literature, easier to find and easier to interpret and reuse, and will provide additional incentives for researchers to share their data.},
	number = {1},
	journaltitle = {D-Lib Magazine},
	shortjournal = {D-Lib Magazine},
	author = {Burton, Adrian and Aryani, Amir and Koers, Hylke and Manghi, Paolo and La Bruzzo, Sandro and Stocker, Markus and Diepenbroek, Michael and Schindler, Uwe and Fenner, Martin},
	date = {2017-01},
	langid = {english},
}

@article{ji_survey_2022,
	title = {A Survey on Knowledge Graphs: Representation, Acquisition, and Applications},
	volume = {33},
	issn = {2162-2388},
	doi = {10.1109/TNNLS.2021.3070843},
	shorttitle = {A Survey on Knowledge Graphs},
	abstract = {Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions.},
	pages = {494--514},
	number = {2},
	journaltitle = {{IEEE} Transactions on Neural Networks and Learning Systems},
	author = {Ji, Shaoxiong and Pan, Shirui and Cambria, Erik and Marttinen, Pekka and Yu, Philip S.},
	date = {2022-02},
	note = {Conference Name: {IEEE} Transactions on Neural Networks and Learning Systems},
	keywords = {Cognition, Deep learning, Extraterrestrial measurements, Knowledge acquisition, Knowledge based systems, knowledge graph, knowledge graph completion ({KGC}), reasoning, relation extraction, representation learning, Semantics, Task analysis, Taxonomy},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\X8H4ZGXT\\Ji et al. - 2022 - A Survey on Knowledge Graphs Representation, Acquisition, and Applications.pdf:application/pdf},
}

@inproceedings{farber_semopenalex_2023,
	location = {Cham},
	title = {{SemOpenAlex}: The Scientific Landscape in 26 Billion {RDF} Triples},
	isbn = {978-3-031-47243-5},
	doi = {10.1007/978-3-031-47243-5_6},
	shorttitle = {{SemOpenAlex}},
	abstract = {We present {SemOpenAlex}, an extensive {RDF} knowledge graph that contains over 26 billion triples about scientific publications and their associated entities, such as authors, institutions, journals, and concepts. {SemOpenAlex} is licensed under {CC}0, providing free and open access to the data. We offer the data through multiple channels, including {RDF} dump files, a {SPARQL} endpoint, and as a data source in the Linked Open Data cloud, complete with resolvable {URIs} and links to other data sources. Moreover, we provide embeddings for knowledge graph entities using high-performance computing. {SemOpenAlex} enables a broad range of use-case scenarios, such as exploratory semantic search via our website, large-scale scientific impact quantification, and other forms of scholarly big data analytics within and across scientific disciplines. Additionally, it enables academic recommender systems, such as recommending collaborators, publications, and venues, including explainability capabilities. Finally, {SemOpenAlex} can serve for {RDF} query optimization benchmarks, creating scholarly knowledge-guided language models, and as a hub for semantic scientific publishing.  Data and Services: https://semopenalex.orghttps://w3id.org/{SemOpenAlexCode}:https://github.com/metaphacts/semopenalex/Data License:Creative Commons Zero ({CC}0)Code License:{MIT} License},
	pages = {94--112},
	booktitle = {The Semantic Web – {ISWC} 2023},
	publisher = {Springer Nature Switzerland},
	author = {Färber, Michael and Lamprecht, David and Krause, Johan and Aung, Linn and Haase, Peter},
	editor = {Payne, Terry R. and Presutti, Valentina and Qi, Guilin and Poveda-Villalón, María and Stoilos, Giorgos and Hollink, Laura and Kaoudi, Zoi and Cheng, Gong and Li, Juanzi},
	date = {2023},
	langid = {english},
	keywords = {Digital Libraries, Open Science, Scholarly Data},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DBPYHHAI\\Färber et al. - 2023 - SemOpenAlex The Scientific Landscape in 26 Billion RDF Triples.pdf:application/pdf},
}

@article{verma_scholarly_2023,
	title = {Scholarly knowledge graphs through structuring scholarly communication: a review},
	volume = {9},
	issn = {2198-6053},
	doi = {10.1007/s40747-022-00806-6},
	abstract = {The necessity for scholarly knowledge mining and management has grown significantly as academic literature and its linkages to authors produce enormously. Information extraction, ontology matching, and accessing academic components with relations have become more critical than ever. Therefore, with the advancement of scientific literature, scholarly knowledge graphs have become critical to various applications where semantics can impart meanings to concepts. The objective of study is to report a literature review regarding knowledge graph construction, refinement and utilization in scholarly domain. Based on scholarly literature, the study presents a complete assessment of current state-of-the-art techniques. We presented an analytical methodology to investigate the existing status of scholarly knowledge graphs ({SKG}) by structuring scholarly communication. This review paper investigates the field of applying machine learning, rule-based learning, and natural language processing tools and approaches to construct {SKG}. It further presents the review of knowledge graph utilization and refinement to provide a view of current research efforts. In addition, we offer existing applications and challenges across the board in construction, refinement and utilization collectively. This research will help to identify frontier trends of {SKG} which will motivate future researchers to carry forward their work.},
	pages = {1059--1095},
	number = {1},
	journaltitle = {Complex \& Intelligent Systems},
	shortjournal = {Complex Intell. Syst.},
	author = {Verma, Shilpa and Bhatia, Rajesh and Harit, Sandeep and Batish, Sanjay},
	date = {2023-02-01},
	langid = {english},
	keywords = {Knowledge graph construction, Knowledge graph embedding, Scholarly communication, Utilization},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PWRDQFBZ\\Verma et al. - 2023 - Scholarly knowledge graphs through structuring scholarly communication a review.pdf:application/pdf},
}

@inproceedings{fathalla_towards_2020,
	location = {New York, {NY}, {USA}},
	title = {Towards the semantic formalization of science},
	isbn = {978-1-4503-6866-7},
	url = {https://dl.acm.org/doi/10.1145/3341105.3374132},
	doi = {10.1145/3341105.3374132},
	series = {{SAC} '20},
	abstract = {The past decades have witnessed a huge growth in scholarly information published on the Web, mostly in unstructured or semi-structured formats, which hampers scientific literature exploration and scientometric studies. Past studies on ontologies for structuring scholarly information focused on describing scholarly articles' components, such as document structure, metadata and bibliographies, rather than the scientific work itself. Over the past four years, we have been developing the Science Knowledge Graph Ontologies ({SKGO}), a set of ontologies for modeling the research findings in various fields of modern science resulting in a knowledge graph. Here, we introduce this ontology suite and discuss the design considerations taken into account during its development. We deem that within the next years, a science knowledge graph is likely to become a crucial component for organizing and exploring scientific work.},
	pages = {2057--2059},
	booktitle = {Proceedings of the 35th Annual {ACM} Symposium on Applied Computing},
	publisher = {Association for Computing Machinery},
	author = {Fathalla, Said and Auer, Sören and Lange, Christoph},
	urldate = {2025-01-20},
	date = {2020-03-30},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GLHLBCLS\\Taffa und Usbeck - 2023 - Leveraging LLMs in Scholarly Knowledge Graph Question Answering.pdf:application/pdf;Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\5SGSQYRS\\Fathalla et al. - 2020 - Towards the semantic formalization of science.pdf:application/pdf},
}

@inproceedings{buscaldi_mining_2019,
	location = {Portoroz, Slovenia},
	title = {Mining Scholarly Data for Fine-Grained Knowledge Graph Construction},
	volume = {2377},
	rights = {cc\_by\_nc\_nd\_4},
	url = {http://ceur-ws.org/Vol-2377/paper_3.pdf},
	abstract = {Knowledge graphs ({KG}) are large network of entities and relationships, tipically expressed as {RDF} triples, relevant to a specific domain or an organization. Scientific Knowledge Graphs ({SKGs}) focus on the scholarly domain and typically contain metadata describing research publications such as authors, venues, organizations, research topics, and citations. The next big challenge in this field regards the generation of {SKGs} that also contain a explicit representation of the knowledge presented in research publications. In this paper, we present a preliminary approach that uses a set of {NLP} and Deep Learning methods for extracting entities and relationships from research publications and then integrates them in a {KG}. More specifically, we i) tackle the challenge of knowledge extraction by employing several state-of-the-art Natural Language Processing and Text Mining tools, ii) describe an approach for integrating entities and relationships generated by these tools, iii) analyse an automatically generated Knowledge Graph including 10, 425 entities and 25, 655 relationships derived from 12, 007 publications in the field of Semantic Web, and iv) discuss how Deep Learning methods can be applied to overcome some limitations of the current techniques.},
	eventtitle = {Workshop on Deep Learning for Knowledge Graphs ({DL}4KG2019)},
	pages = {21--30},
	booktitle = {{CEUR} Workshop Proceedings},
	author = {Buscaldi, Davide and Dessì, Danilo and Motta, Enrico and Osborne, Francesco and Reforgiato Recupero, Diego},
	urldate = {2025-01-20},
	date = {2019},
	langid = {english},
	note = {{ISSN}: 1613-0073},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\N7SISY6P\\Buscaldi et al. - 2019 - Mining Scholarly Data for Fine-Grained Knowledge Graph Construction.pdf:application/pdf},
}

@inproceedings{ehrlinger_towards_2016,
	title = {Towards a Definition of Knowledge Graphs},
	abstract = {Recently, the term knowledge graph has been used frequently in research and business, usually in close association with Semantic Web technologies, linked data, large-scale data analytics and cloud computing. Its popularity is clearly in-ﬂuenced by the introduction of Google’s Knowledge Graph in 2012, and since then the term has been widely used without a deﬁnition. A large variety of interpretations has hampered the evolution of a common understanding of knowledge graphs. Numerous research papers refer to Google’s Knowledge Graph, although no oﬃcial documentation about the used methods exists. The prerequisite for widespread academic and commercial adoption of a concept or technology is a common understanding, based ideally on a deﬁnition that is free from ambiguity. We tackle this issue by discussing and deﬁning the term knowledge graph, considering its history and diversity in interpretations and use. Our goal is to propose a deﬁnition of knowledge graphs that serves as basis for discussions on this topic and contributes to a common vision.},
	eventtitle = {International Conference on Semantic Systems},
	author = {Ehrlinger, Lisa and Wöß, Wolfram},
	date = {2016},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZG36DAK7\\Ehrlinger und Wöß - 2016 - Towards a Definition of Knowledge Graphs.pdf:application/pdf},
}

@inproceedings{manghi_data_2012,
	location = {Berlin, Heidelberg},
	title = {The Data Model of the {OpenAIRE} Scientific Communication e-Infrastructure},
	isbn = {978-3-642-35233-1},
	doi = {10.1007/978-3-642-35233-1_18},
	abstract = {The {OpenAIREplus} project aims to further develop and operate the {OpenAIRE} e-infrastructure, in order to provide a central entry point to Open Access and non-Open Access publications and datasets funded by the European Commission and National agencies. The infrastructure provides the services to populate, curate, and enrich an Information Space by collecting metadata descriptions relative to organizations, data sources, projects, funding programmes, persons, publications, and datasets. Stakeholders in the research process and scientific communication, such as researchers, funding agencies, organizations involved in projects, project coordinators, can here find the information to improve their research and statistics to measure the impact of Open Access and funding schemes over research. In this paper, we introduce the functional requirements to be satisfied and describe the {OpenAIREplus} data model entities and relationships required to represent information capable of meeting them.},
	pages = {168--180},
	booktitle = {Metadata and Semantics Research},
	publisher = {Springer},
	author = {Manghi, Paolo and Houssos, Nikos and Mikulicic, Marko and Jörg, Brigitte},
	editor = {Dodero, Juan Manuel and Palomo-Duarte, Manuel and Karampiperis, Pythagoras},
	date = {2012},
	langid = {english},
	keywords = {{CERIF}, data model, {DataCite}, infrastructure, open access},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GM4S6SFS\\Manghi et al. - 2012 - The Data Model of the OpenAIRE Scientific Communication e-Infrastructure.pdf:application/pdf},
}

@inproceedings{ghidini_microsoft_2019,
	location = {Cham},
	title = {The Microsoft Academic Knowledge Graph: A Linked Data Source with 8 Billion Triples of Scholarly Data},
	volume = {11779},
	isbn = {978-3-030-30796-7},
	doi = {10.1007/978-3-030-30796-7_8},
	abstract = {In this paper, we present the Microsoft Academic Knowledge Graph ({MAKG}), a large {RDF} data set with over eight billion triples with information about scientific publications and related entities, such as authors, institutions, journals, and fields of study. The data set is licensed under the Open Data Commons Attribution License ({ODC}-By). By providing the data as {RDF} dump files as well as a data source in the Linked Open Data cloud with resolvable {URIs} and links to other data sources, we bring a vast amount of scholarly data to the Web of Data. Furthermore, we provide entity embeddings for all 210 million represented publications. We facilitate a number of use case scenarios, particularly in the field of digital libraries, such as (1) entity-centric exploration of papers, researchers, affiliations, etc.; (2) data integration tasks using {RDF} as a common data model and links to other data sources; and (3) data analysis and knowledge discovery of scholarly data.},
	pages = {113--129},
	publisher = {Springer International Publishing},
	author = {Färber, Michael},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	date = {2019},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\R2QL7NK4\\Färber - 2019 - The Microsoft Academic Knowledge Graph A Linked Data Source with 8 Billion Triples of Scholarly Dat.pdf:application/pdf},
}

@article{farber_linked_2023,
	title = {Linked Papers With Code: The Latest in Machine Learning as an {RDF} Knowledge Graph},
	rights = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2310.20475},
	doi = {10.48550/ARXIV.2310.20475},
	shorttitle = {Linked Papers With Code},
	abstract = {In this paper, we introduce Linked Papers With Code ({LPWC}), an {RDF} knowledge graph that provides comprehensive, current information about almost 400,000 machine learning publications. This includes the tasks addressed, the datasets utilized, the methods implemented, and the evaluations conducted, along with their results. Compared to its non-{RDF}-based counterpart Papers With Code, {LPWC} not only translates the latest advancements in machine learning into {RDF} format, but also enables novel ways for scientific impact quantification and scholarly key content recommendation. {LPWC} is openly accessible at https://linkedpaperswithcode.com and is licensed under {CC}-{BY}-{SA} 4.0. As a knowledge graph in the Linked Open Data cloud, we offer {LPWC} in multiple formats, from {RDF} dump files to a {SPARQL} endpoint for direct web queries, as well as a data source with resolvable {URIs} and links to the data sources {SemOpenAlex}, Wikidata, and {DBLP}. Additionally, we supply knowledge graph embeddings, enabling {LPWC} to be readily applied in machine learning applications.},
	author = {Färber, Michael and Lamprecht, David},
	urldate = {2025-01-20},
	date = {2023},
	note = {Publisher: {arXiv}
Version Number: 1},
	keywords = {Artificial Intelligence (cs.{AI}), {FOS}: Computer and information sciences, Digital Libraries (cs.{DL})},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JZ28XUI6\\Färber und Lamprecht - 2023 - Linked Papers With Code The Latest in Machine Learning as an RDF Knowledge Graph.pdf:application/pdf},
}

@article{hofer_construction_2024,
	title = {Construction of Knowledge Graphs: Current State and Challenges},
	volume = {15},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2078-2489},
	url = {https://www.mdpi.com/2078-2489/15/8/509},
	doi = {10.3390/info15080509},
	shorttitle = {Construction of Knowledge Graphs},
	abstract = {With Knowledge Graphs ({KGs}) at the center of numerous applications such as recommender systems and question-answering, the need for generalized pipelines to construct and continuously update such {KGs} is increasing. While the individual steps that are necessary to create {KGs} from unstructured sources (e.g., text) and structured data sources (e.g., databases) are mostly well researched for their one-shot execution, their adoption for incremental {KG} updates and the interplay of the individual steps have hardly been investigated in a systematic manner so far. In this work, we first discuss the main graph models for {KGs} and introduce the major requirements for future {KG} construction pipelines. Next, we provide an overview of the necessary steps to build high-quality {KGs}, including cross-cutting topics such as metadata management, ontology development, and quality assurance. We then evaluate the state of the art of {KG} construction with respect to the introduced requirements for specific popular {KGs}, as well as some recent tools and strategies for {KG} construction. Finally, we identify areas in need of further research and improvement.},
	pages = {509},
	number = {8},
	journaltitle = {Information},
	shortjournal = {Information},
	author = {Hofer, Marvin and Obraczka, Daniel and Saeedi, Alieh and Köpcke, Hanna and Rahm, Erhard},
	urldate = {2025-01-20},
	date = {2024-08-22},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ELLUHBU3\\Hofer et al. - 2024 - Construction of Knowledge Graphs Current State and Challenges.pdf:application/pdf},
}

@misc{brack_analysing_2021,
	title = {Analysing the Requirements for an Open Research Knowledge Graph: Use Cases, Quality Requirements and Construction Strategies},
	url = {http://arxiv.org/abs/2102.06021},
	doi = {10.48550/arXiv.2102.06021},
	shorttitle = {Analysing the Requirements for an Open Research Knowledge Graph},
	abstract = {Current science communication has a number of drawbacks and bottlenecks which have been subject of discussion lately: Among others, the rising number of published articles makes it nearly impossible to get a full overview of the state of the art in a certain field, or reproducibility is hampered by fixed-length, document-based publications which normally cannot cover all details of a research work. Recently, several initiatives have proposed knowledge graphs ({KG}) for organising scientific information as a solution to many of the current issues. The focus of these proposals is, however, usually restricted to very specific use cases. In this paper, we aim to transcend this limited perspective and present a comprehensive analysis of requirements for an Open Research Knowledge Graph ({ORKG}) by (a) collecting and reviewing daily core tasks of a scientist, (b) establishing their consequential requirements for a {KG}-based system, (c) identifying overlaps and specificities, and their coverage in current solutions. As a result, we map necessary and desirable requirements for successful {KG}-based science communication, derive implications, and outline possible solutions.},
	number = {{arXiv}:2102.06021},
	publisher = {{arXiv}},
	author = {Brack, Arthur and Hoppe, Anett and Stocker, Markus and Auer, Sören and Ewerth, Ralph},
	urldate = {2025-01-20},
	date = {2021-02-11},
	eprinttype = {arxiv},
	eprint = {2102.06021 [cs]},
	keywords = {Computer Science - Information Retrieval, Computer Science - Digital Libraries},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\9XV724KR\\Brack et al. - 2021 - Analysing the Requirements for an Open Research Knowledge Graph Use Cases, Quality Requirements and.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\HKENMGRY\\2102.html:text/html},
}

@incollection{sattler_cs-kg_2022,
	location = {Cham},
	title = {{CS}-{KG}: A Large-Scale Knowledge Graph of Research Entities and Claims in Computer Science},
	volume = {13489},
	isbn = {978-3-031-19433-7},
	url = {https://link.springer.com/10.1007/978-3-031-19433-7_39},
	shorttitle = {{CS}-{KG}},
	abstract = {In recent years, we saw the emergence of several approaches for producing machine-readable, semantically rich, interlinked description of the content of research publications, typically encoded as knowledge graphs. A common limitation of these solutions is that they address a low number of articles, either because they rely on human experts to summarize information from the literature or because they focus on speciﬁc research areas. In this paper, we introduce the Computer Science Knowledge Graph ({CS}-{KG}), a large-scale knowledge graph composed by over 350M {RDF} triples describing 41M statements from 6.7M articles about 10M entities linked by 179 semantic relations. It was automatically generated and will be periodically updated by applying an information extraction pipeline on a large repository of research papers. {CS}-{KG} is much larger than all comparable solutions and oﬀers a very comprehensive representation of tasks, methods, materials, and metrics in Computer Science. It can support a variety of intelligent services, such as advanced literature search, document classiﬁcation, article recommendation, trend forecasting, hypothesis generation, and many others. {CS}-{KG} was evaluated against a benchmark of manually annotated statements, yielding excellent results.},
	pages = {678--696},
	booktitle = {The Semantic Web – {ISWC} 2022},
	publisher = {Springer International Publishing},
	author = {Dessí, Danilo and Osborne, Francesco and Reforgiato Recupero, Diego and Buscaldi, Davide and Motta, Enrico},
	editor = {Sattler, Ulrike and Hogan, Aidan and Keet, Maria and Presutti, Valentina and Almeida, João Paulo A. and Takeda, Hideaki and Monnin, Pierre and Pirrò, Giuseppe and d’Amato, Claudia},
	urldate = {2025-01-21},
	date = {2022},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\K4B5QQRA\\Dessí et al. - 2022 - CS-KG A Large-Scale Knowledge Graph of Research Entities and Claims in Computer Science.pdf:application/pdf},
}

@article{stocker_skg4eosc_2022,
	title = {{SKG}4EOSC - Scholarly Knowledge Graphs for {EOSC}: Establishing a backbone of knowledge graphs for {FAIR} Scholarly Information in {EOSC}},
	volume = {8},
	issn = {2367-7163},
	url = {https://riojournal.com/article/83789/},
	doi = {10.3897/rio.8.e83789},
	shorttitle = {{SKG}4EOSC - Scholarly Knowledge Graphs for {EOSC}},
	abstract = {In the age of advanced information systems powering fast-paced knowledge economies that face global societal challenges, it is no longer adequate to express scholarly information - an essential resource for modern economies - primarily as article narratives in document form. Despite being a well-established tradition in scholarly communication, {PDF}-based text publishing is hindering scientific progress as it buries scholarly information into non-machine-readable formats. The key objective of {SKG}4EOSC is to improve science productivity through development and implementation of services for text and data conversion, and production, curation, and re-use of {FAIR} scholarly information. This will be achieved by (1) establishing the Open Research Knowledge Graph ({ORKG}, orkg.org), a service operated by the {SKG}4EOSC coordinator, as a Hub for access to {FAIR} scholarly information in the {EOSC}; (2) lifting to {EOSC} of numerous and heterogeneous domain-specific research infrastructures through the {ORKG} Hub’s harmonized access facilities; and (3) leverage the Hub to support cross-disciplinary research and policy decisions addressing societal challenges. {SKG}4EOSC will pilot the devised approaches and technologies in four research domains: biodiversity crisis, precision oncology, circular processes, and human cooperation. With the aim to improve machine-based scholarly information use, {SKG}4EOSC addresses an important current and future need of researchers. It extends the application of the {FAIR} data principles to scholarly communication practices, hence a more comprehensive coverage of the entire research lifecycle. Through explicit, machine actionable provenance links between {FAIR} scholarly information, primary data and contextual entities, it will substantially contribute to reproducibility, validation and trust in science. The resulting advanced machine support will catalyse new discoveries in basic research and solutions in key application areas.},
	pages = {e83789},
	journaltitle = {Research Ideas and Outcomes},
	shortjournal = {{RIO}},
	author = {Stocker, Markus and Heger, Tina and Schweidtmann, Artur and Ćwiek-Kupczyńska, Hanna and Penev, Lyubomir and Dojchinovski, Milan and Willighagen, Egon and Vidal, Maria-Esther and Turki, Houcemeddine and Balliet, Daniel and Tiddi, Ilaria and Kuhn, Tobias and Mietchen, Daniel and Karras, Oliver and Vogt, Lars and Hellmann, Sebastian and Jeschke, Jonathan and Krajewski, Paweł and Auer, Sören},
	urldate = {2025-01-21},
	date = {2022-03-15},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\J8XMBBUU\\Stocker et al. - 2022 - SKG4EOSC - Scholarly Knowledge Graphs for EOSC Establishing a backbone of knowledge graphs for FAIR.pdf:application/pdf},
}

@online{noauthor_pdf_nodate,
	title = {[{PDF}] Data integration and disintegration: Managing Springer Nature {SciGraph} with {SHACL} and {OWL} {\textbar} Semantic Scholar},
	url = {https://www.semanticscholar.org/paper/Data-integration-and-disintegration%3A-Managing-with-Hammond-Pasin/1eb9d54fac20963a6050db178865579ff4564833},
	urldate = {2025-01-21},
}

@inproceedings{hammond_data_2017,
	title = {Data integration and disintegration: Managing Springer Nature {SciGraph} with {SHACL} and {OWL}},
	booktitle = {International Workshop on the Semantic Web},
	author = {Hammond, Tony and Pasin, Michele and Theodoridis, Evangelos},
	date = {2017},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\6FSLXY5M\\Abbassi und Faiz - 2016 - RDF-4X a scalable solution for RDF quads store in the cloud.pdf:application/pdf},
}

@inproceedings{manghi_openaire_2019,
	title = {The {OpenAIRE} Research Graph Data Model},
	rights = {Creative Commons Attribution 4.0 International, Open Access},
	url = {https://zenodo.org/record/2643199},
	doi = {10.5281/ZENODO.2643199},
	abstract = {The purpose of the European {OpenAIRE} infrastructure is  to facilitate, foster, support, and monitor Open Science scholarly communication in Europe. The  infrastructure has been operational for almost a decade and successful in linking people,  ideas and resources in support of the free flow, access, sharing, and re-use of research outcomes. To this aim it offers dissemination and training on Open Access and Open Science, facilitates exchange of knowledge, and operates the  technical services required to facilitate and monitor Open Science publishing trends and research impact across geographic and discipline boundaries. {OpenAIRE} services populate a research graph whose objects are scientific results, organizations, funders, communities, organizations, and data sources. In this article we describe the data model, inspired by several existing metadata standards.},
	publisher = {Zenodo},
	author = {Manghi, Paolo and Bardi, Alessia and Atzori, Claudio and Baglioni, Miriam and Manola, Natalia and Schirrwagen, Jochen and Principe, Pedro},
	urldate = {2025-01-21},
	date = {2019-04-17},
	langid = {english},
	doi = {10.5281/ZENODO.2643199},
	note = {Version Number: 1.3},
	keywords = {Open Science, Monitoring, {OpenAIRE}, Research Graph},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\AW46CM84\\Manghi et al. - 2019 - The OpenAIRE Research Graph Data Model.pdf:application/pdf},
}

@inproceedings{aryani_research_2017,
	title = {Research Graph: Building a Distributed Graph of Scholarly Works using Research Data Switchboard},
	rights = {Creative Commons Attribution 4.0 International},
	doi = {10.4225/03/58C696655AF8A},
	abstract = {This is the pre-print version of a paper accepted in Open Repository Conference in Brisbane, Australia, June 2017.{\textless}br{\textgreater}\textbf{Abstract:}{\textless}br{\textgreater}In this paper, we discuss an open collaborative project called Research Graph derived from the outcome of the Research Data Alliance ({RDA}) working group on Data Description Registry Interoperability. This project addresses the problem of connecting scholarly works across heterogeneous systems. The {RDA} working group recommendation provided a solution for connecting publications and research data (data in research) across multiple open access repositories using co-authorship model and jointly funded research projects. Research Graph adopts and extends this work by creating a distributed graph that connects open access repositories to close research management systems traditionally locked behind the firewall. In addition, the distributed graph addresses the challenge of scalability and enables individual universities and repositories to hold a small and manageable graph and synthesis this graph with trusted partner organisations. {\textless}br{\textgreater}},
	publisher = {Monash University},
	author = {Aryani, Amir and Wang, Jingbo},
	date = {2017},
	note = {Artwork Size: 505565 Bytes},
	keywords = {{FOS}: Computer and information sciences, Information Systems},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\A5655D8P\\Aryani und Wang - 2017 - Research Graph Building a Distributed Graph of Scholarly Works using Research Data Switchboard.pdf:application/pdf},
}

@article{domingo-fernandez_covid-19_2021,
	title = {{COVID}-19 Knowledge Graph: a computable, multi-modal, cause-and-effect knowledge model of {COVID}-19 pathophysiology},
	volume = {37},
	rights = {http://creativecommons.org/licenses/by-nc/4.0/},
	issn = {1367-4803, 1367-4811},
	doi = {10.1093/bioinformatics/btaa834},
	abstract = {Abstract
            
              Summary
              The {COVID}-19 crisis has elicited a global response by the scientific community that has led to a burst of publications on the pathophysiology of the virus. However, without coordinated efforts to organize this knowledge, it can remain hidden away from individual research groups. By extracting and formalizing this knowledge in a structured and computable form, as in the form of a knowledge graph, researchers can readily reason and analyze this information on a much larger scale. Here, we present the {COVID}-19 Knowledge Graph, an expansive cause-and-effect network constructed from scientific literature on the new coronavirus that aims to provide a comprehensive view of its pathophysiology. To make this resource available to the research community and facilitate its exploration and analysis, we also implemented a web application and released the {KG} in multiple standard formats.
            
            
              Availability and implementation
              The {COVID}-19 Knowledge Graph is publicly available under {CC}-0 license at https://github.com/covid19kg and https://bikmi.covid19-knowledgespace.de.
            
            
              Supplementary information
              Supplementary data are available at Bioinformatics online.},
	pages = {1332--1334},
	number = {9},
	journaltitle = {Bioinformatics},
	author = {Domingo-Fernández, Daniel and Baksi, Shounak and Schultz, Bruce and Gadiya, Yojana and Karki, Reagon and Raschka, Tamara and Ebeling, Christian and Hofmann-Apitius, Martin and Kodamullil, Alpha Tom},
	editor = {Cowen, Lenore},
	date = {2021-06-09},
	langid = {english},
	file = {Volltext:C\:\\Users\\Marco\\Zotero\\storage\\BP78ZNBS\\Domingo-Fernández et al. - 2021 - COVID-19 Knowledge Graph a computable, multi-modal, cause-and-effect knowledge model of COVID-19 pa.pdf:application/pdf},
}

@article{harth_investigating_2020,
	title = {Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach},
	volume = {12123},
	doi = {10.1007/978-3-030-49461-2_16},
	abstract = {Knowledge about the software used in scientific investigations is necessary for different reasons, including provenance of the results, measuring software impact to attribute developers, and bibliometric software citation analysis in general. Additionally, providing information about whether and how the software and the source code are available allows an assessment about the state and role of open source software in science in general. While such analyses can be done manually, large scale analyses require the application of automated methods of information extraction and linking. In this paper, we present {SoftwareKG}—a knowledge graph that contains information about software mentions from more than 51,000 scientific articles from the social sciences. A silver standard corpus, created by a distant and weak supervision approach, and a gold standard corpus, created by manual annotation, were used to train an {LSTM} based neural network to identify software mentions in scientific articles. The model achieves a recognition rate of .82 F-score in exact matches. As a result, we identified more than 133,000 software mentions. For entity disambiguation, we used the public domain knowledge base {DBpedia}. Furthermore, we linked the entities of the knowledge graph to other knowledge bases such as the Microsoft Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we illustrate, how {SoftwareKG} can be used to assess the role of software in the social sciences.},
	pages = {271--286},
	author = {Schindler, David and Zapilko, Benjamin and Krüger, Frank},
	editor = {Harth, Andreas and Kirrane, Sabrina and Ngonga Ngomo, Axel-Cyrille and Paulheim, Heiko and Rula, Anisa and Gentile, Anna Lisa and Haase, Peter and Cochez, Michael},
	date = {2020},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\AAZKKYEW\\Schindler et al. - 2020 - Investigating Software Usage in the Social Sciences A Knowledge Graph Approach.pdf:application/pdf},
}

@article{spadaro_cooperation_2022,
	title = {The Cooperation Databank: Machine-Readable Science Accelerates Research Synthesis},
	volume = {17},
	issn = {1745-6916, 1745-6924},
	doi = {10.1177/17456916211053319},
	shorttitle = {The Cooperation Databank},
	abstract = {Publishing studies using standardized, machine-readable formats will enable machines to perform meta-analyses on demand. To build a semantically enhanced technology that embodies these functions, we developed the Cooperation Databank ({CoDa})—a databank that contains 2,636 studies on human cooperation (1958–2017) conducted in 78 societies involving 356,283 participants. Experts annotated these studies along 312 variables, including the quantitative results (13,959 effects). We designed an ontology that defines and relates concepts in cooperation research and that can represent the relationships between results of correlational and experimental studies. We have created a research platform that, given the data set, enables users to retrieve studies that test the relation of variables with cooperation, visualize these study results, and perform (a) meta-analyses, (b) metaregressions, (c) estimates of publication bias, and (d) statistical power analyses for future studies. We leveraged the data set with visualization tools that allow users to explore the ontology of concepts in cooperation research and to plot a citation network of the history of studies. {CoDa} offers a vision of how publishing studies in a machine-readable format can establish institutions and tools that improve scientific practices and knowledge.},
	pages = {1472--1489},
	number = {5},
	journaltitle = {Perspectives on Psychological Science},
	shortjournal = {Perspect Psychol Sci},
	author = {Spadaro, Giuliana and Tiddi, Ilaria and Columbus, Simon and Jin, Shuxian and Ten Teije, Annette and {CoDa Team} and Balliet, Daniel},
	date = {2022-09},
	langid = {english},
	file = {Volltext:C\:\\Users\\Marco\\Zotero\\storage\\YXTFE5WF\\Spadaro et al. - 2022 - The Cooperation Databank Machine-Readable Science Accelerates Research Synthesis.pdf:application/pdf},
}

@article{penev_openbiodiv_2019,
	title = {{OpenBiodiv}: A Knowledge Graph for Literature-Extracted Linked Open Data in Biodiversity Science},
	volume = {7},
	rights = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2304-6775},
	doi = {10.3390/publications7020038},
	shorttitle = {{OpenBiodiv}},
	abstract = {Hundreds of years of biodiversity research have resulted in the accumulation of a substantial pool of communal knowledge; however, most of it is stored in silos isolated from each other, such as published articles or monographs. The need for a system to store and manage collective biodiversity knowledge in a community-agreed and interoperable open format has evolved into the concept of the Open Biodiversity Knowledge Management System ({OBKMS}). This paper presents {OpenBiodiv}: An {OBKMS} that utilizes semantic publishing workflows, text and data mining, common standards, ontology modelling and graph database technologies to establish a robust infrastructure for managing biodiversity knowledge. It is presented as a Linked Open Dataset generated from scientific literature. {OpenBiodiv} encompasses data extracted from more than 5000 scholarly articles published by Pensoft and many more taxonomic treatments extracted by Plazi from journals of other publishers. The data from both sources are converted to Resource Description Framework ({RDF}) and integrated in a graph database using the {OpenBiodiv}-O ontology and an {RDF} version of the Global Biodiversity Information Facility ({GBIF}) taxonomic backbone. Through the application of semantic technologies, the project showcases the value of open publishing of Findable, Accessible, Interoperable, Reusable ({FAIR}) data towards the establishment of open science practices in the biodiversity domain.},
	pages = {38},
	number = {2},
	journaltitle = {Publications},
	shortjournal = {Publications},
	author = {Penev, Lyubomir and Dimitrova, Mariya and Senderov, Viktor and Zhelezov, Georgi and Georgiev, Teodor and Stoev, Pavel and Simov, Kiril},
	date = {2019-05-29},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\8D2BW5SJ\\Penev et al. - 2019 - OpenBiodiv A Knowledge Graph for Literature-Extracted Linked Open Data in Biodiversity Science.pdf:application/pdf},
}

@inproceedings{groth_conference_2016,
	location = {Cham},
	title = {Conference Linked Data: The {ScholarlyData} Project},
	volume = {9982},
	isbn = {978-3-319-46546-3 978-3-319-46547-0},
	url = {https://link.springer.com/10.1007/978-3-319-46547-0_16},
	doi = {10.1007/978-3-319-46547-0_16},
	shorttitle = {Conference Linked Data},
	abstract = {The Semantic Web Dog Food ({SWDF}) is the reference linked dataset of the Semantic Web community about papers, people, organisations, and events related to its academic conferences. In this paper we analyse the existing problems of generating, representing and maintaining Linked Data for the {SWDF}. With this work (i) we provide a refactored and cleaned {SWDF} dataset; (ii) we use a novel data model which improves the Semantic Web Conference Ontology, adopting best ontology design practices and (iii) we provide an open source workflow to support a healthy growth of the dataset beyond the Semantic Web conferences.},
	pages = {150--158},
	publisher = {Springer International Publishing},
	author = {Nuzzolese, Andrea Giovanni and Gentile, Anna Lisa and Presutti, Valentina and Gangemi, Aldo},
	editor = {Groth, Paul and Simperl, Elena and Gray, Alasdair and Sabou, Marta and Krötzsch, Markus and Lecue, Freddy and Flöck, Fabian and Gil, Yolanda},
	urldate = {2025-01-21},
	date = {2016},
	langid = {english},
	doi = {10.1007/978-3-319-46547-0_16},
	note = {Book Title: The Semantic Web – {ISWC} 2016
Series Title: Lecture Notes in Computer Science},
}

@article{peroni_opencitations_2020,
	title = {{OpenCitations}, an infrastructure organization for open scholarship},
	volume = {1},
	issn = {2641-3337},
	url = {https://direct.mit.edu/qss/article/1/1/428-444/15580},
	doi = {10.1162/qss_a_00023},
	abstract = {{OpenCitations} is an infrastructure organization for open scholarship dedicated to the publication of open citation data as Linked Open Data using Semantic Web technologies, thereby providing a disruptive alternative to traditional proprietary citation indexes. Open citation data are valuable for bibliometric analysis, increasing the reproducibility of large-scale analyses by enabling publication of the source data. Following brief introductions to the development and benefits of open scholarship and to Semantic Web technologies, this paper describes {OpenCitations} and its data sets, tools, services, and activities. These include the {OpenCitations} Data Model; the {SPAR} (Semantic Publishing and Referencing) Ontologies; {OpenCitations}’ open software of generic applicability for searching, browsing, and providing {REST} {APIs} over resource description framework ({RDF}) triplestores; Open Citation Identifiers ({OCIs}) and the {OpenCitations} {OCI} Resolution Service; the {OpenCitations} Corpus ({OCC}), a database of open downloadable bibliographic and citation data made available in {RDF} under a Creative Commons public domain dedication; and the {OpenCitations} Indexes of open citation data, of which the first and largest is {COCI}, the {OpenCitations} Index of Crossref Open {DOI}-to-{DOI} Citations, which currently contains over 624 million bibliographic citations and is receiving considerable usage by the scholarly community.},
	pages = {428--444},
	number = {1},
	journaltitle = {Quantitative Science Studies},
	shortjournal = {Quantitative Science Studies},
	author = {Peroni, Silvio and Shotton, David},
	urldate = {2025-01-21},
	date = {2020-02},
	langid = {english},
	file = {Volltext:C\:\\Users\\Marco\\Zotero\\storage\\HGM5AAV9\\Peroni und Shotton - 2020 - OpenCitations, an infrastructure organization for open scholarship.pdf:application/pdf},
}

@thesis{banerjee_knowledge_2024,
	location = {Germany},
	title = {Knowledge Graph Question Answering With Generative Language Models},
	rights = {Database copyright {ProQuest} {LLC}; {ProQuest} does not claim copyright in the individual underlying works; This work is published under[ https://creativecommons.org/licenses/by/4.0/{\textbar}https://creativecommons.org/licenses/by/4.0/]  (the "License"). Notwithstanding the {ProQuest} Terms and Conditions},
	url = {https://www.proquest.com/docview/3143985996/abstract/3893F0C5845D4D1DPQ/1},
	abstract = {A Knowledge Graph ({KG}) is a data structure that stores information about the world in the form of nodes and edges. The nodes represent people, places, things etc., while the edges store the relationships between the nodes. The nodes are also known as entities, while the edges are known as relations or predicates. Several popular search engines today make use of such {KGs} in the background. Some well-known and freely available {KGs} are {DBpedia} and Wikidata.
One way to access information from a {KG} is through Question Answering. For example, web-based search engines today give people the ability to type their questions and receive answers. Unfortunately, the current state of search engines leaves much to be desired in the complexity of the questions that a user may type. Current search engines work best when the search term is a keyword or a set of words. Processing complete sentences, with complex logical rules, is still an open problem.
One large step in the direction of language understanding has been the arrival of pre-trained Language Models, such as {BERT}. Such models have been trained on large amounts of text corpus, and surprisingly, some variants of these models, such as T5 and {BART}, develop a remarkable ability to generate text, the likes of which are difficult to distinguish from that produced by a human author. These models are also called generative Language Models and are a central focus of this thesis.
Given a question by a user, how does one fetch an answer from the {KG}? This task is commonly known as Knowledge Graph Question Answering ({KGQA}). One of the techniques is to convert the user’s question to a logical form, or a structured query. One popular query language the reader might be familiar with is {SQL}. {SQL}, though, is appropriate for relational databases. In the {KG} world, the analogue would be a language called {SPARQL}. The task of converting the natural language text to a logical form is known as semantic parsing.
To be able to execute a {SPARQL} query on a {KG}, the {SPARQL} schema must be valid, e.g., it must be syntactically correct, and it should be logically correct, e.g., one can not expect the correct answer if {AND} is replaced with an {OR}. The other requirement is that the constants in the query, such as entity and relation {IDs}, have to be placed in the correct manner in the query. In this thesis, we explore the abilities of generative Language Models ({LMs}) in the task of {KGQA},with a focus on the semantic parsing approach.
This dissertation hypothesizes that generative {LMs} can be used effectively for the task of {KGQA}. We form two research questions over this hypothesis, and to answer these research questions, a series of five topically interconnected publications are presented in a cumulative fashion. We first test two popular generative {LMs} on the task of semantic parsing. We compare the performance of these models to their non-pre-trained predecessors. To this end, we utilize some well-known and openly available datasets, which address well-known {KGs}. Later, we develop methods to improve the default performance of such models on this task.},
	pagetotal = {132},
	institution = {Universitaet Hamburg (Germany)},
	type = {phdthesis},
	author = {Banerjee, Debayan},
	urldate = {2025-01-21},
	date = {2024},
	note = {{ISBN}: 9798346770350},
	keywords = {Adaptation, Error analysis, Logic, Mathematics, Search engines, Semantics, Web studies},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ACAAFSEE\\Banerjee - 2024 - Knowledge Graph Question Answering With Generative Language Models.pdf:application/pdf},
}

@misc{iv_baracks_2019,
	title = {Barack's Wife Hillary: Using Knowledge-Graphs for Fact-Aware Language Modeling},
	url = {http://arxiv.org/abs/1906.07241},
	doi = {10.48550/arXiv.1906.07241},
	shorttitle = {Barack's Wife Hillary},
	abstract = {Modeling human language requires the ability to not only generate fluent text but also encode factual knowledge. However, traditional language models are only capable of remembering facts seen at training time, and often have difficulty recalling them. To address this, we introduce the knowledge graph language model ({KGLM}), a neural language model with mechanisms for selecting and copying facts from a knowledge graph that are relevant to the context. These mechanisms enable the model to render information it has never seen before, as well as generate out-of-vocabulary tokens. We also introduce the Linked {WikiText}-2 dataset, a corpus of annotated text aligned to the Wikidata knowledge graph whose contents (roughly) match the popular {WikiText}-2 benchmark. In experiments, we demonstrate that the {KGLM} achieves significantly better performance than a strong baseline language model. We additionally compare different language model's ability to complete sentences requiring factual knowledge, showing that the {KGLM} outperforms even very large language models in generating facts.},
	number = {{arXiv}:1906.07241},
	publisher = {{arXiv}},
	author = {{IV}, Robert L. Logan and Liu, Nelson F. and Peters, Matthew E. and Gardner, Matt and Singh, Sameer},
	urldate = {2025-01-21},
	date = {2019-06-20},
	eprinttype = {arxiv},
	eprint = {1906.07241 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\THUMGUSS\\IV et al. - 2019 - Barack's Wife Hillary Using Knowledge-Graphs for Fact-Aware Language Modeling.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\T7TNSEIW\\1906.html:text/html},
}

@misc{li_graph_2023,
	title = {Graph Reasoning for Question Answering with Triplet Retrieval},
	url = {http://arxiv.org/abs/2305.18742},
	doi = {10.48550/arXiv.2305.18742},
	abstract = {Answering complex questions often requires reasoning over knowledge graphs ({KGs}). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into {KG} encoder, e.g. graph neural networks ({GNNs}), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in {KGs} that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from {KGs} and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both {CommonsenseQA} and {OpenbookQA} datasets show that our method can outperform state-of-the-art up to 4.6\% absolute accuracy.},
	number = {{arXiv}:2305.18742},
	publisher = {{arXiv}},
	author = {Li, Shiyang and Gao, Yifan and Jiang, Haoming and Yin, Qingyu and Li, Zheng and Yan, Xifeng and Zhang, Chao and Yin, Bing},
	urldate = {2025-01-21},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2305.18742 [cs]},
	keywords = {finished, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\LCFZ22UF\\Li et al. - 2023 - Graph Reasoning for Question Answering with Triplet Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\HXPTC633\\2305.html:text/html},
}

@misc{wen_mindmap_2024,
	title = {{MindMap}: Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models},
	doi = {10.48550/arXiv.2308.09729},
	shorttitle = {{MindMap}},
	abstract = {Large language models ({LLMs}) have achieved remarkable performance in natural language understanding and generation tasks. However, they often suffer from limitations such as difficulty in incorporating new knowledge, generating hallucinations, and explaining their reasoning process. To address these challenges, we propose a novel prompting pipeline, named {\textbackslash}method, that leverages knowledge graphs ({KGs}) to enhance {LLMs}' inference and transparency. Our method enables {LLMs} to comprehend {KG} inputs and infer with a combination of implicit and external knowledge. Moreover, our method elicits the mind map of {LLMs}, which reveals their reasoning pathways based on the ontology of knowledge. We evaluate our method on diverse question {\textbackslash}\& answering tasks, especially in medical domains, and show significant improvements over baselines. We also introduce a new hallucination evaluation benchmark and analyze the effects of different components of our method. Our results demonstrate the effectiveness and robustness of our method in merging knowledge from {LLMs} and {KGs} for combined inference. To reproduce our results and extend the framework further, we make our codebase available at https://github.com/wyl-willing/{MindMap}.},
	number = {{arXiv}:2308.09729},
	publisher = {{arXiv}},
	author = {Wen, Yilin and Wang, Zifeng and Sun, Jimeng},
	date = {2024-03-02},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, finished, related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\3S7UACN4\\Wen et al. - 2024 - MindMap Knowledge Graph Prompting Sparks Graph of Thoughts in Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\RUHACJZL\\2308.html:text/html},
}

@misc{luo_chatrule_2024,
	title = {{ChatRule}: Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning},
	url = {http://arxiv.org/abs/2309.01538},
	doi = {10.48550/arXiv.2309.01538},
	shorttitle = {{ChatRule}},
	abstract = {Logical rules are essential for uncovering the logical connections between relations, which could improve reasoning performance and provide interpretable results on knowledge graphs ({KGs}). Although there have been many efforts to mine meaningful logical rules over {KGs}, existing methods suffer from computationally intensive searches over the rule space and a lack of scalability for large-scale {KGs}. Besides, they often ignore the semantics of relations which is crucial for uncovering logical connections. Recently, large language models ({LLMs}) have shown impressive performance in the field of natural language processing and various applications, owing to their emergent ability and generalizability. In this paper, we propose a novel framework, {ChatRule}, unleashing the power of large language models for mining logical rules over knowledge graphs. Specifically, the framework is initiated with an {LLM}-based rule generator, leveraging both the semantic and structural information of {KGs} to prompt {LLMs} to generate logical rules. To refine the generated rules, a rule ranking module estimates the rule quality by incorporating facts from existing {KGs}. Last, the ranked rules can be used to conduct reasoning over {KGs}. {ChatRule} is evaluated on four large-scale {KGs}, w.r.t. different rule quality metrics and downstream tasks, showing the effectiveness and scalability of our method.},
	number = {{arXiv}:2309.01538},
	publisher = {{arXiv}},
	author = {Luo, Linhao and Ju, Jiaxin and Xiong, Bo and Li, Yuan-Fang and Haffari, Gholamreza and Pan, Shirui},
	urldate = {2025-01-21},
	date = {2024-01-22},
	eprinttype = {arxiv},
	eprint = {2309.01538 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, not related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\2QMHE5RZ\\Luo et al. - 2024 - ChatRule Mining Logical Rules with Large Language Models for Knowledge Graph Reasoning.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\XQC98LU8\\2309.html:text/html},
}

@misc{wang_boosting_2024,
	title = {Boosting Language Models Reasoning with Chain-of-Knowledge Prompting},
	url = {http://arxiv.org/abs/2306.06427},
	doi = {10.48550/arXiv.2306.06427},
	abstract = {Recently, Chain-of-Thought ({CoT}) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like ``Let's think step by step'' or multiple in-context exemplars with well-designed rationales to elicit Large Language Models ({LLMs}) to generate intermediate reasoning steps. However, the generated rationales often come with mistakes, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge ({CoK}) prompting, where we aim at eliciting {LLMs} to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from {CoK}, we additionally introduce a F{\textasciicircum}2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the {LLM} to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.},
	number = {{arXiv}:2306.06427},
	publisher = {{arXiv}},
	author = {Wang, Jianing and Sun, Qiushi and Li, Xiang and Gao, Ming},
	urldate = {2025-01-21},
	date = {2024-06-03},
	eprinttype = {arxiv},
	eprint = {2306.06427 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\P3MWV3IM\\Wang et al. - 2024 - Boosting Language Models Reasoning with Chain-of-Knowledge Prompting.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\Q8PW8RU6\\2306.html:text/html},
}

@inproceedings{huang_mvp-tuning_2023,
	location = {Toronto, Canada},
	title = {{MVP}-Tuning: Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning},
	url = {https://aclanthology.org/2023.acl-long.750/},
	doi = {10.18653/v1/2023.acl-long.750},
	shorttitle = {{MVP}-Tuning},
	abstract = {Recent advances in pre-trained language models ({PLMs}) have facilitated the development ofcommonsense reasoning tasks. However, existing methods rely on multi-hop knowledgeretrieval and thus suffer low accuracy due toembedded noise in the acquired knowledge. In addition, these methods often attain highcomputational costs and nontrivial knowledgeloss because they encode the knowledge independently of the {PLM}, making it less relevant to the task and thus resulting in a poorlocal optimum. In this work, we propose {MultiView} Knowledge Retrieval with Prompt Tuning ({MVP}-Tuning). {MVP}-Tuning leveragessimilar question-answer pairs in the training setto improve knowledge retrieval and employsa single prompt-tuned {PLM} to model knowledge and input text jointly. We conduct our experiments on five commonsense reasoning {QAbenchmarks} to show that {MVP}-Tuning outperforms all other baselines in 4 out of 5 datasetswith less than 2\% trainable parameters. {MVPTuning} even gets a new state-of-the-art resulton {OpenBookQA} and is number one on theleaderboard.},
	eventtitle = {{ACL} 2023},
	pages = {13417--13432},
	booktitle = {Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Huang, Yongfeng and Li, Yanyang and Xu, Yichong and Zhang, Lin and Gan, Ruyi and Zhang, Jiaxing and Wang, Liwei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	urldate = {2025-01-22},
	date = {2023-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\58TVGBE4\\Huang et al. - 2023 - MVP-Tuning Multi-View Knowledge Retrieval with Prompt Tuning for Commonsense Reasoning.pdf:application/pdf},
}

@misc{li_unioqa_2024,
	title = {{UniOQA}: A Unified Framework for Knowledge Graph Question Answering with Large Language Models},
	url = {http://arxiv.org/abs/2406.02110},
	doi = {10.48550/arXiv.2406.02110},
	shorttitle = {{UniOQA}},
	abstract = {{OwnThink} stands as the most extensive Chinese open-domain knowledge graph introduced in recent times. Despite prior attempts in question answering over {OwnThink} ({OQA}), existing studies have faced limitations in model representation capabilities, posing challenges in further enhancing overall accuracy in question answering. In this paper, we introduce {UniOQA}, a unified framework that integrates two complementary parallel workflows. Unlike conventional approaches, {UniOQA} harnesses large language models ({LLMs}) for precise question answering and incorporates a direct-answer-prediction process as a cost-effective complement. Initially, to bolster representation capacity, we fine-tune an {LLM} to translate questions into the Cypher query language ({CQL}), tackling issues associated with restricted semantic understanding and hallucinations. Subsequently, we introduce the Entity and Relation Replacement algorithm to ensure the executability of the generated {CQL}. Concurrently, to augment overall accuracy in question answering, we further adapt the Retrieval-Augmented Generation ({RAG}) process to the knowledge graph. Ultimately, we optimize answer accuracy through a dynamic decision algorithm. Experimental findings illustrate that {UniOQA} notably advances {SpCQL} Logical Accuracy to 21.2\% and Execution Accuracy to 54.9\%, achieving the new state-of-the-art results on this benchmark. Through ablation experiments, we delve into the superior representation capacity of {UniOQA} and quantify its performance breakthrough.},
	number = {{arXiv}:2406.02110},
	publisher = {{arXiv}},
	author = {Li, Zhuoyang and Deng, Liran and Liu, Hui and Liu, Qiaoqiao and Du, Junzhao},
	urldate = {2025-01-22},
	date = {2024-06-04},
	eprinttype = {arxiv},
	eprint = {2406.02110 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\CP3S2WCD\\Li et al. - 2024 - UniOQA A Unified Framework for Knowledge Graph Question Answering with Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3A4AVK7T\\2406.html:text/html},
}

@inproceedings{kaplan_introducing_2022,
	location = {New York, {NY}, {USA}},
	title = {Introducing an Evaluation Method for Taxonomies},
	isbn = {978-1-4503-9613-4},
	doi = {10.1145/3530019.3535305},
	series = {{EASE} '22},
	abstract = {Background: Taxonomies are crucial for the development of a research field, as they play a major role in structuring a complex body of knowledge and help to classify processes, approaches, and solutions. While there is an increasing interest in taxonomies in the software engineering ({SE}) research field, we observe that {SE} taxonomies are rarely evaluated. Aim: To raise awareness and provide operational guidance on how to evaluate a taxonomy, this paper presents a three step evaluation method evaluating its structure, applicability, and purpose. Method: To show the feasibility and applicability of our approach, we provide a running example and additionally illustrate our approach to a practical case study in {SE} research. Results and Conclusion: Our method with operational guidance enables {SE} researchers to systematically evaluate and improve the quality of their taxonomies and support reviewers to systematically assess a taxonomy’s quality.},
	pages = {311--316},
	booktitle = {Proceedings of the 26th International Conference on Evaluation and Assessment in Software Engineering},
	publisher = {Association for Computing Machinery},
	author = {Kaplan, Angelika and Kühn, Thomas and Hahner, Sebastian and Benkler, Niko and Keim, Jan and Fuchß, Dominik and Corallo, Sophie and Heinrich, Robert},
	date = {2022-06-13},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\FH44Q2RG\\Kaplan et al. - 2022 - Introducing an Evaluation Method for Taxonomies.pdf:application/pdf},
}

@inproceedings{zhang_pretrain-kge_2020,
	location = {Online},
	title = {Pretrain-{KGE}: Learning Knowledge Representation from Pretrained Language Models},
	doi = {10.18653/v1/2020.findings-emnlp.25},
	shorttitle = {Pretrain-{KGE}},
	abstract = {Conventional knowledge graph embedding ({KGE}) often suffers from limited knowledge representation, leading to performance degradation especially on the low-resource problem. To remedy this, we propose to enrich knowledge representation via pretrained language models by leveraging world knowledge from pretrained models. Specifically, we present a universal training framework named Pretrain-{KGE} consisting of three phases: semantic-based fine-tuning phase, knowledge extracting phase and {KGE} training phase. Extensive experiments show that our proposed Pretrain-{KGE} can improve results over {KGE} models, especially on solving the low-resource problem.},
	eventtitle = {Findings 2020},
	pages = {259--266},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Zhang, Zhiyuan and Liu, Xiaoqian and Zhang, Yi and Su, Qi and Sun, Xu and He, Bin},
	editor = {Cohn, Trevor and He, Yulan and Liu, Yang},
	date = {2020-11},
	keywords = {finished, Training},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\MUQEVQAX\\Zhang et al. - 2020 - Pretrain-KGE Learning Knowledge Representation from Pretrained Language Models.pdf:application/pdf},
}

@misc{wang_kepler_2020,
	title = {{KEPLER}: A Unified Model for Knowledge Embedding and Pre-trained Language Representation},
	doi = {10.48550/arXiv.1911.06136},
	shorttitle = {{KEPLER}},
	abstract = {Pre-trained language representation models ({PLMs}) cannot well capture factual knowledge from text. In contrast, knowledge embedding ({KE}) methods can effectively represent the relational facts in knowledge graphs ({KGs}) with informative entity embeddings, but conventional {KE} models cannot take full advantage of the abundant textual information. In this paper, we propose a unified model for Knowledge Embedding and Pre-trained {LanguagE} Representation ({KEPLER}), which can not only better integrate factual knowledge into {PLMs} but also produce effective text-enhanced {KE} with the strong {PLMs}. In {KEPLER}, we encode textual entity descriptions with a {PLM} as their embeddings, and then jointly optimize the {KE} and language modeling objectives. Experimental results show that {KEPLER} achieves state-of-the-art performances on various {NLP} tasks, and also works remarkably well as an inductive {KE} model on {KG} link prediction. Furthermore, for pre-training and evaluating {KEPLER}, we construct Wikidata5M, a large-scale {KG} dataset with aligned entity descriptions, and benchmark state-of-the-art {KE} methods on it. It shall serve as a new {KE} benchmark and facilitate the research on large {KG}, inductive {KE}, and {KG} with text. The source code can be obtained from https://github.com/{THU}-{KEG}/{KEPLER}.},
	number = {{arXiv}:1911.06136},
	publisher = {{arXiv}},
	author = {Wang, Xiaozhi and Gao, Tianyu and Zhu, Zhaocheng and Zhang, Zhengyan and Liu, Zhiyuan and Li, Juanzi and Tang, Jian},
	date = {2020-11-23},
	eprinttype = {arxiv},
	eprint = {1911.06136 [cs]},
	keywords = {Computer Science - Computation and Language, finished, Training},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\9KXDRAS7\\Wang et al. - 2020 - KEPLER A Unified Model for Knowledge Embedding and Pre-trained Language Representation.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\26GUFHJR\\1911.html:text/html},
}

@misc{nayyeri_integrating_2023,
	title = {Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces},
	doi = {10.48550/arXiv.2208.02743},
	abstract = {Knowledge Graphs, such as Wikidata, comprise structural and textual knowledge in order to represent knowledge. For each of the two modalities dedicated approaches for graph embedding and language models learn patterns that allow for predicting novel structural knowledge. Few approaches have integrated learning and inference with both modalities and these existing ones could only partially exploit the interaction of structural and textual knowledge. In our approach, we build on existing strong representations of single modalities and we use hypercomplex algebra to represent both, (i), single-modality embedding as well as, (ii), the interaction between different modalities and their complementary means of knowledge representation. More specifically, we suggest Dihedron and Quaternion representations of 4D hypercomplex numbers to integrate four modalities namely structural knowledge graph embedding, word-level representations (e.g.{\textbackslash} Word2vec, Fasttext), sentence-level representations (Sentence transformer), and document-level representations (sentence transformer, Doc2vec). Our unified vector representation scores the plausibility of labelled edges via Hamilton and Dihedron products, thus modeling pairwise interactions between different modalities. Extensive experimental evaluation on standard benchmark datasets shows the superiority of our two new models using abundant textual information besides sparse structural knowledge to enhance performance in link prediction tasks.},
	number = {{arXiv}:2208.02743},
	publisher = {{arXiv}},
	author = {Nayyeri, Mojtaba and Wang, Zihao and Akter, Mst Mahfuja and Alam, Mirza Mohtashim and Rony, Md Rashad Al Hasan and Lehmann, Jens and Staab, Steffen},
	date = {2023-08-16},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\CN5XR78L\\Nayyeri et al. - 2023 - Integrating Knowledge Graph embedding and pretrained Language Models in Hypercomplex Spaces.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\S7FGIMSE\\2208.html:text/html},
}

@misc{huang_endowing_2022,
	title = {Endowing Language Models with Multimodal Knowledge Graph Representations},
	url = {http://arxiv.org/abs/2206.13163},
	doi = {10.48550/arXiv.2206.13163},
	abstract = {We propose a method to make natural language understanding models more parameter efficient by storing knowledge in an external knowledge graph ({KG}) and retrieving from this {KG} using a dense index. Given (possibly multilingual) downstream task data, e.g., sentences in German, we retrieve entities from the {KG} and use their multimodal representations to improve downstream task performance. We use the recently released {VisualSem} {KG} as our external knowledge repository, which covers a subset of Wikipedia and {WordNet} entities, and compare a mix of tuple-based and graph-based algorithms to learn entity and relation representations that are grounded on the {KG} multimodal information. We demonstrate the usefulness of the learned entity representations on two downstream tasks, and show improved performance on the multilingual named entity recognition task by \$0.3{\textbackslash}\%\$--\$0.7{\textbackslash}\%\$ F1, while we achieve up to \$2.5{\textbackslash}\%\$ improvement in accuracy on the visual sense disambiguation task. All our code and data are available in: {\textbackslash}url\{https://github.com/iacercalixto/visualsem-kg\}.},
	number = {{arXiv}:2206.13163},
	publisher = {{arXiv}},
	author = {Huang, Ningyuan and Deshpande, Yash R. and Liu, Yibo and Alberts, Houda and Cho, Kyunghyun and Vania, Clara and Calixto, Iacer},
	urldate = {2025-01-23},
	date = {2022-06-27},
	eprinttype = {arxiv},
	eprint = {2206.13163 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\WPMIS7NX\\Huang et al. - 2022 - Endowing Language Models with Multimodal Knowledge Graph Representations.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\7YP8MEUG\\2206.html:text/html},
}

@article{alam_language_2022,
	title = {Language Model Guided Knowledge Graph Embeddings},
	volume = {10},
	issn = {2169-3536},
	url = {https://ieeexplore.ieee.org/document/9831788/?arnumber=9831788},
	doi = {10.1109/ACCESS.2022.3191666},
	abstract = {Knowledge graph embedding models have become a popular approach for knowledge graph completion through predicting the plausibility of (potential) triples. This is performed by transforming the entities and relations of the knowledge graph into an embedding space. However, knowledge graphs often include further textual information stored in literal, which is ignored by such embedding models. As a consequence, the learning process stays limited to the structure and the connections between the entities, which has the potential to negatively influence the performance. We bridge this gap by leveraging the capabilities of pre-trained language models to include textual knowledge in the learning process of embedding models. This is achieved by introducing a new loss function that guides embedding models in measuring the likelihood of triples by taking such complementary knowledge into consideration. The proposed solution is a model-independent loss function that can be plugged into any knowledge graph embedding model. In this paper, Sentence-{BERT} and {fastText} are used as pre-trained language models from which the embeddings of the textual knowledge are obtained and injected into the loss function. The loss function contains a trainable slack variable that determines the degree to which the language models influence the plausibility of triples. Our experimental evaluation on six benchmarks, namely Nations, {UMLS}, {WordNet}, and three versions of {CodEx} confirms the advantage of using pre-trained language models for boosting the accuracy of knowledge graph embedding models. We showcase this by performing evaluations on top of the five well-known knowledge graph embedding models such as {TransE}, {RotatE}, {ComplEx}, {DistMult}, and {QuatE}. The results show an improvement in accuracy up to 9\% on {UMLS} dataset for the Distmult model and 4.2\% on the Nations dataset for the {ComplEx} model when they are guided by pre-trained language models. We additionally studied the effect of multiple factors such as the structure of the knowledge graphs and training steps and presented them as ablation studies.},
	pages = {76008--76020},
	journaltitle = {{IEEE} Access},
	author = {Alam, Mirza Mohtashim and Rony, Md Rashad Al Hasan and Nayyeri, Mojtaba and Mohiuddin, Karishma and Akter, M. S. T. Mahfuja and Vahdati, Sahar and Lehmann, Jens},
	urldate = {2025-01-23},
	date = {2022},
	note = {Conference Name: {IEEE} Access},
	keywords = {Task analysis, training-based, Adaptation models, Computational modeling, Knowledge engineering, Knowledge graph, knowledge graph embeddings, language models, link prediction, Predictive models, Unified modeling language, not related},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\T8EW66ET\\Alam et al. - 2022 - Language Model Guided Knowledge Graph Embeddings.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\F9H6FJL5\\9831788.html:text/html},
}

@misc{wang_language_2023,
	title = {Language Models as Knowledge Embeddings},
	url = {http://arxiv.org/abs/2206.12617},
	doi = {10.48550/arXiv.2206.12617},
	abstract = {Knowledge embeddings ({KE}) represent a knowledge graph ({KG}) by embedding entities and relations into continuous vector spaces. Existing methods are mainly structure-based or description-based. Structure-based methods learn representations that preserve the inherent structure of {KGs}. They cannot well represent abundant long-tail entities in real-world {KGs} with limited structural information. Description-based methods leverage textual information and language models. Prior approaches in this direction barely outperform structure-based ones, and suffer from problems like expensive negative sampling and restrictive description demand. In this paper, we propose {LMKE}, which adopts Language Models to derive Knowledge Embeddings, aiming at both enriching representations of long-tail entities and solving problems of prior description-based methods. We formulate description-based {KE} learning with a contrastive learning framework to improve efficiency in training and evaluation. Experimental results show that {LMKE} achieves state-of-the-art performance on {KE} benchmarks of link prediction and triple classification, especially for long-tail entities.},
	number = {{arXiv}:2206.12617},
	publisher = {{arXiv}},
	author = {Wang, Xintao and He, Qianyu and Liang, Jiaqing and Xiao, Yanghua},
	urldate = {2025-01-23},
	date = {2023-06-29},
	eprinttype = {arxiv},
	eprint = {2206.12617 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\Y5JIFBE4\\Wang et al. - 2023 - Language Models as Knowledge Embeddings.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3AR6R6QG\\2206.html:text/html},
}

@misc{wang_reasoning_2023,
	title = {Reasoning Through Memorization: Nearest Neighbor Knowledge Graph Embeddings},
	url = {http://arxiv.org/abs/2201.05575},
	doi = {10.48550/arXiv.2201.05575},
	shorttitle = {Reasoning Through Memorization},
	abstract = {Previous knowledge graph embedding approaches usually map entities to representations and utilize score functions to predict the target entities, yet they typically struggle to reason rare or emerging unseen entities. In this paper, we propose {kNN}-{KGE}, a new knowledge graph embedding approach with pre-trained language models, by linearly interpolating its entity distribution with k-nearest neighbors. We compute the nearest neighbors based on the distance in the entity embedding space from the knowledge store. Our approach can allow rare or emerging entities to be memorized explicitly rather than implicitly in model parameters. Experimental results demonstrate that our approach can improve inductive and transductive link prediction results and yield better performance for low-resource settings with only a few triples, which might be easier to reason via explicit memory. Code is available at https://github.com/zjunlp/{KNN}-{KG}.},
	number = {{arXiv}:2201.05575},
	publisher = {{arXiv}},
	author = {Wang, Peng and Xie, Xin and Wang, Xiaohan and Zhang, Ningyu},
	urldate = {2025-01-23},
	date = {2023-07-30},
	eprinttype = {arxiv},
	eprint = {2201.05575 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\D857CWLH\\Wang et al. - 2023 - Reasoning Through Memorization Nearest Neighbor Knowledge Graph Embeddings.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\LD9HGFFA\\2201.html:text/html},
}

@misc{xie_lambdakg_2023,
	title = {{LambdaKG}: A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings},
	url = {http://arxiv.org/abs/2210.00305},
	doi = {10.48550/arXiv.2210.00305},
	shorttitle = {{LambdaKG}},
	abstract = {Knowledge Graphs ({KGs}) often have two characteristics: heterogeneous graph structure and text-rich entity/relation information. Text-based {KG} embeddings can represent entities by encoding descriptions with pre-trained language models, but no open-sourced library is specifically designed for {KGs} with {PLMs} at present. In this paper, we present {LambdaKG}, a library for {KGE} that equips with many pre-trained language models (e.g., {BERT}, {BART}, T5, {GPT}-3), and supports various tasks (e.g., knowledge graph completion, question answering, recommendation, and knowledge probing). {LambdaKG} is publicly open-sourced at https://github.com/zjunlp/{PromptKG}/tree/main/{lambdaKG}, with a demo video at http://deepke.zjukg.cn/lambdakg.mp4 and long-term maintenance.},
	number = {{arXiv}:2210.00305},
	publisher = {{arXiv}},
	author = {Xie, Xin and Li, Zhoubo and Wang, Xiaohan and Xi, Zekun and Zhang, Ningyu},
	urldate = {2025-01-23},
	date = {2023-09-14},
	eprinttype = {arxiv},
	eprint = {2210.00305 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\TMXK43Z3\\Xie et al. - 2023 - LambdaKG A Library for Pre-trained Language Model-Based Knowledge Graph Embeddings.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\AFHUV56Z\\2210.html:text/html},
}

@misc{fang_karpa_2024,
	title = {{KARPA}: A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's Reasoning Path Aggregation},
	url = {http://arxiv.org/abs/2412.20995},
	doi = {10.48550/arXiv.2412.20995},
	shorttitle = {{KARPA}},
	abstract = {Large language models ({LLMs}) demonstrate exceptional performance across a variety of tasks, yet they are often affected by hallucinations and the timeliness of knowledge. Leveraging knowledge graphs ({KGs}) as external knowledge sources has emerged as a viable solution, but existing methods for {LLM}-based knowledge graph question answering ({KGQA}) are often limited by step-by-step decision-making on {KGs}, restricting the global planning and reasoning capabilities of {LLMs}, or they require fine-tuning or pre-training on specific {KGs}. To address these challenges, we propose Knowledge graph Assisted Reasoning Path Aggregation ({KARPA}), a novel framework that harnesses the global planning abilities of {LLMs} for efficient and accurate {KG} reasoning. {KARPA} operates in three steps: pre-planning relation paths using the {LLM}'s global planning capabilities, matching semantically relevant paths via an embedding model, and reasoning over these paths to generate answers. Unlike existing {KGQA} methods, {KARPA} avoids stepwise traversal, requires no additional training, and is adaptable to various {LLM} architectures. Extensive experimental results show that {KARPA} achieves state-of-the-art performance in {KGQA} tasks, delivering both high efficiency and accuracy. Our code will be available on Github.},
	number = {{arXiv}:2412.20995},
	publisher = {{arXiv}},
	author = {Fang, Siyuan and Ma, Kaijing and Zheng, Tianyu and Du, Xinrun and Lu, Ningxuan and Zhang, Ge and Tang, Qingkun},
	urldate = {2025-01-23},
	date = {2024-12-30},
	eprinttype = {arxiv},
	eprint = {2412.20995 [cs]},
	keywords = {finished, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, code not available},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\BHT9VU7A\\Fang et al. - 2024 - KARPA A Training-free Method of Adapting Knowledge Graph as References for Large Language Model's R.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2TG8SDPJ\\2412.html:text/html},
}

@misc{xiong_interactive-kbqa_2024,
	title = {Interactive-{KBQA}: Multi-Turn Interactions for Knowledge Base Question Answering with Large Language Models},
	url = {http://arxiv.org/abs/2402.15131},
	doi = {10.48550/arXiv.2402.15131},
	shorttitle = {Interactive-{KBQA}},
	abstract = {This study explores the realm of knowledge base question answering ({KBQA}). {KBQA} is considered a challenging task, particularly in parsing intricate questions into executable logical forms. Traditional semantic parsing ({SP})-based methods require extensive data annotations, which result in significant costs. Recently, the advent of few-shot in-context learning, powered by large language models ({LLMs}), has showcased promising capabilities. However, fully leveraging {LLMs} to parse questions into logical forms in low-resource scenarios poses a substantial challenge. To tackle these hurdles, we introduce Interactive-{KBQA}, a framework designed to generate logical forms through direct interaction with knowledge bases ({KBs}). Within this framework, we have developed three generic {APIs} for {KB} interaction. For each category of complex question, we devised exemplars to guide {LLMs} through the reasoning processes. Our method achieves competitive results on the {WebQuestionsSP}, {ComplexWebQuestions}, {KQA} Pro, and {MetaQA} datasets with a minimal number of examples (shots). Importantly, our approach supports manual intervention, allowing for the iterative refinement of {LLM} outputs. By annotating a dataset with step-wise reasoning processes, we showcase our model's adaptability and highlight its potential for contributing significant enhancements to the field.},
	number = {{arXiv}:2402.15131},
	publisher = {{arXiv}},
	author = {Xiong, Guanming and Bao, Junwei and Zhao, Wen},
	urldate = {2025-01-23},
	date = {2024-07-19},
	eprinttype = {arxiv},
	eprint = {2402.15131 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, training-based, not related, code available},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\85I4H5CR\\Xiong et al. - 2024 - Interactive-KBQA Multi-Turn Interactions for Knowledge Base Question Answering with Large Language.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\4SEKU4NH\\2402.html:text/html},
}

@misc{li_chain--knowledge_2024,
	title = {Chain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous Sources},
	url = {http://arxiv.org/abs/2305.13269},
	doi = {10.48550/arXiv.2305.13269},
	shorttitle = {Chain-of-Knowledge},
	abstract = {We present chain-of-knowledge ({CoK}), a novel framework that augments large language models ({LLMs}) by dynamically incorporating grounding information from heterogeneous sources. It results in more factual rationales and reduced hallucination in generation. Specifically, {CoK} consists of three stages: reasoning preparation, dynamic knowledge adapting, and answer consolidation. Given a knowledge-intensive question, {CoK} first prepares several preliminary rationales and answers while identifying the relevant knowledge domains. If there is no majority consensus among the answers from samples, {CoK} corrects the rationales step by step by adapting knowledge from the identified domains. These corrected rationales can plausibly serve as a better foundation for the final answer consolidation. Unlike prior studies that primarily use unstructured data, {CoK} also leverages structured knowledge sources such as Wikidata and tables that provide more reliable factual information. To access both unstructured and structured knowledge sources in the dynamic knowledge adapting stage, we propose an adaptive query generator that allows the generation of queries for various types of query languages, including {SPARQL}, {SQL}, and natural sentences. Moreover, to minimize error propagation between rationales, {CoK} corrects the rationales progressively using preceding corrected rationales to generate and correct subsequent rationales. Extensive experiments show that {CoK} consistently improves the performance of {LLMs} on knowledge-intensive tasks across different domains.},
	number = {{arXiv}:2305.13269},
	publisher = {{arXiv}},
	author = {Li, Xingxuan and Zhao, Ruochen and Chia, Yew Ken and Ding, Bosheng and Joty, Shafiq and Poria, Soujanya and Bing, Lidong},
	urldate = {2025-01-23},
	date = {2024-02-21},
	eprinttype = {arxiv},
	eprint = {2305.13269 [cs]},
	keywords = {Computer Science - Computation and Language, not related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\3HLQXJF2\\Li et al. - 2024 - Chain-of-Knowledge Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneou.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VJXA3TKS\\2305.html:text/html},
}

@misc{huang_joint_2024,
	title = {Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph},
	url = {http://arxiv.org/abs/2401.02212},
	doi = {10.48550/arXiv.2401.02212},
	abstract = {Temporal Knowledge Graph ({TKG}) is an extension of regular knowledge graph by attaching the time scope. Existing temporal knowledge graph question answering ({TKGQA}) models solely approach simple questions, owing to the prior assumption that each question only contains a single temporal fact with explicit/implicit temporal constraints. Hence, they perform poorly on questions which own multiple temporal facts. In this paper, we propose {\textbackslash}textbf\{{\textbackslash}underline\{J\}\}oint {\textbackslash}textbf\{{\textbackslash}underline\{M\}\}ulti {\textbackslash}textbf\{{\textbackslash}underline\{F\}\}acts {\textbackslash}textbf\{{\textbackslash}underline\{R\}\}easoning {\textbackslash}textbf\{{\textbackslash}underline\{N\}\}etwork ({JMFRN}), to jointly reasoning multiple temporal facts for accurately answering {\textbackslash}emph\{complex\} temporal questions. Specifically, {JMFRN} first retrieves question-related temporal facts from {TKG} for each entity of the given complex question. For joint reasoning, we design two different attention ({\textbackslash}ie entity-aware and time-aware) modules, which are suitable for universal settings, to aggregate entities and timestamps information of retrieved facts. Moreover, to filter incorrect type answers, we introduce an additional answer type discrimination task. Extensive experiments demonstrate our proposed method significantly outperforms the state-of-art on the well-known complex temporal question benchmark {TimeQuestions}.},
	number = {{arXiv}:2401.02212},
	publisher = {{arXiv}},
	author = {Huang, Rikui and Wei, Wei and Qu, Xiaoye and Xie, Wenfeng and Mao, Xianling and Chen, Dangyang},
	urldate = {2025-01-23},
	date = {2024-01-04},
	eprinttype = {arxiv},
	eprint = {2401.02212 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, not related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\SF7FNLSL\\Huang et al. - 2024 - Joint Multi-Facts Reasoning Network For Complex Temporal Question Answering Over Knowledge Graph.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\GXRMU4U4\\2401.html:text/html},
}

@article{cao_knowledge_2024,
	title = {Knowledge Graph Embedding: A Survey from the Perspective of Representation Spaces},
	volume = {56},
	issn = {0360-0300, 1557-7341},
	doi = {10.1145/3643806},
	shorttitle = {Knowledge Graph Embedding},
	abstract = {Knowledge graph embedding ({KGE}) is an increasingly popular technique that aims to represent entities and relations of knowledge graphs into low-dimensional semantic spaces for a wide spectrum of applications such as link prediction, knowledge reasoning and knowledge completion. In this article, we provide a systematic review of existing {KGE} techniques based on representation spaces. Particularly, we build a fine-grained classification to categorise the models based on three mathematical perspectives of the representation spaces: (1) algebraic perspective, (2) geometric perspective and (3) analytical perspective. We introduce the rigorous definitions of fundamental mathematical spaces before diving into {KGE} models and their mathematical properties. We further discuss different {KGE} methods over the three categories, as well as summarise how spatial advantages work over different embedding needs. By collating the experimental results from downstream tasks, we also explore the advantages of mathematical space in different scenarios and the reasons behind them. We further state some promising research directions from a representation space perspective, with which we hope to inspire researchers to design their {KGE} models as well as their related applications with more consideration of their mathematical space properties.},
	pages = {1--42},
	number = {6},
	journaltitle = {{ACM} Computing Surveys},
	shortjournal = {{ACM} Comput. Surv.},
	author = {Cao, Jiahang and Fang, Jinyuan and Meng, Zaiqiao and Liang, Shangsong},
	date = {2024-06-30},
	langid = {english},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\7PSKIPNA\\Cao et al. - 2024 - Knowledge Graph Embedding A Survey from the Perspective of Representation Spaces.pdf:application/pdf},
}

@misc{salnikov_large_2023,
	title = {Large Language Models Meet Knowledge Graphs to Answer Factoid Questions},
	url = {http://arxiv.org/abs/2310.02166},
	doi = {10.48550/arXiv.2310.02166},
	abstract = {Recently, it has been shown that the incorporation of structured knowledge into Large Language Models significantly improves the results for a variety of {NLP} tasks. In this paper, we propose a method for exploring pre-trained Text-to-Text Language Models enriched with additional information from Knowledge Graphs for answering factoid questions. More specifically, we propose an algorithm for subgraphs extraction from a Knowledge Graph based on question entities and answer candidates. Then, we procure easily interpreted information with Transformer-based models through the linearization of the extracted subgraphs. Final re-ranking of the answer candidates with the extracted information boosts Hits@1 scores of the pre-trained text-to-text language models by 4-6\%.},
	number = {{arXiv}:2310.02166},
	publisher = {{arXiv}},
	author = {Salnikov, Mikhail and Le, Hai and Rajput, Prateek and Nikishina, Irina and Braslavski, Pavel and Malykh, Valentin and Panchenko, Alexander},
	urldate = {2025-01-23},
	date = {2023-10-03},
	eprinttype = {arxiv},
	eprint = {2310.02166 [cs]},
	keywords = {Computer Science - Computation and Language, training-based, not related},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\TWYTXBVC\\Salnikov et al. - 2023 - Large Language Models Meet Knowledge Graphs to Answer Factoid Questions.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\EELGMP6P\\2310.html:text/html},
}

@inproceedings{zhao_kg-cot_2024,
	location = {Jeju, South Korea},
	title = {{KG}-{CoT}: Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Aware Question Answering},
	isbn = {978-1-956792-04-1},
	url = {https://www.ijcai.org/proceedings/2024/734},
	doi = {10.24963/ijcai.2024/734},
	shorttitle = {{KG}-{CoT}},
	abstract = {Large language models ({LLMs}) encounter challenges such as hallucination and factual errors in knowledge-intensive tasks. One the one hand, {LLMs} sometimes struggle to generate reliable answers based on the black-box parametric knowledge, due to the lack of responsible knowledge. Moreover, fragmented knowledge facts extracted by knowledge retrievers fail to provide explicit and coherent reasoning paths for improving {LLM} reasoning. To address these challenges, we propose {KG}-{CoT}, a novel knowledge-augmented paradigm that leverages a small-scale step-by-step graph reasoning model to reason over knowledge graphs ({KGs}) and utilizes a reasoning path generation method to generate chains of knowledge with high conﬁdence for large-scale {LLMs}. Extensive experiments demonstrate that our {KG}-{CoT} signiﬁcantly improves the performance of {LLMs} on knowledgeintensive question answering tasks, such as multihop, single-hop, and open-domain question answering benchmarks, without ﬁne-tuning {LLMs}. Moreover, {KG}-{CoT} can reduce the number of {API} calls and cost and can generalize to various {LLMs} in a lightweight plug-and-play manner.},
	eventtitle = {Thirty-Third International Joint Conference on Artificial Intelligence \{{IJCAI}-24\}},
	pages = {6642--6650},
	booktitle = {Proceedings of the Thirty-{ThirdInternational} Joint Conference on Artificial Intelligence},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Zhao, Ruilin and Zhao, Feng and Wang, Long and Wang, Xianzhi and Xu, Guandong},
	urldate = {2025-01-23},
	date = {2024-08},
	langid = {english},
	keywords = {training-based, not related},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\XW5D6ZRM\\Zhao et al. - 2024 - KG-CoT Chain-of-Thought Prompting of Large Language Models over Knowledge Graphs for Knowledge-Awar.pdf:application/pdf},
}

@inproceedings{shen_reasoning_2025,
	location = {Abu Dhabi, {UAE}},
	title = {Reasoning with Trees: Faithful Question Answering over Knowledge Graph},
	url = {https://aclanthology.org/2025.coling-main.211/},
	shorttitle = {Reasoning with Trees},
	abstract = {Recent advancements in large language models ({LLMs}) have shown remarkable progress in reasoning capabilities, yet they still face challenges in complex, multi-step reasoning tasks. This study introduces Reasoning with Trees ({RwT}), a novel framework that synergistically integrates {LLMs} with knowledge graphs ({KGs}) to enhance reasoning performance and interpretability. {RwT} reformulates knowledge graph question answering ({KGQA}) as a discrete decision-making problem, leveraging Monte Carlo Tree Search ({MCTS}) to iteratively refine reasoning paths. This approach mirrors human-like reasoning by dynamically integrating the {LLM}`s internal knowledge with external {KG} information. We propose a real-data guided iteration technique to train an evaluation model that assesses action values, improving the efficiency of the {MCTS} process. Experimental results on two benchmark {KGQA} datasets demonstrate that {RwT} significantly outperforms existing state-of-the-art methods, with an average performance improvement of 9.81\%. Notably, {RwT} achieves these improvements without requiring complete retraining of the {LLM}, offering a more efficient and adaptable approach to enhancing {LLM} reasoning capabilities.},
	eventtitle = {{COLING} 2025},
	pages = {3138--3157},
	booktitle = {Proceedings of the 31st International Conference on Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Shen, Tiesunlong and Wang, Jin and Zhang, Xuejie and Cambria, Erik},
	editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and Al-Khalifa, Hend and Eugenio, Barbara Di and Schockaert, Steven},
	urldate = {2025-01-23},
	date = {2025-01},
	keywords = {not related},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\W8RGY8AS\\Shen et al. - 2025 - Reasoning with Trees Faithful Question Answering over Knowledge Graph.pdf:application/pdf},
}

@misc{sui_fidelis_2024,
	title = {{FiDeLiS}: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering},
	doi = {10.48550/arXiv.2405.13873},
	shorttitle = {{FiDeLiS}},
	abstract = {Large language models are often challenged by generating erroneous or `hallucinated' responses, especially in complex reasoning tasks. To mitigate this, we propose a retrieval augmented reasoning method, {FiDeLiS}, which enhances knowledge graph question answering by anchoring responses to structured, verifiable reasoning paths. {FiDeLiS} uses a keyword-enhanced retrieval mechanism that fetches relevant entities and relations from a vector-based index of {KGs} to ensure high-recall retrieval. Once these entities and relations are retrieved, our method constructs candidate reasoning paths which are then refined using a stepwise beam search. This ensures that all the paths we create can be confidently linked back to {KGs}, ensuring they are accurate and reliable. A distinctive feature of our approach is its blend of natural language planning with beam search to optimize the selection of reasoning paths. Moreover, we redesign the way reasoning paths are scored by transforming this process into a deductive reasoning task, allowing the {LLM} to assess the validity of the paths through deductive reasoning rather than traditional logit-based scoring. This helps avoid misleading reasoning chains and reduces unnecessary computational demand. Extensive experiments demonstrate that our method, even as a training-free method which has lower computational costs and superior generality, outperforms established strong baselines across three datasets.},
	number = {{arXiv}:2405.13873},
	publisher = {{arXiv}},
	author = {Sui, Yuan and He, Yufei and Liu, Nian and He, Xiaoxin and Wang, Kun and Hooi, Bryan},
	date = {2024-10-10},
	eprinttype = {arxiv},
	eprint = {2405.13873 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\24NKADYK\\Sui et al. - 2024 - FiDeLiS Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\VQZBI5P4\\2405.html:text/html},
}

@misc{wang_reasoning_2024,
	title = {Reasoning on Efficient Knowledge Paths: Knowledge Graph Guides Large Language Model for Domain Question Answering},
	doi = {10.48550/arXiv.2404.10384},
	shorttitle = {Reasoning on Efficient Knowledge Paths},
	abstract = {Large language models ({LLMs}), such as {GPT}3.5, {GPT}4 and {LLAMA}2 perform surprisingly well and outperform human experts on many tasks. However, in many domain-specific evaluations, these {LLMs} often suffer from hallucination problems due to insufficient training of relevant corpus. Furthermore, fine-tuning large models may face problems such as the {LLMs} are not open source or the construction of high-quality domain instruction is difficult. Therefore, structured knowledge databases such as knowledge graph can better provide domain background knowledge for {LLMs} and make full use of the reasoning and analysis capabilities of {LLMs}. In some previous works, {LLM} was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question. Especially for the question that require a multi-hop reasoning path, frequent calls to {LLM} will consume a lot of computing power. Moreover, when choosing the reasoning path, {LLM} will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps. In this paper, we integrated and optimized a pipeline for selecting reasoning paths from {KG} based on {LLM}, which can reduce the dependency on {LLM}. In addition, we propose a simple and effective subgraph retrieval method based on chain of thought ({CoT}) and page rank which can returns the paths most likely to contain the answer. We conduct experiments on three datasets: {GenMedGPT}-5k [14], {WebQuestions} [2], and {CMCQA} [21]. Finally, {RoK} can demonstrate that using fewer {LLM} calls can achieve the same results as previous {SOTAs} models.},
	number = {{arXiv}:2404.10384},
	publisher = {{arXiv}},
	author = {Wang, Yuqi and Jiang, Boran and Luo, Yi and He, Dawei and Cheng, Peng and Gao, Liangcai},
	date = {2024-04-16},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, finished},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\UGG8IANL\\Wang et al. - 2024 - Reasoning on Efficient Knowledge PathsKnowledge Graph Guides Large Language Model for Domain Questi.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\AZBM67AI\\2404.html:text/html},
}

@article{usbeck_qald-10_2023,
	title = {{QALD}-10 – The 10th challenge on question answering over linked data},
	volume = {Preprint},
	issn = {1570-0844},
	doi = {10.3233/SW-233471},
	abstract = {Knowledge Graph Question Answering ({KGQA}) has gained attention from both industry and academia over the past decade. Researchers proposed a substantial amount of benchmarking datasets with different properties, pushing the development in this field f},
	pages = {1--15},
	issue = {Preprint},
	journaltitle = {Semantic Web},
	author = {Usbeck, Ricardo and Yan, Xi and Perevalov, Aleksandr and Jiang, Longquan and Schulz, Julius and Kraft, Angelie and Möller, Cedric and Huang, Junbo and Reineke, Jan and Ngonga Ngomo, Axel-Cyrille and Saleem, Muhammad and Both, Andreas},
	date = {2023-01-01},
	langid = {english},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6SCX689Q\\Usbeck et al. - 2023 - QALD-10 – The 10th challenge on question answering over linked data.pdf:application/pdf},
}

@article{usman_taxonomies_2017,
	title = {Taxonomies in software engineering: A Systematic mapping study and a revised taxonomy development method},
	volume = {85},
	issn = {0950-5849},
	doi = {10.1016/j.infsof.2017.01.006},
	shorttitle = {Taxonomies in software engineering},
	abstract = {Context: Software Engineering ({SE}) is an evolving discipline with new subareas being continuously developed and added. To structure and better understand the {SE} body of knowledge, taxonomies have been proposed in all {SE} knowledge areas. Objective: The objective of this paper is to characterize the state-of-the-art research on {SE} taxonomies. Method: A systematic mapping study was conducted, based on 270 primary studies. Results: An increasing number of {SE} taxonomies have been published since 2000 in a broad range of venues, including the top {SE} journals and conferences. The majority of taxonomies can be grouped into the following {SWEBOK} knowledge areas: construction (19.55\%), design (19.55\%), requirements (15.50\%) and maintenance (11.81\%). Illustration (45.76\%) is the most frequently used approach for taxonomy validation. Hierarchy (53.14\%) and faceted analysis (39.48\%) are the most frequently used classification structures. Most taxonomies rely on qualitative procedures to classify subject matter instances, but in most cases (86.53\%) these procedures are not described in sufficient detail. The majority of the taxonomies (97\%) target unique subject matters and many taxonomy-papers are cited frequently. Most {SE} taxonomies are designed in an ad-hoc way. To address this issue, we have revised an existing method for developing taxonomies in a more systematic way. Conclusion: There is a strong interest in taxonomies in {SE}, but few taxonomies are extended or revised. Taxonomy design decisions regarding the used classification structures, procedures and descriptive bases are usually not well described and motivated.},
	pages = {43--59},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Usman, Muhammad and Britto, Ricardo and Börstler, Jürgen and Mendes, Emilia},
	date = {2017-05-01},
	keywords = {Classification, Software engineering, Systematic mapping study, Taxonomy},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\9RMJN2ST\\Usman et al. - 2017 - Taxonomies in software engineering A Systematic mapping study and a revised taxonomy development me.pdf:application/pdf;ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\5AAY2G9U\\S0950584917300472.html:text/html},
}

@misc{chakraborty_introduction_2019,
	title = {Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs},
	doi = {10.48550/arXiv.1907.09361},
	abstract = {Question answering has emerged as an intuitive way of querying structured data sources, and has attracted significant advancements over the years. In this article, we provide an overview over these recent advancements, focusing on neural network based question answering systems over knowledge graphs. We introduce readers to the challenges in the tasks, current paradigms of approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point, and ease their process of making informed decisions while creating their own {QA} system.},
	number = {{arXiv}:1907.09361},
	publisher = {{arXiv}},
	author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
	date = {2019-07-22},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\CGYEJPDX\\Chakraborty et al. - 2019 - Introduction to Neural Network based Approaches for Question Answering over Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\MHSLYB8R\\1907.html:text/html},
}

@misc{leidner_question_2002,
	title = {Question Answering over Unstructured Data without Domain Restrictions},
	url = {http://arxiv.org/abs/cs/0207058},
	doi = {10.48550/arXiv.cs/0207058},
	abstract = {Information needs are naturally represented as questions. Automatic Natural-Language Question Answering ({NLQA}) has only recently become a practical task on a larger scale and without domain constraints. This paper gives a brief introduction to the field, its history and the impact of systematic evaluation competitions. It is then demonstrated that an {NLQA} system for English can be built and evaluated in a very short time using off-the-shelf parsers and thesauri. The system is based on Robust Minimal Recursion Semantics ({RMRS}) and is portable with respect to the parser used as a frontend. It applies atomic term unification supported by question classification and {WordNet} lookup for semantic similarity matching of parsed question representation and free text.},
	number = {{arXiv}:cs/0207058},
	publisher = {{arXiv}},
	author = {Leidner, Jochen L.},
	urldate = {2025-01-24},
	date = {2002-07-18},
	eprinttype = {arxiv},
	eprint = {cs/0207058},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2TU65NMU\\0207058.html:text/html},
}

@inproceedings{hermjakob_parsing_2001,
	title = {Parsing and Question Classification for Question Answering},
	url = {https://aclanthology.org/W01-1203/},
	booktitle = {Proceedings of the {ACL} 2001 Workshop on Open-Domain Question Answering},
	author = {Hermjakob, Ulf},
	urldate = {2025-01-24},
	date = {2001},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\KS5QL4VW\\Hermjakob - 2001 - Parsing and Question Classification for Question Answering.pdf:application/pdf},
}

@misc{pan_rag_2024,
	title = {A {RAG} Approach for Generating Competency Questions in Ontology Engineering},
	doi = {10.48550/arXiv.2409.08820},
	abstract = {Competency question ({CQ}) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models ({LLMs}), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to {LLMs}, we present a retrieval-augmented generation ({RAG}) approach that uses {LLMs} for the automatic generation of {CQs} given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the {RAG} and different temperature setting of the {LLM}. We conduct experiments using {GPT}-4 on two domain ontology engineering tasks and compare results against ground-truth {CQs} constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the {RAG} improves the performance of {LLMs} on generating {CQs} for concrete ontology engineering tasks.},
	number = {{arXiv}:2409.08820},
	publisher = {{arXiv}},
	author = {Pan, Xueli and Ossenbruggen, Jacco van and Boer, Victor de and Huang, Zhisheng},
	date = {2024-09-13},
	keywords = {Computer Science - Artificial Intelligence, no type information},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\95JBRIN5\\Pan et al. - 2024 - A RAG Approach for Generating Competency Questions in Ontology Engineering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3YCF48HA\\2409.html:text/html},
}

@article{kamper_types_2020,
	title = {Types of Research Questions: Descriptive, Predictive, or Causal},
	volume = {50},
	issn = {0190-6011},
	doi = {10.2519/jospt.2020.0703},
	shorttitle = {Types of Research Questions},
	abstract = {A previous Evidence in Practice article explained why a specific and answerable research question is important for clinicians and researchers. Determining whether a study aims to answer a descriptive, predictive, or causal question should be one of the first things a reader does when reading an article. Any type of question can be relevant and useful to support evidence-based practice, but only if the question is well defined, matched to the right study design, and reported correctly. J Orthop Sports Phys Ther 2020;50(8):468–469. doi:10.2519/jospt.2020.0703},
	pages = {468--469},
	number = {8},
	journaltitle = {Journal of Orthopaedic \& Sports Physical Therapy},
	author = {Kamper, Steven J.},
	date = {2020-08},
	keywords = {clinical practice, evidence-based practice, finished, research, study quality},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\BI2W9X8Q\\Kamper - 2020 - Types of Research Questions Descriptive, Predictive, or Causal.pdf:application/pdf},
}

@incollection{beck_chapter_2023,
	title = {Chapter 18 - The question: types of research questions and how to develop them},
	isbn = {978-0-323-90300-4},
	url = {https://www.sciencedirect.com/science/article/pii/B9780323903004001075},
	series = {Handbook for Designing and Conducting Clinical and Translational Research},
	shorttitle = {Chapter 18 - The question},
	abstract = {Regardless of their level of training, clinicians encounter questions relating to patient care and outcomes on a daily basis. Clarifying an answer to these questions not only benefits the clinicians' immediate patients, but also those of his or her colleagues by adding to the existing scientific literature. Fine tuning one’s initial question into a hypothesis and eventual study and analysis sets the stage for a successful project. Utilizing the acronyms {PICOT}, {FINER}, and {SPIDER} helps researchers set reasonable and measurable objectives and aims, thereby avoiding the common pitfalls in question development.},
	pages = {111--120},
	booktitle = {Translational Surgery},
	publisher = {Academic Press},
	author = {Beck, Lauren L.},
	editor = {Eltorai, Adam E. M. and Bakal, Jeffrey A. and Newell, Paige C. and Osband, Adena J.},
	urldate = {2025-01-25},
	date = {2023-01-01},
	doi = {10.1016/B978-0-323-90300-4.00107-5},
	keywords = {no type information, {FINER}, {PICOT}, Research aim, Research objective, Research questions, Study development},
	file = {ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\9TTAIFPL\\B9780323903004001075.html:text/html},
}

@article{ahmed_research_2018,
	title = {Research Questions ({RQs}) Classification for Systematic Literature Reviews in Software Engineering},
	volume = {6},
	abstract = {Background: A systematic literature review ({SLR}) is a methodology used to aggregate all relevant existing evidence to answer a research question of interest. Although crucial, the process of conducting {SLR} can be time consuming, and must often be conducted manually.
Objective: The aim of this paper is to support the process of answering {SLR} research questions by first detecting the question class or expected answer type using approach used in question answering field.
Method: We built a data set of research questions ({RQs}) collected from {SLR} papers in software engineering field and labeled it with our proposed taxonomy. The proposed question taxonomy or answer type consists of 6 classes derived from the data set. From the data set questions we extracted three types of features, lexical features like ngram, syntactic features like part of speech and semantic features like Hypernym of head word. We used Support Vector Machine ({SVM}) and Naïve Bayes classifiers to classify questions into its corresponding answer type using the mentioned features.
Results: The {SVM} showed accuracy 97\% when using lexical features, and 95 \% when using syntactic features, but when combining lexical, syntactic and semantic features the accuracy increased to 98\% which is higher than accuracy showed by naïve bayes (79\%), with the same features.
Conclusion: The results that obtained by {SVM} with a combination of the three types of features are very good and can be used in developing a system for answer extraction process when performing an {SLR}.},
	number = {5},
	author = {Ahmed, Zuhal Hamad},
	date = {2018},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\DZ3UFEFQ\\Ahmed - 2018 - Research Questions (RQs) Classification for Systematic Literature Reviews in Software Engineering.pdf:application/pdf},
}

@inproceedings{liu_taxonomy_2015,
	location = {New York, {NY}, {USA}},
	title = {A Taxonomy for Classifying Questions Asked in Social Question and Answering},
	isbn = {978-1-4503-3146-3},
	doi = {10.1145/2702613.2732928},
	series = {{CHI} {EA} '15},
	abstract = {The rapid advancement of Web2.0 technologies has made social networking sites, such as Facebook and twitter, important venues for individuals to seek and share information. As understanding the information needs of users is crucial for designing and developing tools to support their social Q\&amp;A behaviors, in this paper, we present a new way of classifying questions from a design perspective, with the aim of facilitating the development of question routing systems according to individual's information need. As an attempt to understand the questioner's intent in social question and answering environments, we propose a taxonomy of questions posted on Twitter, called {ASK}. Our taxonomy uncovers three different kinds of questions: accuracy, social, and knowledge. In addition, to enable automatic detection on these three types of information needs, we measured and reported on the differences in {ASK} types of questions reflected at both lexical and syntactic levels.},
	pages = {1947--1952},
	booktitle = {Proceedings of the 33rd Annual {ACM} Conference Extended Abstracts on Human Factors in Computing Systems},
	publisher = {Association for Computing Machinery},
	author = {Liu, Zhe and Jansen, Bernard J.},
	date = {2015-04-18},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DPQGCG9R\\Liu und Jansen - 2015 - A Taxonomy for Classifying Questions Asked in Social Question and Answering.pdf:application/pdf},
}

@article{stol_abc_2018-1,
	title = {The {ABC} of Software Engineering Research},
	volume = {27},
	issn = {1049-331X},
	url = {https://dl.acm.org/doi/10.1145/3241743},
	doi = {10.1145/3241743},
	abstract = {A variety of research methods and techniques are available to {SE} researchers, and while several overviews exist, there is consistency neither in the research methods covered nor in the terminology used. Furthermore, research is sometimes critically reviewed for characteristics inherent to the methods. We adopt a taxonomy from the social sciences, termed here the {ABC} framework for {SE} research, which offers a holistic view of eight archetypal research strategies. {ABC} refers to the research goal that strives for generalizability over Actors (A) and precise measurement of their Behavior (B), in a realistic Context (C). The {ABC} framework uses two dimensions widely considered to be key in research design: the level of obtrusiveness of the research and the generalizability of research findings. We discuss metaphors for each strategy and their inherent limitations and potential strengths. We illustrate these research strategies in two key {SE} domains, global software engineering and requirements engineering, and apply the framework on a sample of 75 articles. Finally, we discuss six ways in which the framework can advance {SE} research.},
	pages = {11:1--11:51},
	number = {3},
	journaltitle = {{ACM} Trans. Softw. Eng. Methodol.},
	author = {Stol, Klaas-Jan and Fitzgerald, Brian},
	urldate = {2025-01-25},
	date = {2018-09-17},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GBFDJIMR\\Stol und Fitzgerald - 2018 - The ABC of Software Engineering Research.pdf:application/pdf},
}

@inproceedings{theisen_writing_2017,
	title = {Writing Good Software Engineering Research Papers: Revisited},
	url = {https://ieeexplore.ieee.org/abstract/document/7965369},
	doi = {10.1109/ICSE-C.2017.51},
	shorttitle = {Writing Good Software Engineering Research Papers},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented "Writing Good Software Engineering Research Papers" in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering ({ICSE}) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to {ICSE} 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository ({MSR}) papers, a category of papers not seen in 2002. The advent of {MSR} papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	eventtitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	pages = {402--402},
	booktitle = {2017 {IEEE}/{ACM} 39th International Conference on Software Engineering Companion ({ICSE}-C)},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	urldate = {2025-01-25},
	date = {2017-05},
	keywords = {no type information, Software engineering, research, abstracts, Computer science, Conferences, Data mining, guidelines, Market research, Software, writing, Writing},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JKD4WJ2B\\Theisen et al. - 2017 - Writing Good Software Engineering Research Papers Revisited.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\4DY9J7T2\\7965369.html:text/html},
}

@inproceedings{navarro-almanza_towards_2017,
	title = {Towards Supporting Software Engineering Using Deep Learning: A Case of Software Requirements Classification},
	url = {https://ieeexplore.ieee.org/abstract/document/8337942},
	doi = {10.1109/CONISOFT.2017.00021},
	shorttitle = {Towards Supporting Software Engineering Using Deep Learning},
	abstract = {Software Requirements are the basis of high-quality software development process, each step is related to {SR}, these represent the needs and expectations of the software in a very detailed form. The software requirement classification ({SRC}) task requires a lot of human effort, specially when there are huge of requirements, therefore, the automation of {SRC} have been addressed using Natural Language Processing ({NLP}) and Information Retrieval ({IR}) techniques, however, generally requires human effort to analyze and create features from corpus (set of requirements). In this work, we propose to use Deep Learning ({DL}) to classify software requirements without labor intensive feature engineering. The model that we propose is based on Convolutional Neural Network ({CNN}) that has been state of art in other natural language related tasks. To evaluate our proposed model, {PROMISE} corpus was used, contains a set of labeled requirements in functional and 11 different categories of non-functional requirements. We achieve promising results on {SRC} using {CNN} even without handcrafted features.},
	eventtitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
	pages = {116--120},
	booktitle = {2017 5th International Conference in Software Engineering Research and Innovation ({CONISOFT})},
	author = {Navarro-Almanza, Raul and Juarez-Ramirez, Reyes and Licea, Guillermo},
	urldate = {2025-01-25},
	date = {2017-10},
	keywords = {Data models, Machine learning, Task analysis, no type information, Software Engineering, Software engineering, Software, Computer architecture, Convolutional Neural Network, Convolutional neural networks, Software Requirement Classification, Word Embedding},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\4CHUMTQC\\Navarro-Almanza et al. - 2017 - Towards Supporting Software Engineering Using Deep Learning A Case of Software Requirements Classif.pdf:application/pdf},
}

@inproceedings{kotonya_towards_2003,
	title = {Towards a classification model for component-based software engineering research},
	url = {https://ieeexplore.ieee.org/abstract/document/1231566},
	doi = {10.1109/EURMIC.2003.1231566},
	abstract = {Accurate and timely information is a key motivator in the widespread adoption of {CBSE} technology in Europe. Although there are overlaps and informal communications between researchers and adopters of {CBSE} technology in Europe, there is no systematic mechanism for information interchange between the two. {CBSEnet} is a European Union initiative to create an Internet-based forum for the exchange of information between researchers and adopters of {CBSE}. We describe a proposed classification model for {CBSE} research that will form the basis for structuring the {CBSEnet} knowledge base.},
	eventtitle = {2003 Proceedings 29th Euromicro Conference},
	pages = {43--52},
	booktitle = {2003 Proceedings 29th Euromicro Conference},
	author = {{Kotonya} and {Sommerville} and {Hall}},
	urldate = {2025-01-25},
	date = {2003-09},
	note = {{ISSN}: 1089-6503},
	keywords = {no type information, Electronic data interchange, Internet, Object oriented programming, Software development management},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\NANNFD7I\\Kotonya et al. - 2003 - Towards a classification model for component-based software engineering research.pdf:application/pdf},
}

@inproceedings{britto_blooms_2015,
	title = {Bloom's taxonomy in software engineering education: A systematic mapping study},
	url = {https://ieeexplore.ieee.org/abstract/document/7344084},
	doi = {10.1109/FIE.2015.7344084},
	shorttitle = {Bloom's taxonomy in software engineering education},
	abstract = {Designing and assessing learning outcomes could be a challenging activity for any Software Engineering ({SE}) educator. To support the process of designing and assessing {SE} courses, educators have been applied the cognitive domain of Bloom's taxonomy. However, to the best of our knowledge, the evidence on the usage of Bloom's taxonomy in {SE} higher education has not yet been systematically aggregated or reviewed. Therefore, in this paper we report the state of the art on the usage of Bloom's taxonomy in {SE} education, identified by conducted a systematic mapping study. As a result of the performed systematic mapping study, 26 studies were deemed as relevant. The main findings from these studies are: i) Bloom's taxonomy has mostly been applied at undergraduate level for both design and assessment of software engineering courses; ii) software construction is the leading {SE} subarea in which Bloom's taxonomy has been applied. The results clearly point out the usefulness of Bloom's taxonomy in the {SE} education context. We intend to use the results from this systematic mapping study to develop a set of guidelines to support the usage of Bloom's taxonomy cognitive levels to design and assess {SE} courses.},
	eventtitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
	pages = {1--8},
	booktitle = {2015 {IEEE} Frontiers in Education Conference ({FIE})},
	author = {Britto, Ricardo and Usman, Muhammad},
	urldate = {2025-01-25},
	date = {2015-10},
	keywords = {no type information, Taxonomy, Software engineering, Data mining, Software, Context, Education, Systematics},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\L6J3LNFA\\Britto und Usman - 2015 - Bloom's taxonomy in software engineering education A systematic mapping study.pdf:application/pdf},
}

@article{britto_extended_2016,
	title = {An extended global software engineering taxonomy},
	volume = {4},
	issn = {2195-1721},
	url = {https://doi.org/10.1186/s40411-016-0029-2},
	doi = {10.1186/s40411-016-0029-2},
	abstract = {In Global Software Engineering ({GSE}), the need for a common terminology and knowledge classification has been identified to facilitate the sharing and combination of knowledge by {GSE} researchers and practitioners. A {GSE} taxonomy was recently proposed to address such a need, focusing on a core set of dimensions; however its dimensions do not represent an exhaustive list of relevant {GSE} factors. Therefore, this study extends the existing taxonomy, incorporating new {GSE} dimensions that were identified by means of two empirical studies conducted recently.},
	pages = {3},
	number = {1},
	journaltitle = {Journal of Software Engineering Research and Development},
	shortjournal = {J Softw Eng Res Dev},
	author = {Britto, Ricardo and Wohlin, Claes and Mendes, Emilia},
	urldate = {2025-01-25},
	date = {2016-06-07},
	langid = {english},
	keywords = {no type information, Taxonomy, Global software engineering, Knowledge classification, Software engineering management},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GHXB96P5\\Britto et al. - 2016 - An extended global software engineering taxonomy.pdf:application/pdf},
}

@inproceedings{bolotova_non-factoid_2022,
	location = {New York, {NY}, {USA}},
	title = {A Non-Factoid Question-Answering Taxonomy},
	isbn = {978-1-4503-8732-3},
	doi = {10.1145/3477495.3531926},
	series = {{SIGIR} '22},
	abstract = {Non-factoid question answering ({NFQA}) is a challenging and under-researched task that requires constructing long-form answers, such as explanations or opinions, to open-ended non-factoid questions - {NFQs}. There is still little understanding of the categories of {NFQs} that people tend to ask, what form of answers they expect to see in return, and what the key research challenges of each category are.  This work presents the first comprehensive taxonomy of {NFQ} categories and the expected structure of answers. The taxonomy was constructed with a transparent methodology and extensively evaluated via crowdsourcing. The most challenging categories were identified through an editorial user study. We also release a dataset of categorised {NFQs} and a question category classifier. Finally, we conduct a quantitative analysis of the distribution of question categories using major {NFQA} datasets, showing that the {NFQ} categories that are the most challenging for current {NFQA} systems are poorly represented in these datasets. This imbalance may lead to insufficient system performance for challenging categories. The new taxonomy, along with the category classifier, will aid research in the area, helping to create more balanced benchmarks and to focus models on addressing specific categories.},
	pages = {1196--1207},
	booktitle = {Proceedings of the 45th International {ACM} {SIGIR} Conference on Research and Development in Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Bolotova, Valeriia and Blinov, Vladislav and Scholer, Falk and Croft, W. Bruce and Sanderson, Mark},
	date = {2022-07-07},
	keywords = {finished},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\A7ABRSYP\\Bolotova et al. - 2022 - A Non-Factoid Question-Answering Taxonomy.pdf:application/pdf},
}

@article{steinmetz_what_2021,
	title = {What is in the {KGQA} Benchmark Datasets? Survey on Challenges in Datasets for Question Answering on Knowledge Graphs},
	volume = {10},
	issn = {1861-2040},
	doi = {10.1007/s13740-021-00128-9},
	shorttitle = {What is in the {KGQA} Benchmark Datasets?},
	abstract = {Question Answering based on Knowledge Graphs ({KGQA}) still faces difficult challenges when transforming natural language ({NL}) to {SPARQL} queries. Simple questions only referring to one triple are answerable by most {QA} systems, but more complex questions requiring complex queries containing subqueries or several functions are still a tough challenge within this field of research. Evaluation results of {QA} systems therefore also might depend on the benchmark dataset the system has been tested on. For the purpose to give an overview and reveal specific characteristics, we examined currently available {KGQA} datasets regarding several challenging aspects. This paper presents a detailed look into the datasets and compares them in terms of challenges a {KGQA} system is facing.},
	pages = {241--265},
	number = {3},
	journaltitle = {Journal on Data Semantics},
	shortjournal = {J Data Semant},
	author = {Steinmetz, Nadine and Sattler, Kai-Uwe},
	date = {2021-12-01},
	langid = {english},
	keywords = {Artificial Intelligence, Dataset analysis, finished, Natural language transformation, Pattern recognition, Question answering on knowledge graphs},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\T9Q8XQAM\\Steinmetz und Sattler - 2021 - What is in the KGQA Benchmark Datasets Survey on Challenges in Datasets for Question Answering on K.pdf:application/pdf},
}

@misc{li_scigraphqa_2023,
	title = {{SciGraphQA}: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs},
	url = {http://arxiv.org/abs/2308.03349},
	doi = {10.48550/arXiv.2308.03349},
	shorttitle = {{SciGraphQA}},
	abstract = {In this work, we present {SciGraphQA}, a synthetic multi-turn question-answer dataset related to academic graphs. {SciGraphQA} is 13 times larger than {ChartVQA}, the previously largest chart-visual question-answering dataset. It is also the largest open-sourced chart {VQA} dataset with non-synthetic charts. To build our dataset, we selected 290,000 Computer Science or Machine Learning {ArXiv} papers published between 2010 and 2020, and then used Palm-2 to generate 295K samples of open-vocabulary multi-turn question-answering dialogues about the graphs. As context, we provided the text-only Palm-2 with paper title, abstract, paragraph mentioning the graph, and rich text contextual data from the graph itself, obtaining dialogues with an average 2.23 question-answer turns for each graph. We asked {GPT}-4 to assess the matching quality of our question-answer turns given the paper's context, obtaining an average rating of 8.7/10 on our 3K test set. We evaluated the 0-shot capability of the most popular {MLLM} models such as {LLaVa}, {mPLUGowl}, {BLIP}-2, and {openFlamingo}'s on our dataset, finding {LLaVA}-13B being the most performant with a {CIDEr} score of 0.08. We further enriched the question prompts for {LLAVA} by including the serialized data tables extracted from the graphs using the {DePlot} model, boosting {LLaVA}'s 0-shot {CIDEr} to 0.15. To verify the validity of our dataset, we also fine-tuned {LLaVa} using our dataset, reaching a substantially higher {CIDEr} score of 0.26. We anticipate further accuracy improvement by including segmentation mask tokens and leveraging larger {LLM} backbones coupled with emergent prompting techniques. Our code and data are open-sourced.},
	number = {{arXiv}:2308.03349},
	publisher = {{arXiv}},
	author = {Li, Shengzhi and Tajbakhsh, Nima},
	urldate = {2025-01-25},
	date = {2023-08-07},
	eprinttype = {arxiv},
	eprint = {2308.03349 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, no type information, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\J6NLGJT2\\Li und Tajbakhsh - 2023 - SciGraphQA A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\YTL4Y9BU\\2308.html:text/html},
}

@misc{jia_leveraging_2024,
	title = {Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2405.15374},
	doi = {10.48550/arXiv.2405.15374},
	abstract = {The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science ({CS}) researchers at the Australian National University ({ANU}). The system integrates Large Language Models ({LLMs}) with the {ANU} Scholarly Knowledge Graph ({ASKG}), a structured repository of all research-related artifacts produced at {ANU} in the {CS} field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph ({KG}). To address the limitations of traditional scholarly {KG} construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model ({DDM}) for comprehensive document representation and the {KG}-enhanced Query Processing ({KGQP}) for optimized complex query handling. {DDM} enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while {KGQP} leverages the {KG} structure to improve query accuracy and efficiency with {LLMs}. By combining the {ASKG} with {LLMs}, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic {LLM}-{SPARQL} fusion to retrieve relevant facts and textual nodes from the {ASKG}. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency. We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with {LLMs}.},
	number = {{arXiv}:2405.15374},
	publisher = {{arXiv}},
	author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio J. Rodríguez and Omran, Pouya G.},
	urldate = {2025-01-25},
	date = {2024-05-24},
	eprinttype = {arxiv},
	eprint = {2405.15374 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\QCNQZVQ6\\Jia et al. - 2024 - Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2MLPDKVC\\2405.html:text/html},
}

@inproceedings{nguyen_semantic_2024,
	location = {Singapore},
	title = {Semantic Parsing for Question and Answering over Scholarly Knowledge Graph with Large Language Models},
	isbn = {978-981-97-3076-6},
	doi = {10.1007/978-981-97-3076-6_20},
	abstract = {This paper presents a study to answer the question of how to map a natural language ({NL}) sentence to a semantic representation and its application to question answering over the {DBLP} database. We investigate the deep learning approach using pre-trained models and their fine-tuning on training data for semantic parsing tasks. Experimental results on standard datasets show the effectiveness of pre-trained models in mapping an {NL} sentence to {SPARQL}, a query language for semantic databases. The results also show that the T5 and Flan-T5 models outperform other models in terms of translation accuracy. In addition to the empirical results on pre-trained models, we also consider the problem of examining large language models ({LLMs}) such as Llama and Mistras, or Qwen models for answering questions on the {DBLP} database. Experimental results showed the potentiality of using {LLMs} with chain-of-thought prompting methods. The results indicated that without using training data, we were able to obtain promising results for some types of questions when translating them to {SPARQL}.},
	pages = {284--298},
	booktitle = {New Frontiers in Artificial Intelligence},
	publisher = {Springer Nature},
	author = {Nguyen, Le-Minh and Khang, Le-Nguyen and Anh, Kieu Que and Hien, Nguyen Dieu and Nagai, Yukari},
	editor = {Suzumura, Toyotaro and Bono, Mayumi},
	date = {2024},
	langid = {english},
	keywords = {no type information, Knowledge Graph, Mapping {NL} to {SPARQL}, Question Answering, Relation search, Semantic parsing, Semantic Representation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\AF633YZA\\Nguyen et al. - 2024 - Semantic Parsing for Question and Answering over Scholarly Knowledge Graph with Large Language Model.pdf:application/pdf},
}

@inproceedings{dubey_asknow_2016,
	location = {Cham},
	title = {{AskNow}: A Framework for Natural Language Query Formalization in {SPARQL}},
	isbn = {978-3-319-34129-3},
	doi = {10.1007/978-3-319-34129-3_19},
	shorttitle = {{AskNow}},
	abstract = {Natural Language Query Formalization involves semantically parsing queries in natural language and translating them into their corresponding formal representations. It is a key component for developing question-answering ({QA}) systems on {RDF} data. The chosen formal representation language in this case is often {SPARQL}. In this paper, we propose a framework, called {AskNow}, where users can pose queries in English to a target {RDF} knowledge base (e.g. {DBpedia}), which are first normalized into an intermediary canonical syntactic form, called Normalized Query Structure ({NQS}), and then translated into {SPARQL} queries. {NQS} facilitates the identification of the desire (or expected output information) and the user-provided input information, and establishing their mutual semantic relationship. At the same time, it is sufficiently adaptive to query paraphrasing. We have empirically evaluated the framework with respect to the syntactic robustness of {NQS} and semantic accuracy of the {SPARQL} translator on standard benchmark datasets.},
	pages = {300--316},
	booktitle = {The Semantic Web. Latest Advances and New Domains},
	publisher = {Springer International Publishing},
	author = {Dubey, Mohnish and Dasgupta, Sourish and Sharma, Ankit and Höffner, Konrad and Lehmann, Jens},
	editor = {Sack, Harald and Blomqvist, Eva and d'Aquin, Mathieu and Ghidini, Chiara and Ponzetto, Simone Paolo and Lange, Christoph},
	date = {2016},
	langid = {english},
	keywords = {{DBpedia}, Natural Language Query Formalization ({NLQF}), Query Paraphrases, Semantic Accuracy, {SPARQL} Translation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3W9R43XE\\Dubey et al. - 2016 - AskNow A Framework for Natural Language Query Formalization in SPARQL.pdf:application/pdf},
}

@inproceedings{usbeck_9th_2018,
	title = {9th Challenge on Question Answering over Linked Data ({QALD}-9) (invited paper)},
	url = {https://www.semanticscholar.org/paper/9th-Challenge-on-Question-Answering-over-Linked-Usbeck-Gusmita/4f83e1b64f57ae0d546076279426e85c0e60298b},
	abstract = {Recent years have seen a growing amount of research on question answering ({QA}) over Semantic Web data, shaping an interaction paradigm that allows end users to profit from the expressive power of Semantic Web standards. At the same time, {QA} systems hide their complexity behind an intuitive and easy-touse interface. However, the growing amount of data available on the Semantic Web has led to a heterogeneous data landscape where {QA} systems struggle to keep up with the volume, variety and veracity of the underlying knowledge. The Question Answering over Linked Data ({QALD}) challenges aim to provide up-to-date benchmarks for assessing and comparing state-of-the-art systems that mediate between a user, expressing his or her information need in natural language, and {RDF} data. In the past few years, more than 40 research groups and their systems have taken part in the last nine {QALD} challenges. The {QALD} challenge targets all researchers and practitioners working on querying Linked Data, natural language processing for question answering, multilingual information retrieval and related topics. The main goal is to gain insights into the strengths and shortcomings of different approaches and into possible solutions for coping with the large, heterogeneous and distributed nature of Semantic Web data. {QALD} has a 8-year history. The challenge began in 2011 and is developing benchmarks that are increasingly being used as a standard evaluation venue for question answering over Linked Data. Overviews of past instantiations of the challenge are available from the {CLEF} Working Notes, {CEUR} workshop notes as well as {ESWC} proceedings, see Table 1. This article will give a technical overview of the task and results of the 9th Question Answering over Linked Data challenge.},
	eventtitle = {Semdeep/{NLIWoD}@{ISWC}},
	author = {Usbeck, Ricardo and Gusmita, Ria Hari and Ngomo, A. and Saleem, Muhammad},
	urldate = {2025-01-25},
	date = {2018},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\W3TV3V9D\\Usbeck et al. - 2018 - 9th Challenge on Question Answering over Linked Data (QALD-9) (invited paper).pdf:application/pdf},
}

@misc{serban_generating_2016,
	title = {Generating Factoid Questions With Recurrent Neural Networks: The 30M Factoid Question-Answer Corpus},
	url = {http://arxiv.org/abs/1603.06807},
	doi = {10.48550/arXiv.1603.06807},
	shorttitle = {Generating Factoid Questions With Recurrent Neural Networks},
	abstract = {Over the past decade, large-scale supervised learning corpora have enabled machine learning researchers to make substantial advances. However, to this date, there are no large-scale question-answer corpora available. In this paper we present the 30M Factoid Question-Answer Corpus, an enormous question answer pair corpus produced by applying a novel neural network architecture on the knowledge base Freebase to transduce facts into natural language questions. The produced question answer pairs are evaluated both by human evaluators and using automatic evaluation metrics, including well-established machine translation and sentence similarity metrics. Across all evaluation criteria the question-generation model outperforms the competing template-based baseline. Furthermore, when presented to human evaluators, the generated questions appear comparable in quality to real human-generated questions.},
	number = {{arXiv}:1603.06807},
	publisher = {{arXiv}},
	author = {Serban, Iulian Vlad and García-Durán, Alberto and Gulcehre, Caglar and Ahn, Sungjin and Chandar, Sarath and Courville, Aaron and Bengio, Yoshua},
	urldate = {2025-01-25},
	date = {2016-05-29},
	eprinttype = {arxiv},
	eprint = {1603.06807 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PI2THVTF\\Serban et al. - 2016 - Generating Factoid Questions With Recurrent Neural Networks The 30M Factoid Question-Answer Corpus.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8FZLF5NM\\1603.html:text/html},
}

@inproceedings{zafar_formal_2018,
	location = {Cham},
	title = {Formal Query Generation for Question Answering over Knowledge Bases},
	isbn = {978-3-319-93417-4},
	doi = {10.1007/978-3-319-93417-4_46},
	abstract = {Question answering ({QA}) systems often consist of several components such as Named Entity Disambiguation ({NED}), Relation Extraction ({RE}), and Query Generation ({QG}). In this paper, we focus on the {QG} process of a {QA} pipeline on a large-scale Knowledge Base ({KB}), with noisy annotations and complex sentence structures. We therefore propose {SQG}, a {SPARQL} Query Generator with modular architecture, enabling easy integration with other components for the construction of a fully functional {QA} pipeline. {SQG} can be used on large open-domain {KBs} and handle noisy inputs by discovering a minimal subgraph based on uncertain inputs, that it receives from the {NED} and {RE} components. This ability allows {SQG} to consider a set of candidate entities/relations, as opposed to the most probable ones, which leads to a significant boost in the performance of the {QG} component. The captured subgraph covers multiple candidate walks, which correspond to {SPARQL} queries. To enhance the accuracy, we present a ranking model based on Tree-{LSTM} that takes into account the syntactical structure of the question and the tree representation of the candidate queries to find the one representing the correct intention behind the question. {SQG} outperforms the baseline systems and achieves a macro F1-measure of 75\% on the {LC}-{QuAD} dataset.},
	pages = {714--728},
	booktitle = {The Semantic Web},
	publisher = {Springer International Publishing},
	author = {Zafar, Hamid and Napolitano, Giulio and Lehmann, Jens},
	editor = {Gangemi, Aldo and Navigli, Roberto and Vidal, Maria-Esther and Hitzler, Pascal and Troncy, Raphaël and Hollink, Laura and Tordai, Anna and Alam, Mehwish},
	date = {2018},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3HMWC8A9\\Zafar et al. - 2018 - Formal Query Generation for Question Answering over Knowledge Bases.pdf:application/pdf},
}

@article{yani_challenges_2021,
	title = {Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering: A Survey},
	volume = {12},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	doi = {10.3390/info12070271},
	shorttitle = {Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering},
	abstract = {Simple questions are the most common type of questions used for evaluating a knowledge graph question answering ({KGQA}). A simple question is a question whose answer can be captured by a factoid statement with one relation or predicate. Knowledge graph question answering ({KGQA}) systems are systems whose aim is to automatically answer natural language questions ({NLQs}) over knowledge graphs ({KGs}). There are varieties of researches with different approaches in this area. However, the lack of a comprehensive study to focus on addressing simple questions from all aspects is tangible. In this paper, we present a comprehensive survey of answering simple questions to classify available techniques and compare their advantages and drawbacks in order to have better insights of existing issues and recommendations to direct future works.},
	pages = {271},
	number = {7},
	journaltitle = {Information},
	author = {Yani, Mohammad and Krisnadhi, Adila Alfa},
	date = {2021-07},
	langid = {english},
	keywords = {knowledge graph, question answering, simple questions, survey},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZS927R35\\Yani und Krisnadhi - 2021 - Challenges, Techniques, and Trends of Simple Knowledge Graph Question Answering A Survey.pdf:application/pdf},
}

@inproceedings{suzuki_question_2003,
	location = {Sapporo, Japan},
	title = {Question Classification using {HDAG} Kernel},
	url = {https://aclanthology.org/W03-1208/},
	doi = {10.3115/1119312.1119320},
	eventtitle = {{MultiLing} 2003},
	pages = {61--68},
	booktitle = {Proceedings of the {ACL} 2003 Workshop on Multilingual Summarization and Question Answering},
	publisher = {Association for Computational Linguistics},
	author = {Suzuki, Jun and Taira, Hirotoshi and Sasaki, Yutaka and Maeda, Eisaku},
	urldate = {2025-01-25},
	date = {2003-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\VTNT9RLL\\Suzuki et al. - 2003 - Question Classification using HDAG Kernel.pdf:application/pdf},
}

@inproceedings{bu_function-based_2010,
	location = {Cambridge, {MA}},
	title = {Function-Based Question Classification for General {QA}},
	url = {https://aclanthology.org/D10-1109/},
	eventtitle = {{EMNLP} 2010},
	pages = {1119--1128},
	booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Bu, Fan and Zhu, Xingwei and Hao, Yu and Zhu, Xiaoyan},
	editor = {Li, Hang and Màrquez, Lluís},
	urldate = {2025-01-25},
	date = {2010-10},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HBB3WH6L\\Bu et al. - 2010 - Function-Based Question Classification for General QA.pdf:application/pdf},
}

@inproceedings{hashemi_performance_2019,
	location = {New York, {NY}, {USA}},
	title = {Performance Prediction for Non-Factoid Question Answering},
	isbn = {978-1-4503-6881-0},
	url = {https://dl.acm.org/doi/10.1145/3341981.3344249},
	doi = {10.1145/3341981.3344249},
	series = {{ICTIR} '19},
	abstract = {Estimating the quality of a result list, often referred to as query performance prediction ({QPP}), is a challenging and important task in information retrieval. It can be used as feedback to users, search engines, and system administrators. Although predicting the performance of retrieval models has been extensively studied for the ad-hoc retrieval task, the effectiveness of performance prediction methods for question answering ({QA}) systems is relatively unstudied. The short length of answers, the dominance of neural models in {QA}, and the re-ranking nature of most {QA} systems make performance prediction for {QA} a unique, important, and technically interesting task. In this paper, we introduce and motivate the task of performance prediction for non-factoid question answering and propose a neural performance predictor for this task. Our experiments on two recent datasets demonstrate that the proposed model outperforms competitive baselines in all settings.},
	pages = {55--58},
	booktitle = {Proceedings of the 2019 {ACM} {SIGIR} International Conference on Theory of Information Retrieval},
	publisher = {Association for Computing Machinery},
	author = {Hashemi, Helia and Zamani, Hamed and Croft, W. Bruce},
	urldate = {2025-01-25},
	date = {2019-09-26},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\H7C4YCFS\\Hashemi et al. - 2019 - Performance Prediction for Non-Factoid Question Answering.pdf:application/pdf},
}

@misc{bajaj_ms_2018,
	title = {{MS} {MARCO}: A Human Generated {MAchine} Reading {COmprehension} Dataset},
	url = {http://arxiv.org/abs/1611.09268},
	doi = {10.48550/arXiv.1611.09268},
	shorttitle = {{MS} {MARCO}},
	abstract = {We introduce a large scale {MAchine} Reading {COmprehension} dataset, which we name {MS} {MARCO}. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the {MS} {MARCO} dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes {MS} {MARCO} from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.},
	number = {{arXiv}:1611.09268},
	publisher = {{arXiv}},
	author = {Bajaj, Payal and Campos, Daniel and Craswell, Nick and Deng, Li and Gao, Jianfeng and Liu, Xiaodong and Majumder, Rangan and {McNamara}, Andrew and Mitra, Bhaskar and Nguyen, Tri and Rosenberg, Mir and Song, Xia and Stoica, Alina and Tiwary, Saurabh and Wang, Tong},
	urldate = {2025-01-25},
	date = {2018-10-31},
	eprinttype = {arxiv},
	eprint = {1611.09268 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\JSFLTXCA\\Bajaj et al. - 2018 - MS MARCO A Human Generated MAchine Reading COmprehension Dataset.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\6GPZ3U5B\\1611.html:text/html},
}

@inproceedings{soleimani_nlquad_2021,
	location = {Online},
	title = {{NLQuAD}: A Non-Factoid Long Question Answering Data Set},
	url = {https://aclanthology.org/2021.eacl-main.106/},
	doi = {10.18653/v1/2021.eacl-main.106},
	shorttitle = {{NLQuAD}},
	abstract = {We introduce {NLQuAD}, the first data set with baseline methods for non-factoid long question answering, a task requiring document-level language understanding. In contrast to existing span detection question answering data sets, {NLQuAD} has non-factoid questions that are not answerable by a short span of text and demanding multiple-sentence descriptive answers and opinions. We show the limitation of the F1 score for evaluation of long answers and introduce Intersection over Union ({IoU}), which measures position-sensitive overlap between the predicted and the target answer spans. To establish baseline performances, we compare {BERT}, {RoBERTa}, and Longformer models. Experimental results and human evaluations show that Longformer outperforms the other architectures, but results are still far behind a human upper bound, leaving substantial room for improvements. {NLQuAD}`s samples exceed the input limitation of most pre-trained Transformer-based models, encouraging future research on long sequence language models.},
	eventtitle = {{EACL} 2021},
	pages = {1245--1255},
	booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
	publisher = {Association for Computational Linguistics},
	author = {Soleimani, Amir and Monz, Christof and Worring, Marcel},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	urldate = {2025-01-25},
	date = {2021-04},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M7TLBDYR\\Soleimani et al. - 2021 - NLQuAD A Non-Factoid Long Question Answering Data Set.pdf:application/pdf},
}

@inproceedings{bloehdorn_ontology-based_2007,
	location = {Berlin, Heidelberg},
	title = {Ontology-Based Question Answering for Digital Libraries},
	isbn = {978-3-540-74851-9},
	doi = {10.1007/978-3-540-74851-9_2},
	abstract = {In this paper we present an approach to question answering over heterogeneous knowledge sources that makes use of different ontology management components within the scenario of a digital library application. We present a principled framework for integrating structured metadata and unstructured resource content in a seamless manner which can then be flexibly queried using structured queries expressed in natural language. The novelty of the approach lies in the combination of different semantic technologies providing a clear benefit for the application scenario considered. The resulting system is implemented as part of the digital library of British Telecommunications ({BT}). The original contribution of our paper lies in the architecture we present allowing for the non-straightforward integration of the different components we consider.},
	pages = {14--25},
	booktitle = {Research and Advanced Technology for Digital Libraries},
	publisher = {Springer},
	author = {Bloehdorn, Stephan and Cimiano, Philipp and Duke, Alistair and Haase, Peter and Heizmann, Jörg and Thurlow, Ian and Völker, Johanna},
	editor = {Kovács, László and Fuhr, Norbert and Meghini, Carlo},
	date = {2007},
	langid = {english},
	keywords = {Conjunctive Query, Digital Library, Intellectual Capital, Knowledge Source, Resource Description Framework},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XUA7RHGF\\Bloehdorn et al. - 2007 - Ontology-Based Question Answering for Digital Libraries.pdf:application/pdf},
}

@misc{jauhar_tabmcq_2016,
	title = {{TabMCQ}: A Dataset of General Knowledge Tables and Multiple-choice Questions},
	url = {http://arxiv.org/abs/1602.03960},
	doi = {10.48550/arXiv.1602.03960},
	shorttitle = {{TabMCQ}},
	abstract = {We describe two new related resources that facilitate modelling of general knowledge reasoning in 4th grade science exams. The first is a collection of curated facts in the form of tables, and the second is a large set of crowd-sourced multiple-choice questions covering the facts in the tables. Through the setup of the crowd-sourced annotation task we obtain implicit alignment information between questions and tables. We envisage that the resources will be useful not only to researchers working on question answering, but also to people investigating a diverse range of other applications such as information extraction, question parsing, answer type identification, and lexical semantic modelling.},
	number = {{arXiv}:1602.03960},
	publisher = {{arXiv}},
	author = {Jauhar, Sujay Kumar and Turney, Peter and Hovy, Eduard},
	urldate = {2025-01-25},
	date = {2016-02-12},
	eprinttype = {arxiv},
	eprint = {1602.03960 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\BZIEL7IT\\Jauhar et al. - 2016 - TabMCQ A Dataset of General Knowledge Tables and Multiple-choice Questions.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\24MCNUGZ\\1602.html:text/html},
}

@article{zhao_ontology_2009,
	title = {Ontology Classification for Semantic-Web-Based Software Engineering},
	volume = {2},
	rights = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/{IEEE}.html},
	issn = {1939-1374},
	url = {http://ieeexplore.ieee.org/document/5161251/},
	doi = {10.1109/TSC.2009.20},
	abstract = {The semantic Web is the second generation of the Web, which helps sharing and reusing data across application, enterprise, and community boundaries. Ontology defines a set of representational primitives with which a domain of knowledge is modeled. The main purpose of the semantic Web and ontology is to integrate heterogeneous data and enable interoperability among disparate systems. Ontology has been used to model software engineering knowledge by denoting the artifacts that are designed or produced during the engineering process. The semantic Web allows publishing reusable software engineering knowledge resources and providing services for searching and querying. This paper classifies the ontologies developed for software engineering, reviews the current efforts on applying the semantic Web techniques on different software engineering aspects, and presents the benefits of their applications. We also foresee the possible future research directions.},
	pages = {303--317},
	number = {4},
	journaltitle = {{IEEE} Transactions on Services Computing},
	shortjournal = {{IEEE} Trans. Serv. Comput.},
	author = {Zhao, Yajing and Dong, Jing and Peng, Tu},
	urldate = {2025-01-25},
	date = {2009-10},
}

@inproceedings{allam_question_2016,
	title = {The Question Answering Systems : A Survey .},
	shorttitle = {The Question Answering Systems},
	abstract = {Question Answering ({QA}) is a specialized area in the field of Information Retrieval ({IR}). The {QA} systems are concerned with providing relevant answers in response to questions proposed in natural language. {QA} is therefore composed of three distinct modules, each of which has a core component beside other supplementary components. These three core components are: question classification, information retrieval, and answer extraction. Question classification plays an essential role in {QA} systems by classifying the submitted question according to its type. Information retrieval is very important for question answering, because if no correct answers are present in a document, no further processing could be carried out to find an answer. Finally, answer extraction aims to retrieve the answer for a question asked by the user. This survey paper provides an overview of Question-Answering and its system architecture, as well as the previous related work comparing each research against the others with respect to the components that were covered and the approaches that were followed. At the end, the survey provides an analytical discussion of the proposed {QA} models, along with their main contributions, experimental results, and limitations.},
	author = {Allam, Ali Mohamed Nabil and Haggag, Mohamed H.},
	date = {2016},
	keywords = {finished},
}

@article{molla_question_2007,
	title = {Question Answering in Restricted Domains: An Overview},
	volume = {33},
	issn = {0891-2017},
	url = {https://doi.org/10.1162/coli.2007.33.1.41},
	doi = {10.1162/coli.2007.33.1.41},
	shorttitle = {Question Answering in Restricted Domains},
	abstract = {Automated question answering has been a topic of research and development since the earliest {AI} applications. Computing power has increased since the first such systems were developed, and the general methodology has changed from the use of hand-encoded knowledge bases about simple domains to the use of text collections as the main knowledge source over more complex domains. Still, many research issues remain. The focus of this article is on the use of restricted domains for automated question answering. The article contains a historical perspective on question answering over restricted domains and an overview of the current methods and applications used in restricted domains. A main characteristic of question answering in restricted domains is the integration of domain-specific information that is either developed for question answering or that has been developed for other purposes. We explore the main methods developed to leverage this domain-specific information.},
	pages = {41--61},
	number = {1},
	journaltitle = {Computational Linguistics},
	shortjournal = {Computational Linguistics},
	author = {Mollá, Diego and Vicedo, José Luis},
	urldate = {2025-01-25},
	date = {2007-03-01},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\M2AHTZT4\\Mollá und Vicedo - 2007 - Question Answering in Restricted Domains An Overview.pdf:application/pdf},
}

@inproceedings{unger_template-based_2012,
	location = {New York, {NY}, {USA}},
	title = {Template-based question answering over {RDF} data},
	isbn = {978-1-4503-1229-5},
	url = {https://dl.acm.org/doi/10.1145/2187836.2187923},
	doi = {10.1145/2187836.2187923},
	series = {{WWW} '12},
	abstract = {As an increasing amount of {RDF} data is published as Linked Data, intuitive ways of accessing this data become more and more important. Question answering approaches have been proposed as a good compromise between intuitiveness and expressivity. Most question answering systems translate questions into triples which are matched against the {RDF} data to retrieve an answer, typically relying on some similarity metric. However, in many cases, triples do not represent a faithful representation of the semantic structure of the natural language question, with the result that more expressive queries can not be answered. To circumvent this problem, we present a novel approach that relies on a parse of the question to produce a {SPARQL} template that directly mirrors the internal structure of the question. This template is then instantiated using statistical entity identification and predicate detection. We show that this approach is competitive and discuss cases of questions that can be answered with our approach but not with competing approaches.},
	pages = {639--648},
	booktitle = {Proceedings of the 21st international conference on World Wide Web},
	publisher = {Association for Computing Machinery},
	author = {Unger, Christina and Bühmann, Lorenz and Lehmann, Jens and Ngonga Ngomo, Axel-Cyrille and Gerber, Daniel and Cimiano, Philipp},
	urldate = {2025-01-25},
	date = {2012-04-16},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ULBUFL56\\Unger et al. - 2012 - Template-based question answering over RDF data.pdf:application/pdf},
}

@inproceedings{zhang_question_2003,
	location = {New York, {NY}, {USA}},
	title = {Question classification using support vector machines},
	isbn = {978-1-58113-646-3},
	url = {https://dl.acm.org/doi/10.1145/860435.860443},
	doi = {10.1145/860435.860443},
	series = {{SIGIR} '03},
	abstract = {Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors ({NN}), Naive Bayes ({NB}), Decision Tree ({DT}), Sparse Network of Winnows ({SNoW}), and Support Vector Machines ({SVM}) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the {SVM} outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the {SVM} to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the {TREC} {QA} track.},
	pages = {26--32},
	booktitle = {Proceedings of the 26th annual international {ACM} {SIGIR} conference on Research and development in informaion retrieval},
	publisher = {Association for Computing Machinery},
	author = {Zhang, Dell and Lee, Wee Sun},
	urldate = {2025-01-25},
	date = {2003-07-28},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\V9Q4AG85\\Zhang und Lee - 2003 - Question classification using support vector machines.pdf:application/pdf},
}

@misc{zhu_retrieving_2021,
	title = {Retrieving and Reading: A Comprehensive Survey on Open-domain Question Answering},
	url = {http://arxiv.org/abs/2101.00774},
	doi = {10.48550/arXiv.2101.00774},
	shorttitle = {Retrieving and Reading},
	abstract = {Open-domain Question Answering ({OpenQA}) is an important task in Natural Language Processing ({NLP}), which aims to answer a question in the form of natural language based on large-scale unstructured documents. Recently, there has been a surge in the amount of research literature on {OpenQA}, particularly on techniques that integrate with neural Machine Reading Comprehension ({MRC}). While these research works have advanced performance to new heights on benchmark datasets, they have been rarely covered in existing surveys on {QA} systems. In this work, we review the latest research trends in {OpenQA}, with particular attention to systems that incorporate neural {MRC} techniques. Specifically, we begin with revisiting the origin and development of {OpenQA} systems. We then introduce modern {OpenQA} architecture named "Retriever-Reader" and analyze the various systems that follow this architecture as well as the specific techniques adopted in each of the components. We then discuss key challenges to developing {OpenQA} systems and offer an analysis of benchmarks that are commonly used. We hope our work would enable researchers to be informed of the recent advancement and also the open challenges in {OpenQA} research, so as to stimulate further progress in this field.},
	number = {{arXiv}:2101.00774},
	publisher = {{arXiv}},
	author = {Zhu, Fengbin and Lei, Wenqiang and Wang, Chao and Zheng, Jianming and Poria, Soujanya and Chua, Tat-Seng},
	urldate = {2025-01-25},
	date = {2021-05-08},
	eprinttype = {arxiv},
	eprint = {2101.00774 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\YFT9UH6M\\Zhu et al. - 2021 - Retrieving and Reading A Comprehensive Survey on Open-domain Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8KVXAFE2\\2101.html:text/html},
}

@article{mavi_multi-hop_2024,
	title = {Multi-hop Question Answering},
	volume = {17},
	issn = {1554-0669, 1554-0677},
	url = {https://www.nowpublishers.com/article/Details/INR-102},
	doi = {10.1561/1500000102},
	abstract = {Multi-hop Question Answering},
	pages = {457--586},
	number = {5},
	journaltitle = {Foundations and Trends® in Information Retrieval},
	shortjournal = {{INR}},
	author = {Mavi, Vaibhav and Jangra, Anubhav and Jatowt, Adam},
	urldate = {2025-01-25},
	date = {2024-06-12},
	note = {Publisher: Now Publishers, Inc.},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\Q6Y8HH3F\\Mavi et al. - 2024 - Multi-hop Question Answering.pdf:application/pdf},
}

@article{chakraborty_introduction_2021,
	title = {Introduction to neural network-based question answering over knowledge graphs},
	volume = {11},
	rights = {© 2021 The Authors. {WIREs} Data Mining and Knowledge Discovery published by Wiley Periodicals {LLC}.},
	issn = {1942-4795},
	doi = {10.1002/widm.1389},
	abstract = {Question answering has emerged as an intuitive way of querying structured data sources and has attracted significant advancements over the years. A large body of recent work on question answering over knowledge graphs ({KGQA}) employs neural network-based systems. In this article, we provide an overview of these neural network-based methods for {KGQA}. We introduce readers to the formalism and the challenges of the task, different paradigms and approaches, discuss notable advancements, and outline the emerging trends in the field. Through this article, we aim to provide newcomers to the field with a suitable entry point to semantic parsing for {KGQA}, and ease their process of making informed decisions while creating their own {QA} systems. This article is categorized under: Technologies {\textgreater} Machine Learning Technologies {\textgreater} Prediction Technologies {\textgreater} Artificial Intelligence},
	pages = {e1389},
	number = {3},
	journaltitle = {{WIREs} Data Mining and Knowledge Discovery},
	author = {Chakraborty, Nilesh and Lukovnikov, Denis and Maheshwari, Gaurav and Trivedi, Priyansh and Lehmann, Jens and Fischer, Asja},
	date = {2021},
	langid = {english},
	keywords = {deep learning, knowledge graphs, no type information, question answering},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GD8N2ZSF\\Chakraborty et al. - 2021 - Introduction to neural network-based question answering over knowledge graphs.pdf:application/pdf},
}

@online{noauthor_finding_nodate,
	title = {Finding An Answer Based on the Recognition of the Question Focus - Archive ouverte {HAL}},
	url = {https://hal.science/hal-02458025/},
	urldate = {2025-01-25},
}

@inproceedings{ferret_finding_2001,
	title = {Finding An Answer Based on the Recognition of the Question Focus},
	url = {https://www.semanticscholar.org/paper/Finding-An-Answer-Based-on-the-Recognition-of-the-Ferret-Grau/5fd3474b58acece35004ac4a08f742be8d58d1cc},
	abstract = {In this report we describe how the {QALC} system (the Question-Answering program of the {LIR} group at {LIMSI}-{CNRS}, already involved in the {QA}-track evaluation at {TREC}9), was improved in order to better extract the very answer in selected sentences. The purpose of the main Question-Answering track in {TREC}10 was to find text sequences no longer than 50 characters or to produce a "no answer" response in case of a lack of answer in the {TREC} corpus.},
	eventtitle = {Text Retrieval Conference},
	author = {Ferret, Olivier and Grau, Brigitte and Hurault-Plantet, Martine and Illouz, Gabriel and Monceaux, Laura and Robba, Isabelle and Vilnat, A.},
	urldate = {2025-01-25},
	date = {2001},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PAPDKUMU\\Ferret et al. - 2001 - Finding An Answer Based on the Recognition of the Question Focus.pdf:application/pdf},
}

@inproceedings{chen_open-domain_2020,
	location = {Online},
	title = {Open-Domain Question Answering},
	url = {https://aclanthology.org/2020.acl-tutorials.8/},
	doi = {10.18653/v1/2020.acl-tutorials.8},
	abstract = {This tutorial provides a comprehensive and coherent overview of cutting-edge research in open-domain question answering ({QA}), the task of answering questions using a large collection of documents of diversified topics. We will start by first giving a brief historical background, discussing the basic setup and core technical challenges of the research problem, and then describe modern datasets with the common evaluation metrics and benchmarks. The focus will then shift to cutting-edge models proposed for open-domain {QA}, including two-stage retriever-reader approaches, dense retriever and end-to-end training, and retriever-free methods. Finally, we will cover some hybrid approaches using both text and large knowledge bases and conclude the tutorial with important open questions. We hope that the tutorial will not only help the audience to acquire up-to-date knowledge but also provide new perspectives to stimulate the advances of open-domain {QA} research in the next phase.},
	pages = {34--37},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Danqi and Yih, Wen-tau},
	editor = {Savary, Agata and Zhang, Yue},
	urldate = {2025-01-26},
	date = {2020-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\G8PYH4NM\\Chen und Yih - 2020 - Open-Domain Question Answering.pdf:application/pdf},
}

@article{lopez_evaluating_2013,
	title = {Evaluating question answering over linked data},
	volume = {21},
	issn = {1570-8268},
	url = {https://www.sciencedirect.com/science/article/pii/S157082681300022X},
	doi = {10.1016/j.websem.2013.05.006},
	series = {Special Issue on Evaluation of Semantic Technologies},
	abstract = {The availability of large amounts of open, distributed, and structured semantic data on the web has no precedent in the history of computer science. In recent years, there have been important advances in semantic search and question answering over {RDF} data. In particular, natural language interfaces to online semantic data have the advantage that they can exploit the expressive power of Semantic Web data models and query languages, while at the same time hiding their complexity from the user. However, despite the increasing interest in this area, there are no evaluations so far that systematically evaluate this kind of systems, in contrast to traditional question answering and search interfaces to document spaces. To address this gap, we have set up a series of evaluation challenges for question answering over linked data. The main goal of the challenge was to get insight into the strengths, capabilities, and current shortcomings of question answering systems as interfaces to query linked data sources, as well as benchmarking how these interaction paradigms can deal with the fact that the amount of {RDF} data available on the web is very large and heterogeneous with respect to the vocabularies and schemas used. Here, we report on the results from the first and second of such evaluation campaigns. We also discuss how the second evaluation addressed some of the issues and limitations which arose from the first one, as well as the open issues to be addressed in future competitions.},
	pages = {3--13},
	journaltitle = {Journal of Web Semantics},
	shortjournal = {Journal of Web Semantics},
	author = {Lopez, Vanessa and Unger, Christina and Cimiano, Philipp and Motta, Enrico},
	urldate = {2025-01-26},
	date = {2013-08-01},
	keywords = {Question answering, Evaluation, Linked data, Natural language, Semantic Web},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\HJIV7U9D\\Lopez et al. - 2013 - Evaluating question answering over linked data.pdf:application/pdf},
}

@article{li_learning_2006,
	title = {Learning question classifiers: the role of semantic information},
	volume = {12},
	issn = {1469-8110, 1351-3249},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/learning-question-classifiers-the-role-of-semantic-information/F3E3EBFC2061BBF74A6EB926A3A3B291},
	doi = {10.1017/S1351324905003955},
	shorttitle = {Learning question classifiers},
	abstract = {To respond correctly to a free form factual question given a large collection of text data, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer. This work presents a machine learning approach to question classification. Guided by a layered semantic hierarchy of answer types, we develop a hierarchical classifier that classifies questions into fine-grained classes. This work also performs a systematic study of the use of semantic information sources in natural language classification tasks. It is shown that, in the context of question classification, augmenting the input of the classifier with appropriate semantic category information results in significant improvements to classification accuracy. We show accurate results on a large collection of free-form questions used in {TREC} 10 and 11.},
	pages = {229--249},
	number = {3},
	journaltitle = {Natural Language Engineering},
	author = {Li, Xin and Roth, Dan},
	urldate = {2025-01-26},
	date = {2006-09},
	langid = {english},
	keywords = {no type information},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\LUYXMDNY\\Li und Roth - 2006 - Learning question classifiers the role of semantic information.pdf:application/pdf},
}

@article{kwiatkowski_natural_2019,
	title = {Natural Questions: A Benchmark for Question Answering Research},
	volume = {7},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43518},
	doi = {10.1162/tacl_a_00276},
	shorttitle = {Natural Questions},
	abstract = {We present the Natural Questions corpus, a question answering data set. Questions consist of real anonymized, aggregated queries issued to the Google search engine. An annotator is presented with a question along with a Wikipedia page from the top 5 search results, and annotates a long answer (typically a paragraph) and a short answer (one or more entities) if present on the page, or marks null if no long/short answer is present. The public release consists of 307,373 training examples with single annotations; 7,830 examples with 5-way annotations for development data; and a further 7,842 examples with 5-way annotated sequestered as test data. We present experiments validating quality of the data. We also describe analysis of 25-way annotations on 302 examples, giving insights into human variability on the annotation task. We introduce robust metrics for the purposes of evaluating question answering systems; demonstrate high human upper bounds on these metrics; and establish baseline results using competitive methods drawn from related literature.},
	pages = {453--466},
	journaltitle = {Transactions of the Association for Computational Linguistics},
	shortjournal = {Transactions of the Association for Computational Linguistics},
	author = {Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
	urldate = {2025-01-26},
	date = {2019-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\WK6R49PT\\Kwiatkowski et al. - 2019 - Natural Questions A Benchmark for Question Answering Research.pdf:application/pdf},
}

@inproceedings{yih_value_2016-1,
	location = {Berlin, Germany},
	title = {The Value of Semantic Parse Labeling for Knowledge Base Question Answering},
	doi = {10.18653/v1/P16-2033},
	abstract = {We demonstrate the value of collecting semantic parse labels for knowledge base question answering. In particular, (1) unlike previous studies on small-scale datasets, we show that learning from labeled semantic parses significantly improves overall performance, resulting in absolute 5 point gain compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering.},
	eventtitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages = {201--206},
	booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Yih, Wen-tau and Richardson, Matthew and Meek, Chris and Chang, Ming-Wei and Suh, Jina},
	date = {2016},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HP2EE935\\Yih et al. - 2016 - The Value of Semantic Parse Labeling for Knowledge Base Question Answering.pdf:application/pdf},
}

@article{gu_beyond_2021-1,
	title = {Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases},
	url = {https://dl.acm.org/doi/10.1145/3442381.3449992},
	doi = {10.1145/3442381.3449992},
	shorttitle = {Beyond I.I.D.},
	abstract = {Existing studies on question answering on knowledge bases ({KBQA}) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale {KBs} because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that {KBQA} models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of {KBQA} models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, {GrailQA}, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel {BERT}-based {KBQA} model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like {BERT} in the generalization of {KBQA}.1},
	pages = {3477--3488},
	journaltitle = {Proceedings of the Web Conference 2021},
	author = {Gu, Yu and Kase, Sue and Vanni, Michelle and Sadler, Brian and Liang, Percy and Yan, Xifeng and Su, Yu},
	urldate = {2025-01-26},
	date = {2021-04-19},
	langid = {english},
	note = {Conference Name: {WWW} '21: The Web Conference 2021
{ISBN}: 9781450383127
Place: Ljubljana Slovenia
Publisher: {ACM}},
	file = {Eingereichte Version:C\:\\Users\\Marco\\Zotero\\storage\\XK9QHNZ8\\Gu et al. - 2021 - Beyond I.I.D. Three Levels of Generalization for Question Answering on Knowledge Bases.pdf:application/pdf},
}

@misc{emonet_llm-based_2024,
	title = {{LLM}-based {SPARQL} Query Generation from Natural Language over Federated Knowledge Graphs},
	url = {http://arxiv.org/abs/2410.06062},
	doi = {10.48550/arXiv.2410.06062},
	abstract = {We introduce a Retrieval-Augmented Generation ({RAG}) system for translating user questions into accurate federated {SPARQL} queries over bioinformatics knowledge graphs ({KGs}) leveraging Large Language Models ({LLMs}). To enhance accuracy and reduce hallucinations in query generation, our system utilises metadata from the {KGs}, including query examples and schema information, and incorporates a validation step to correct generated queries. The system is available online at chat.expasy.org.},
	number = {{arXiv}:2410.06062},
	publisher = {{arXiv}},
	author = {Emonet, Vincent and Bolleman, Jerven and Duvaud, Severine and Farias, Tarcisio Mendes de and Sima, Ana Claudia},
	urldate = {2025-01-26},
	date = {2024-10-21},
	eprinttype = {arxiv},
	eprint = {2410.06062 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Databases, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XGBZJIGP\\Emonet et al. - 2024 - LLM-based SPARQL Query Generation from Natural Language over Federated Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\QRXXID7D\\2410.html:text/html},
}

@misc{jia_leveraging_2024-1,
	title = {Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph},
	url = {http://arxiv.org/abs/2405.15374},
	doi = {10.48550/arXiv.2405.15374},
	abstract = {The proposed research aims to develop an innovative semantic query processing system that enables users to obtain comprehensive information about research works produced by Computer Science ({CS}) researchers at the Australian National University ({ANU}). The system integrates Large Language Models ({LLMs}) with the {ANU} Scholarly Knowledge Graph ({ASKG}), a structured repository of all research-related artifacts produced at {ANU} in the {CS} field. Each artifact and its parts are represented as textual nodes stored in a Knowledge Graph ({KG}). To address the limitations of traditional scholarly {KG} construction and utilization methods, which often fail to capture fine-grained details, we propose a novel framework that integrates the Deep Document Model ({DDM}) for comprehensive document representation and the {KG}-enhanced Query Processing ({KGQP}) for optimized complex query handling. {DDM} enables a fine-grained representation of the hierarchical structure and semantic relationships within academic papers, while {KGQP} leverages the {KG} structure to improve query accuracy and efficiency with {LLMs}. By combining the {ASKG} with {LLMs}, our approach enhances knowledge utilization and natural language understanding capabilities. The proposed system employs an automatic {LLM}-{SPARQL} fusion to retrieve relevant facts and textual nodes from the {ASKG}. Initial experiments demonstrate that our framework is superior to baseline methods in terms of accuracy retrieval and query efficiency. We showcase the practical application of our framework in academic research scenarios, highlighting its potential to revolutionize scholarly knowledge management and discovery. This work empowers researchers to acquire and utilize knowledge from documents more effectively and provides a foundation for developing precise and reliable interactions with {LLMs}.},
	number = {{arXiv}:2405.15374},
	publisher = {{arXiv}},
	author = {Jia, Runsong and Zhang, Bowen and Méndez, Sergio J. Rodríguez and Omran, Pouya G.},
	urldate = {2025-01-26},
	date = {2024-05-24},
	eprinttype = {arxiv},
	eprint = {2405.15374 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, no type information},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\TDBT34CI\\Jia et al. - 2024 - Leveraging Large Language Models for Semantic Query Processing in a Scholarly Knowledge Graph.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\L5WIMD2R\\2405.html:text/html},
}

@article{frank_question_2007,
	title = {Question answering from structured knowledge sources},
	volume = {5},
	issn = {1570-8683},
	url = {https://www.sciencedirect.com/science/article/pii/S157086830500090X},
	doi = {10.1016/j.jal.2005.12.006},
	series = {Questions and Answers: Theoretical and Applied Perspectives},
	abstract = {We present an implemented approach for domain-restricted question answering from structured knowledge sources, based on robust semantic analysis in a hybrid {NLP} system architecture. We perform question interpretation and answer extraction in an architecture that builds on a lexical-conceptual structure for question interpretation, which is interfaced with domain-specific concepts and properties in a structured knowledge base. Question interpretation involves a limited amount of domain-specific inferences, and accounts for higher-level quantificational questions. Question interpretation and answer extraction are modular components that interact in clearly defined ways. We derive so-called proto queries from the linguistic representations, which provide partial constraints for answer extraction from the underlying knowledge sources. The search queries we construct from proto queries effectively compute minimal spanning trees from the underlying knowledge sources. Our approach naturally extends to multilingual question answering, and has been developed as a prototype system for two application domains: the domain of Nobel prize winners, and the domain of Language Technology, on the basis of the large ontology underlying the information portal {LT} World.},
	pages = {20--48},
	number = {1},
	journaltitle = {Journal of Applied Logic},
	shortjournal = {Journal of Applied Logic},
	author = {Frank, Anette and Krieger, Hans-Ulrich and Xu, Feiyu and Uszkoreit, Hans and Crysmann, Berthold and Jörg, Brigitte and Schäfer, Ulrich},
	urldate = {2025-01-26},
	date = {2007-03-01},
	keywords = {no type information, Data base queries, Hybrid {NLP}, Multilinguality, Ontology modeling, {QA}, Question semantics, {RMRS}},
	file = {ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\NTWNJ2GR\\S157086830500090X.html:text/html},
}

@inproceedings{garcia_aqualog_2006,
	location = {New York City, {USA}},
	title = {{AquaLog}: An ontology-driven Question Answering System to interface the Semantic Web},
	url = {https://aclanthology.org/N06-4005/},
	shorttitle = {{AquaLog}},
	pages = {269--272},
	booktitle = {Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Demonstrations},
	publisher = {Association for Computational Linguistics},
	author = {Garcia, Vanessa Lopez and Motta, Enrico and Uren, Victoria},
	editor = {Rudnicky, Alex and Dowding, John and Milic-Frayling, Natasa},
	urldate = {2025-01-26},
	date = {2006-06},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\JNKRABN4\\Garcia et al. - 2006 - AquaLog An ontology-driven Question Answering System to interface the Semantic Web.pdf:application/pdf},
}

@inproceedings{feng_multi-hop_2022,
	location = {Seattle, United States},
	title = {Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge},
	url = {https://aclanthology.org/2022.findings-naacl.12/},
	doi = {10.18653/v1/2022.findings-naacl.12},
	abstract = {Open-domain question answering systems need to answer question of our interests with structured and unstructured information. However, existing approaches only select one source to generate answer or only conduct reasoning on structured information. In this paper, we pro- pose a Document-Entity Heterogeneous Graph Network, referred to as {DEHG}, to effectively integrate different sources of information, and conduct reasoning on heterogeneous information. {DEHG} employs a graph constructor to integrate structured and unstructured information, a context encoder to represent nodes and question, a heterogeneous information reasoning layer to conduct multi-hop reasoning on both information sources, and an answer decoder to generate answers for the question. Experimental results on {HybirdQA} dataset show that {DEHG} outperforms the state-of-the-art methods.},
	eventtitle = {Findings 2022},
	pages = {151--156},
	booktitle = {Findings of the Association for Computational Linguistics: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Yue and Han, Zhen and Sun, Mingming and Li, Ping},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	urldate = {2025-01-26},
	date = {2022-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\3FQGH4C2\\Feng et al. - 2022 - Multi-Hop Open-Domain Question Answering over Structured and Unstructured Knowledge.pdf:application/pdf},
}

@inproceedings{ester_density-based_1996,
	location = {Portland, Oregon},
	title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
	series = {{KDD}'96},
	abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm {DBSCAN} relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. {DBSCAN} requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of {DBSCAN} using synthetic data and real data of the {SEQUOIA} 2000 benchmark. The results of our experiments demonstrate that (1) {DBSCAN} is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm {CLAR}-{ANS}, and that (2) {DBSCAN} outperforms {CLARANS} by a factor of more than 100 in terms of efficiency.},
	pages = {226--231},
	booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
	publisher = {{AAAI} Press},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
	urldate = {2025-01-28},
	date = {1996-08-02},
}

@article{basili_methodology_1984,
	title = {A Methodology for Collecting Valid Software Engineering Data},
	volume = {{SE}-10},
	issn = {1939-3520},
	doi = {10.1109/TSE.1984.5010301},
	abstract = {An effective data collection method for evaluating software development methodologies and for studying the software development process is described. The method uses goal-directed data collection to evaluate methodologies with respect to the claims made for them. Such claims are used as a basis for defining the goals of the data collection, establishing a list of questions of interest to be answered by data analysis, defining a set of data categorization schemes, and designing a data collection form. The data to be collected are based on the changes made to the software during development, and are obtained when the changes are made. To ensure accuracy of the data, validation is performed concurrently with software development and data collection. Validation is based on interviews with those people supplying the data. Results from using the methodology show that data validation is a necessary part of change data collection. Without it, as much as 50 percent of the data may be erroneous. Feasibility of the data collection methodology was demonstrated by applying it to five different projects in two different environments. The application showed that the methodology was both feasible and useful.},
	pages = {728--738},
	number = {6},
	journaltitle = {{IEEE} Transactions on Software Engineering},
	author = {Basili, Victor R. and Weiss, David M.},
	date = {1984-11},
	keywords = {Application software, Control systems, Data collection, data collection methodology, Data engineering, error analysis, error classification, Error correction, Error correction codes, Laboratories, Programming, Software engineering, software engineering experimentation, Software maintenance, Software testing},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\Y9PXC2V3\\Basili und Weiss - 1984 - A Methodology for Collecting Valid Software Engineering Data.pdf:application/pdf},
}

@article{basili_goal_nodate,
	title = {{THE} {GOAL} {QUESTION} {METRIC} {APPROACH}},
	author = {Basili, Victor R and Caldiera, Gianluigi and Rombach, H Dieter},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\AQ2V2P8W\\Basili et al. - THE GOAL QUESTION METRIC APPROACH.pdf:application/pdf},
}

@inproceedings{bass_software_1999,
	title = {Software architecture in practice},
	url = {https://www.semanticscholar.org/paper/Software-architecture-in-practice-Bass-Clements/bbb9a237eb0cb75812d05c2d6428253bb1627a56},
	abstract = {From the Book: 
 
Our goals for the first edition were threefold. First, we wanted to show through authentic case studies actual examples of software architectures solving real-world problems. Second, we wanted to establish and show the strong connection between an architecture and an organization's business goals. And third, we wanted to explain the importance of software architecture in achieving the quality goals for a system. 
 
Our goals for this second edition are the same, but the passage of time since the writing of the first edition has brought new developments in the field and new understanding of the important underpinnings of software architecture. We reflect the new developments with new case studies and the new understanding both through new chapters and through additions to and elaboration of the existing chapters. 
 
Architecture analysis, design, reconstruction, and documentation have all had major developments since the first edition. Architecture analysis has developed into a mature field with industrial-strength methods. This is reflected by a new chapter about the architecture tradeoff analysis method ({ATAM}). The {ATAM} has been adopted by industrial organizations as a technique for evaluating their software architectures. 
 
Architecture design has also had major developments since the first edition. The capturing of quality requirements, the achievement of those requirements through small-scale and large-scale architectural approaches (tactics and patterns, respectively), and a design method that reflects knowledge of how to achieve qualities are all captured in various chapters. Three new chapters treat understanding quality requirements, achieving qualities, and theattribute driven design ({ADD}) method, respectively. 
 
Architecture reconstruction or reverse engineering is an essential activity for capturing undocumented architectures. It can be used as a portion of a design project, an analysis project, or to provide input into a decision process to determine what to use as a basis for reconstructing an existing system. In the first edition, we briefly mentioned a tool set (Dali) and its uses in the re-engineering context; in in this edition the topic merits its own chapter. 
 
Documenting software architectures is another topic that has matured considerably in the recent past. When the first edition was published, the Unified Modeling Language ({UML}) was just arriving on the scene. Now it is firmly entrenched, a reality reflected by all-new diagrams. But more important, an understanding of what kind of information to capture about an architecture, beyond what notation to use, has emerged. A new chapter covers architecture documentation. 
 
The understanding of the application of software architecture to enable organizations to efficiently produce a variety of systems based on a single architecture is summarized in a totally rewritten chapter on software product lines. The chapter reinforces the link between architecture and an organization's business goals, as product lines, based around a software architecture, can enable order-of-magnitude improvements in cost, quality, and time to market. 
 
In addition to the architectural developments, the technology for constructing distributed and Web-based systems has become prominent in today's economy. We reflect this trend by updating the World Wide Web chapter, by using Web-based examples for the {ATAM} chapter and the chapter on building systems from components, by replacing the {CORBA} case study with one on Enterprise {JavaBeans} ({EJB}), and by introducing a case study on a wireless {EJB} system designed to support wearable computers for maintenance technicians. 
 
Finally, we have added a chapter that looks more closely at the financial aspects of architectures. There we introduce a method--the {CBAM}--for basing architectural decisions on economic criteria, in addition to the technical criteria that we had focused on previously. 
 
As in the first edition, we use the architecture business cycle as a unifying motif and all of the case studies are described in terms of the quality goals that motivated the system design and how the architecture for the system achieves those quality goals. 
 
In this edition, as in the first, we were very aware that our primary audience is practitioners, so we focus on presenting material that has been found useful in many industrial applications, as well as what we expect practice to be in the near future. 
 
We hope that you enjoy reading it at least as much as we enjoyed writing it. 
 
 
0321154959P12162002},
	eventtitle = {{SEI} series in software engineering},
	author = {Bass, L. and Clements, P. and Kazman, R.},
	urldate = {2025-01-31},
	date = {1999},
	file = {Software_Architecture_In_Practice:C\:\\Users\\Marco\\Zotero\\storage\\GMAWRU9R\\Software_Architecture_In_Practice.pdf:application/pdf},
}

@book{juristo_basics_2010,
	edition = {1st},
	title = {Basics of Software Engineering Experimentation},
	isbn = {978-1-4419-5011-6},
	abstract = {Basics of Software Engineering Experimentation is a practical guide to experimentation in a field which has long been underpinned by suppositions, assumptions, speculations and beliefs. It demonstrates to software engineers how Experimental Design and Analysis can be used to validate their beliefs and ideas. The book does not assume its readers have an in-depth knowledge of mathematics, specifying the conceptual essence of the techniques to use in the design and analysis of experiments and keeping the mathematical calculations clear and simple. Basics of Software Engineering Experimentation is practically oriented and is specially written for software engineers, all the examples being based on real and fictitious software engineering experiments.},
	pagetotal = {420},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Juristo, Natalia and Moreno, Ana M.},
	date = {2010-11},
}

@inproceedings{petersen_systematic_2008,
	title = {Systematic Mapping Studies in Software Engineering},
	url = {https://www.scienceopen.com/hosted-document?doi=10.14236/ewic/EASE2008.8},
	doi = {10.14236/ewic/EASE2008.8},
	abstract = {{BACKGROUND}: A software engineering systematic map is a defined method to build a classification scheme and structure a software engineering field of interest. The analysis of results focuses on frequencies of publications for categories within the scheme. Thereby, the coverage of the research field can be determined. Different facets of the scheme can also be combined to answer more specific research questions. {OBJECTIVE}: We describe how to conduct a systematic mapping study in software engineering and provide guidelines. We also compare systematic maps and systematic reviews to clarify how to chose between them. This comparison leads to a set of guidelines for systematic maps. {METHOD}: We have defined a systematic mapping process and applied it to complete a systematic mapping study. Furthermore, we compare systematic maps with systematic reviews by systematically analyzing existing systematic reviews. {RESULTS}: We describe a process for software engineering systematic mapping studies and compare it to systematic reviews. Based on this, guidelines for conducting systematic maps are defined. {CONCLUSIONS}: Systematic maps and reviews are different in terms of goals, breadth, validity issues and implications. Thus, they should be used complementarily and require different methods (e.g., for analysis).},
	eventtitle = {12th International Conference on Evaluation and Assessment in Software Engineering ({EASE})},
	publisher = {{BCS} Learning \& Development},
	author = {Petersen, Kai and Feldt, Robert and Mujtaba, Shahid and Mattsson, Michael},
	urldate = {2025-02-03},
	date = {2008-06-01},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\43YK7GIS\\Petersen et al. - 2008 - Systematic Mapping Studies in Software Engineering.pdf:application/pdf},
}

@article{petersen_guidelines_2015,
	title = {Guidelines for conducting systematic mapping studies in software engineering: An update},
	volume = {64},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584915000646},
	doi = {10.1016/j.infsof.2015.03.007},
	shorttitle = {Guidelines for conducting systematic mapping studies in software engineering},
	abstract = {Objective: To identify how the systematic mapping process is conducted (including search, study selection, analysis and presentation of data, etc.); to identify improvement potentials in conducting the systematic mapping process and updating the guidelines accordingly.
Method: We conducted a systematic mapping study of systematic maps, considering some practices of systematic review guidelines as well (in particular in relation to deﬁning the search and to conduct a quality assessment).
Results: In a large number of studies multiple guidelines are used and combined, which leads to different ways in conducting mapping studies. The reason for combining guidelines was that they differed in the recommendations given.
Conclusion: The most frequently followed guidelines are not sufﬁcient alone. Hence, there was a need to provide an update of how to conduct systematic mapping studies. New guidelines have been proposed consolidating existing ﬁndings.},
	pages = {1--18},
	journaltitle = {Information and Software Technology},
	shortjournal = {Information and Software Technology},
	author = {Petersen, Kai and Vakkalanka, Sairam and Kuzniarz, Ludwik},
	urldate = {2025-02-03},
	date = {2015-08},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\J6ANA8GZ\\Petersen et al. - 2015 - Guidelines for conducting systematic mapping studies in software engineering An update.pdf:application/pdf},
}

@article{theisen_software_2018,
	title = {Software Engineering Research at the International Conference on Software Engineering in 2016},
	volume = {42},
	issn = {0163-5948},
	url = {https://dl.acm.org/doi/10.1145/3149485.3149496},
	doi = {10.1145/3149485.3149496},
	abstract = {With the goal of helping software engineering researchers understand how to improve their papers, Mary Shaw presented "Writing Good Software Engineering Research Papers" in 2003. Shaw analyzed the abstracts of the papers submitted to the 2002 International Conference of Software Engineering ({ICSE}) to determine trends in research question type, contribution type, and validation approach. We revisit Shaw's work to see how the software engineering research community has evolved since 2002. The goal of this paper is to aid software engineering researchers in understanding trends in research question design, research question type, and validation approach by analyzing the abstracts of the papers submitted to {ICSE} 2016. We implemented Shaw's recommendation for replicating her study through the use of multiple coders and the calculation of inter-rater reliability and demonstrate that her approach can be repeated. Our results indicate that reviewers have increased expectations that papers have solid evaluations of the research contribution. Additionally, the 2016 results include at least 17\% mining software repository ({MSR}) papers, a category of papers not seen in 2002. The advent of {MSR} papers has increased the use of generalization/characterization research questions, the production of empirical report contribution, and validation by evaluation.},
	pages = {1--7},
	number = {4},
	journaltitle = {{ACM} {SIGSOFT} Software Engineering Notes},
	shortjournal = {{SIGSOFT} Softw. Eng. Notes},
	author = {Theisen, Christopher and Dunaiski, Marcel and Williams, Laurie and Visser, Willem},
	urldate = {2025-02-08},
	date = {2018-01-11},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\FKSE8BGF\\Theisen et al. - 2018 - Software Engineering Research at the International Conference on Software Engineering in 2016.pdf:application/pdf},
}

@book{montgomery_design_2017,
	title = {Design and Analysis of Experiments},
	isbn = {978-1-119-11347-8},
	abstract = {"The eighth edition of Design and Analysis of Experiments continues to provide extensive and in-depth information on engineering, business, and statistics-as well as informative ways to help readers design and analyze experiments for improving the quality, efficiency and performance of working systems. Furthermore, the text maintains its comprehensive coverage by including: new examples, exercises, and problems (including in the areas of biochemistry and biotechnology); new topics and problems in the area of response surface; new topics in nested and split-plot design; and the residual maximum likelihood method is now emphasized throughout the book"--},
	pagetotal = {752},
	publisher = {John Wiley \& Sons},
	author = {Montgomery, Douglas C.},
	date = {2017},
	langid = {english},
	note = {Google-Books-{ID}: Py7bDgAAQBAJ},
	keywords = {Mathematics / Probability \& Statistics / General, Science / Research \& Methodology, Technology \& Engineering / Industrial Engineering, Technology \& Engineering / Mechanical},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\MSJ4CC9K\\Montgomery - 2017 - Design and Analysis of Experiments.pdf:application/pdf},
}

@book{dean_design_2017,
	location = {Cham},
	title = {Design and Analysis of Experiments},
	rights = {http://www.springer.com/tdm},
	isbn = {978-3-319-52248-7 978-3-319-52250-0},
	url = {http://link.springer.com/10.1007/978-3-319-52250-0},
	series = {Springer Texts in Statistics},
	publisher = {Springer International Publishing},
	author = {Dean, Angela and Voss, Daniel and Draguljić, Danel},
	urldate = {2025-02-16},
	date = {2017},
	langid = {english},
	doi = {10.1007/978-3-319-52250-0},
	keywords = {Analysis of covariance, Analysis of Experiments, Blocking factors, Design of Experiments, Factorial experiments, Fitting models, Inference for Contrasts, Planning experiments, Polynomial regression, Random effects, Variation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SULLFG4R\\Dean et al. - 2017 - Design and Analysis of Experiments.pdf:application/pdf},
}

@inproceedings{feldt_validity_2010,
	title = {Validity Threats in Empirical Software Engineering Research - An Initial Survey},
	abstract = {In judging the quality of a research study it is very important to consider threats to the validity of the study and the results. This is particularly important for empirical research where there is often a multitude of possible threats. With a growing focus on empirical research methods in software engineering it is important that there is a consensus in the community on this importance, that validity analysis is done by every researcher and that there is common terminology and support on how to do and report it. Even though there are previous relevant results they have primarily focused on quantitative research methods and in particular experiments. Here we look at the existing advice and guidelines and then perform a review of 43 papers published in the {ESEM} conference in 2009 and analyse the validity analysis they include and which threats and strategies for overcoming them that were given by the authors. Based on this analysis we then discuss what is working well and less well in validity analysis of empirical software engineering research and present recommendations on how to better support validity analysis in the future.},
	eventtitle = {International Conference on Software Engineering and Knowledge Engineering},
	author = {Feldt, R. and Magazinius, Ana},
	date = {2010},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\DE588H74\\Feldt und Magazinius - Validity Threats in Empirical Software Engineering Research - An Initial Survey.pdf:application/pdf},
}

@misc{chiang_chatbot_2024,
	title = {Chatbot Arena: An Open Platform for Evaluating {LLMs} by Human Preference},
	doi = {10.48550/arXiv.2403.04132},
	shorttitle = {Chatbot Arena},
	abstract = {Large Language Models ({LLMs}) have unlocked new capabilities and applications; however, evaluating the alignment with human preferences still poses significant challenges. To address this issue, we introduce Chatbot Arena, an open platform for evaluating {LLMs} based on human preferences. Our methodology employs a pairwise comparison approach and leverages input from a diverse user base through crowdsourcing. The platform has been operational for several months, amassing over 240K votes. This paper describes the platform, analyzes the data we have collected so far, and explains the tried-and-true statistical methods we are using for efficient and accurate evaluation and ranking of models. We confirm that the crowdsourced questions are sufficiently diverse and discriminating and that the crowdsourced human votes are in good agreement with those of expert raters. These analyses collectively establish a robust foundation for the credibility of Chatbot Arena. Because of its unique value and openness, Chatbot Arena has emerged as one of the most referenced {LLM} leaderboards, widely cited by leading {LLM} developers and companies. Our demo is publicly available at {\textbackslash}url\{https://chat.lmsys.org\}.},
	number = {{arXiv}:2403.04132},
	publisher = {{arXiv}},
	author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
	date = {2024-03-07},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\348ZGH2D\\Chiang et al. - 2024 - Chatbot Arena An Open Platform for Evaluating LLMs by Human Preference.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\JEJV38SW\\2403.html:text/html},
}

@misc{enevoldsen_mmteb_2025,
	title = {{MMTEB}: Massive Multilingual Text Embedding Benchmark},
	doi = {10.48550/arXiv.2502.13595},
	shorttitle = {{MMTEB}},
	abstract = {Text embeddings are typically evaluated on a limited set of tasks, which are constrained by language, domain, and task diversity. To address these limitations and provide a more comprehensive evaluation, we introduce the Massive Multilingual Text Embedding Benchmark ({MMTEB}) - a large-scale, community-driven expansion of {MTEB}, covering over 500 quality-controlled evaluation tasks across 250+ languages. {MMTEB} includes a diverse set of challenging, novel tasks such as instruction following, long-document retrieval, and code retrieval, representing the largest multilingual collection of evaluation tasks for embedding models to date. Using this collection, we develop several highly multilingual benchmarks, which we use to evaluate a representative set of models. We find that while large language models ({LLMs}) with billions of parameters can achieve state-of-the-art performance on certain language subsets and task categories, the best-performing publicly available model is multilingual-e5-large-instruct with only 560 million parameters. To facilitate accessibility and reduce computational cost, we introduce a novel downsampling method based on inter-task correlation, ensuring a diverse selection while preserving relative model rankings. Furthermore, we optimize tasks such as retrieval by sampling hard negatives, creating smaller but effective splits. These optimizations allow us to introduce benchmarks that drastically reduce computational demands. For instance, our newly introduced zero-shot English benchmark maintains a ranking order similar to the full-scale version but at a fraction of the computational cost.},
	number = {{arXiv}:2502.13595},
	publisher = {{arXiv}},
	author = {Enevoldsen, Kenneth and Chung, Isaac and Kerboua, Imene and Kardos, Márton and Mathur, Ashwin and Stap, David and Gala, Jay and Siblini, Wissam and Krzemiński, Dominik and Winata, Genta Indra and Sturua, Saba and Utpala, Saiteja and Ciancone, Mathieu and Schaeffer, Marion and Sequeira, Gabriel and Misra, Diganta and Dhakal, Shreeya and Rystrøm, Jonathan and Solomatin, Roman and Çağatan, Ömer and Kundu, Akash and Bernstorff, Martin and Xiao, Shitao and Sukhlecha, Akshita and Pahwa, Bhavish and Poświata, Rafał and {GV}, Kranthi Kiran and Ashraf, Shawon and Auras, Daniel and Plüster, Björn and Harries, Jan Philipp and Magne, Loïc and Mohr, Isabelle and Hendriksen, Mariya and Zhu, Dawei and Gisserot-Boukhlef, Hippolyte and Aarsen, Tom and Kostkan, Jan and Wojtasik, Konrad and Lee, Taemin and Šuppa, Marek and Zhang, Crystina and Rocca, Roberta and Hamdy, Mohammed and Michail, Andrianos and Yang, John and Faysse, Manuel and Vatolin, Aleksei and Thakur, Nandan and Dey, Manan and Vasani, Dipam and Chitale, Pranjal and Tedeschi, Simone and Tai, Nguyen and Snegirev, Artem and Günther, Michael and Xia, Mengzhou and Shi, Weijia and Lù, Xing Han and Clive, Jordan and Krishnakumar, Gayatri and Maksimova, Anna and Wehrli, Silvan and Tikhonova, Maria and Panchal, Henil and Abramov, Aleksandr and Ostendorff, Malte and Liu, Zheng and Clematide, Simon and Miranda, Lester James and Fenogenova, Alena and Song, Guangyu and Safi, Ruqiya Bin and Li, Wen-Ding and Borghini, Alessia and Cassano, Federico and Su, Hongjin and Lin, Jimmy and Yen, Howard and Hansen, Lasse and Hooker, Sara and Xiao, Chenghao and Adlakha, Vaibhav and Weller, Orion and Reddy, Siva and Muennighoff, Niklas},
	date = {2025-02-19},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\FQC6KPMT\\Enevoldsen et al. - 2025 - MMTEB Massive Multilingual Text Embedding Benchmark.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\U7JVNWG2\\2502.html:text/html},
}

@inproceedings{lehnert_conceptual_1977,
	location = {San Francisco, {CA}, {USA}},
	title = {A conceptual theory of question answering},
	series = {{IJCAI}'77},
	abstract = {A theory of Q/A has been proposed from the perspective of natural language processing that relieson ideas in conceptual information processing and theories of human memory organization. This theory of Q/A has been implemented in a computer program, {QUALM}. {QUALM} is currently used by two story understanding systems ({SAM} and {PAM}) to complete a natural language processing system that reads stories and answers questions about what was read.},
	pages = {158--164},
	booktitle = {Proceedings of the 5th international joint conference on Artificial intelligence - Volume 1},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Lehnert, Wendy G.},
	urldate = {2025-03-02},
	date = {1977-08-22},
}

@online{noauthor_research_nodate,
	title = {Research Software Discovery: How do we Want to Search Research Software and Where do we Want to Find it? - {HedgeDoc}},
	url = {https://pad.gwdg.de/pw7fePCZQLehEA1SEkTQYw#},
	shorttitle = {Research Software Discovery},
	urldate = {2025-03-01},
}

@online{schwetje_bettys_2023,
	title = {Betty's (Re)Search Engine},
	url = {https://nfdi4ing.de/8-2/},
	abstract = {Betty is the {NFDI}4Ing task area covering the research data management challenges faced by engineers who develop their own software solutions. Betty envisions a future in which every engineer produces verified, high-quality software that can be reused and extended. To that end, Betty wants to identify and provide the missing tools, teaching material and recommendations. … Continued},
	titleaddon = {{NFDI}4ING},
	author = {Schwetje, Thorsten},
	urldate = {2025-03-01},
	date = {2023-07-04},
	langid = {american},
	file = {Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\A5GW9A5N\\8-2.html:text/html},
}

@unpublished{sack_knowledge_2024-1,
	title = {Knowledge (Graph) Driven Research Data Management in the Age of {LLMs}},
	url = {https://zenodo.org/records/14191610},
	abstract = {One of the difficulties in general research data management lies into the fact that different scientific disciplines interpret specific concepts differently. Moreover, even if you agree about the research object to be represented, there are various different ways and principles to follow. Ontologies might help to enable {FAIR} research data management, i.e. to make research data findable, accessible, interoperable, and reusable. However, in the age of large language model ({LLM}), the effort of ontology-driven development is questioned and reevaluated. This presentation will give an introduction into research data management ({RDM}) in general, and in knowledge graph-basewd {RDM} in particular. On the example of the German National Research Data Infrastructure programme, a knowledge graph-based {RDM} approach is presented, accompanied by first experiments in how {LLMs} can improve efficiency and quality of the ontological engineering lifecycle.},
	author = {Sack, Harald},
	urldate = {2025-03-01},
	date = {2024-11-20},
	doi = {10.5281/zenodo.14191610},
	keywords = {knowledge graph, Knowledge engineering, cultural heritage, generative {AI}, Knowledge Bases, Knowledge Management, large language model, nfdi, ontological engineering, Ontology},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\URUIPADW\\Sack - 2024 - Knowledge (Graph) Driven Research Data Management in the Age of LLMs.pdf:application/pdf},
}

@misc{gu_knowledge_2022,
	title = {Knowledge Base Question Answering: A Semantic Parsing Perspective},
	doi = {10.48550/arXiv.2209.04994},
	shorttitle = {Knowledge Base Question Answering},
	abstract = {Recent advances in deep learning have greatly propelled the research on semantic parsing. Improvement has since been made in many downstream tasks, including natural language interface to web {APIs}, text-to-{SQL} generation, among others. However, despite the close connection shared with these tasks, research on question answering over knowledge bases ({KBQA}) has comparatively been progressing slowly. We identify and attribute this to two unique challenges of {KBQA}, schema-level complexity and fact-level complexity. In this survey, we situate {KBQA} in the broader literature of semantic parsing and give a comprehensive account of how existing {KBQA} approaches attempt to address the unique challenges. Regardless of the unique challenges, we argue that we can still take much inspiration from the literature of semantic parsing, which has been overlooked by existing research on {KBQA}. Based on our discussion, we can better understand the bottleneck of current {KBQA} research and shed light on promising directions for {KBQA} to keep up with the literature of semantic parsing, particularly in the era of pre-trained language models.},
	number = {{arXiv}:2209.04994},
	publisher = {{arXiv}},
	author = {Gu, Yu and Pahuja, Vardaan and Cheng, Gong and Su, Yu},
	date = {2022-10-24},
	eprinttype = {arxiv},
	eprint = {2209.04994 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7KBP68RS\\Gu et al. - 2022 - Knowledge Base Question Answering A Semantic Parsing Perspective.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\5KG9WVBS\\2209.html:text/html},
}

@article{li_flexkbqa_2024,
	title = {{FlexKBQA}: A Flexible {LLM}-Powered Framework for Few-Shot Knowledge Base Question Answering},
	volume = {38},
	rights = {Copyright (c) 2024 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/29823},
	doi = {10.1609/aaai.v38i17.29823},
	shorttitle = {{FlexKBQA}},
	abstract = {Knowledge base question answering ({KBQA}) is a critical yet challenging task due to the vast number of entities within knowledge bases and the diversity of natural language questions posed by users. Unfortunately, the performance of most {KBQA} models tends to decline significantly in real-world scenarios where high-quality annotated data is insufficient. To mitigate the burden associated with manual annotation, we introduce {FlexKBQA} by utilizing Large Language Models ({LLMs}) as program translators for addressing the challenges inherent in the few-shot {KBQA} task. Specifically, {FlexKBQA} leverages automated algorithms to sample diverse programs, such as {SPARQL} queries, from the knowledge base, which are subsequently converted into natural language questions via {LLMs}. This synthetic dataset facilitates training a specialized lightweight model for the {KB}. Additionally, to reduce the barriers of distribution shift between synthetic data and real user questions, {FlexKBQA} introduces an executionguided self-training method to iterative leverage unlabeled user questions. Furthermore, we explore harnessing the inherent reasoning capability of {LLMs} to enhance the entire framework. Consequently, {FlexKBQA} delivers substantial flexibility, encompassing data annotation, deployment, and being domain agnostic. Through extensive experiments on {GrailQA}, {WebQSP}, and {KQA} Pro, we observe that under the few-shot even the more challenging zero-shot scenarios, {FlexKBQA} achieves impressive results with a few annotations, surpassing all previous baselines and even approaching the performance of supervised models, achieving a remarkable 93\% performance relative to the fully-supervised models. We posit that {FlexKBQA} represents a significant advancement towards exploring better integration of large and lightweight models. Code is available at https://github.com/leezythu/{FlexKBQA}.},
	pages = {18608--18616},
	number = {17},
	journaltitle = {Proceedings of the {AAAI} Conference on Artificial Intelligence},
	author = {Li, Zhenyu and Fan, Sunqi and Gu, Yu and Li, Xiuxing and Duan, Zhichao and Dong, Bowen and Liu, Ning and Wang, Jianyong},
	urldate = {2025-03-04},
	date = {2024-03-24},
	langid = {english},
	note = {Number: 17},
	keywords = {etc., {NLP}: Sentence-level Semantics, Textual Inference},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\6IIWKZW9\\Li et al. - 2024 - FlexKBQA A Flexible LLM-Powered Framework for Few-Shot Knowledge Base Question Answering.pdf:application/pdf},
}

@article{lan_complex_2023,
	title = {Complex Knowledge Base Question Answering: A Survey},
	volume = {35},
	issn = {1558-2191},
	url = {https://ieeexplore.ieee.org/document/9960856/?arnumber=9960856},
	doi = {10.1109/TKDE.2022.3223858},
	shorttitle = {Complex Knowledge Base Question Answering},
	abstract = {Knowledge base question answering ({KBQA}) aims to answer a question over a knowledge base ({KB}). Early studies mainly focused on answering simple questions over {KBs} and achieved great success. However, their performances on complex questions are still far from satisfactory. Therefore, in recent years, researchers propose a large number of novel methods, which looked into the challenges of answering complex questions. In this survey, we review recent advances in {KBQA} with the focus on solving complex questions, which usually contain multiple subjects, express compound relations, or involve numerical operations. In detail, we begin with introducing the complex {KBQA} task and relevant background. Then, we present two mainstream categories of methods for complex {KBQA}, namely semantic parsing-based ({SP}-based) methods and information retrieval-based ({IR}-based) methods. Specifically, we illustrate their procedures with flow designs and discuss their difference and similarity. Next, we summarize the challenges that these two categories of methods encounter when answering complex questions, and explicate advanced solutions as well as techniques used in existing work. After that, we discuss the potential impact of pre-trained language models ({PLMs}) on complex {KBQA}. To help readers catch up with {SOTA} methods, we also provide a comprehensive evaluation and resource about complex {KBQA} task. Finally, we conclude and discuss several promising directions related to complex {KBQA} for future research.},
	pages = {11196--11215},
	number = {11},
	journaltitle = {{IEEE} Transactions on Knowledge and Data Engineering},
	author = {Lan, Yunshi and He, Gaole and Jiang, Jinhao and Jiang, Jing and Zhao, Wayne Xin and Wen, Ji-Rong},
	urldate = {2025-03-04},
	date = {2023-11},
	note = {Conference Name: {IEEE} Transactions on Knowledge and Data Engineering},
	keywords = {Knowledge based systems, Semantics, Task analysis, survey, Cognition, question answering, Compounds, knowledge base, Knowledge base question answering, natural language processing, Question answering (information retrieval), {TV}},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\YAL2BN9G\\Lan et al. - 2023 - Complex Knowledge Base Question Answering A Survey.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Marco\\Zotero\\storage\\NXUR5DP5\\9960856.html:text/html},
}

@inproceedings{perez-beltrachini_semantic_2023,
	location = {Dubrovnik, Croatia},
	title = {Semantic Parsing for Conversational Question Answering over Knowledge Graphs},
	url = {https://aclanthology.org/2023.eacl-main.184/},
	doi = {10.18653/v1/2023.eacl-main.184},
	abstract = {In this paper, we are interested in developing semantic parsers which understand natural language questions embedded in a conversation with a user and ground them to formal queries over definitions in a general purpose knowledge graph ({KG}) with very large vocabularies (covering thousands of concept names and relations, and millions of entities). To this end, we develop a dataset where user questions are annotated with Sparql parses and system answers correspond to execution results thereof. We present two different semantic parsing approaches and highlight the challenges of the task: dealing with large vocabularies, modelling conversation context, predicting queries with multiple entities, and generalising to new questions at test time. We hope our dataset will serve as useful testbed for the development of conversational semantic parsers. Our dataset and models are released at https://github.com/{EdinburghNLP}/{SPICE}.},
	eventtitle = {{EACL} 2023},
	pages = {2507--2522},
	booktitle = {Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Perez-Beltrachini, Laura and Jain, Parag and Monti, Emilio and Lapata, Mirella},
	editor = {Vlachos, Andreas and Augenstein, Isabelle},
	urldate = {2025-03-06},
	date = {2023-05},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XPKIL4XG\\Perez-Beltrachini et al. - 2023 - Semantic Parsing for Conversational Question Answering over Knowledge Graphs.pdf:application/pdf},
}

@misc{deepseek-ai_deepseek-r1_2025,
	title = {{DeepSeek}-R1: Incentivizing Reasoning Capability in {LLMs} via Reinforcement Learning},
	doi = {10.48550/arXiv.2501.12948},
	shorttitle = {{DeepSeek}-R1},
	abstract = {We introduce our first-generation reasoning models, {DeepSeek}-R1-Zero and {DeepSeek}-R1. {DeepSeek}-R1-Zero, a model trained via large-scale reinforcement learning ({RL}) without supervised fine-tuning ({SFT}) as a preliminary step, demonstrates remarkable reasoning capabilities. Through {RL}, {DeepSeek}-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce {DeepSeek}-R1, which incorporates multi-stage training and cold-start data before {RL}. {DeepSeek}-R1 achieves performance comparable to {OpenAI}-o1-1217 on reasoning tasks. To support the research community, we open-source {DeepSeek}-R1-Zero, {DeepSeek}-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from {DeepSeek}-R1 based on Qwen and Llama.},
	number = {{arXiv}:2501.12948},
	publisher = {{arXiv}},
	author = {{DeepSeek}-{AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	date = {2025-01-22},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\7SA9WSNG\\DeepSeek-AI et al. - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\3MWDSIAP\\2501.html:text/html},
}

@misc{malkov_efficient_2018,
	title = {Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World graphs},
	doi = {10.48550/arXiv.1603.09320},
	abstract = {We present a new approach for the approximate K-nearest neighbor search based on navigable small world graphs with controllable hierarchy (Hierarchical {NSW}, {HNSW}). The proposed solution is fully graph-based, without any need for additional search structures, which are typically used at the coarse search stage of the most proximity graph techniques. Hierarchical {NSW} incrementally builds a multi-layer structure consisting from hierarchical set of proximity graphs (layers) for nested subsets of the stored elements. The maximum layer in which an element is present is selected randomly with an exponentially decaying probability distribution. This allows producing graphs similar to the previously studied Navigable Small World ({NSW}) structures while additionally having the links separated by their characteristic distance scales. Starting search from the upper layer together with utilizing the scale separation boosts the performance compared to {NSW} and allows a logarithmic complexity scaling. Additional employment of a heuristic for selecting proximity graph neighbors significantly increases performance at high recall and in case of highly clustered data. Performance evaluation has demonstrated that the proposed general metric space search index is able to strongly outperform previous opensource state-of-the-art vector-only approaches. Similarity of the algorithm to the skip list structure allows straightforward balanced distributed implementation.},
	number = {{arXiv}:1603.09320},
	publisher = {{arXiv}},
	author = {Malkov, Yu A. and Yashunin, D. A.},
	date = {2018-08-14},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Data Structures and Algorithms, Computer Science - Information Retrieval, Computer Science - Social and Information Networks},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RQT2DYEB\\Malkov and Yashunin - 2018 - Efficient and robust approximate nearest neighbor search using Hierarchical Navigable Small World gr.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\5FTAVURU\\1603.html:text/html},
}

@article{zhang_survey_2023,
	title = {A survey on complex factual question answering},
	volume = {4},
	issn = {2666-6510},
	doi = {10.1016/j.aiopen.2022.12.003},
	abstract = {Answering complex factual questions has drawn a lot of attention. Researchers leverage various data sources to support complex {QA}, such as unstructured texts, structured knowledge graphs and relational databases, semi-structured web tables, or even hybrid data sources. However, although the ideas behind these approaches show similarity to some extent, there is not yet a consistent strategy to deal with various data sources. In this survey, we carefully examine how complex factual question answering has evolved across various data sources. We list the similarities among these approaches and group them into the analysis–extend–reason framework, despite the various question types and data sources that they focus on. We also address future directions for difficult factual question answering as well as the relevant benchmarks.},
	pages = {1--12},
	journaltitle = {{AI} Open},
	shortjournal = {{AI} Open},
	author = {Zhang, Lingxi and Zhang, Jing and Ke, Xirui and Li, Haoyang and Huang, Xinmei and Shao, Zhonghui and Cao, Shulin and Lv, Xin},
	date = {2023-01-01},
	keywords = {Complex question, Document-based question answering, Factual question, Knowledge base question answering, Multi-source question answering, Question answering, Table question answering, Text2SQL},
	file = {ScienceDirect Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\E66YFMVI\\S2666651022000249.html:text/html},
}

@inproceedings{devlin_bert_2019,
	location = {Minneapolis, Minnesota},
	title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	doi = {10.18653/v1/N19-1423},
	shorttitle = {{BERT}},
	abstract = {We introduce a new language representation model called {BERT}, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), {BERT} is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained {BERT} model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. {BERT} is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the {GLUE} score to 80.5 (7.7 point absolute improvement), {MultiNLI} accuracy to 86.7\% (4.6\% absolute improvement), {SQuAD} v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and {SQuAD} v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	eventtitle = {{NAACL}-{HLT} 2019},
	pages = {4171--4186},
	booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
	date = {2019-06},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\DJHHR9XG\\Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf},
}

@misc{raffel_exploring_2023,
	title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	doi = {10.48550/arXiv.1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing ({NLP}). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for {NLP} by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for {NLP}, we release our data set, pre-trained models, and code.},
	number = {{arXiv}:1910.10683},
	publisher = {{arXiv}},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	date = {2023-09-19},
	eprinttype = {arxiv},
	eprint = {1910.10683 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\GWWE9RJD\\Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\AEEE79ZE\\1910.html:text/html},
}

@article{gilliland_setting_2008,
	title = {Setting the stage},
	volume = {2},
	pages = {7},
	number = {1},
	journaltitle = {Introduction to metadata},
	author = {Gilliland, Anne J},
	date = {2008},
	note = {Publisher: Online Edition, Version},
	file = {Intro to Metadata Setting the Stage.pdf:C\:\\Users\\Marco\\Zotero\\storage\\G32HKJXC\\Intro to Metadata Setting the Stage.pdf:application/pdf},
}

@book{riley_understanding_2017,
	location = {Baltimore, {MD}},
	title = {Understanding metadata: what is metadata, and what is it for},
	isbn = {978-1-937522-72-8},
	series = {{NISO} Primer series},
	shorttitle = {Understanding metadata},
	publisher = {National Information Standards Organization},
	author = {Riley, Jenn},
	date = {2017},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\L4RZY85U\\Riley - 2017 - Understanding metadata what is metadata, and what is it for.pdf:application/pdf},
}

@misc{li_graph_2023-1,
	title = {Graph Reasoning for Question Answering with Triplet Retrieval},
	url = {http://arxiv.org/abs/2305.18742},
	doi = {10.48550/arXiv.2305.18742},
	abstract = {Answering complex questions often requires reasoning over knowledge graphs ({KGs}). State-of-the-art methods often utilize entities in questions to retrieve local subgraphs, which are then fed into {KG} encoder, e.g. graph neural networks ({GNNs}), to model their local structures and integrated into language models for question answering. However, this paradigm constrains retrieved knowledge in local subgraphs and discards more diverse triplets buried in {KGs} that are disconnected but useful for question answering. In this paper, we propose a simple yet effective method to first retrieve the most relevant triplets from {KGs} and then rerank them, which are then concatenated with questions to be fed into language models. Extensive results on both {CommonsenseQA} and {OpenbookQA} datasets show that our method can outperform state-of-the-art up to 4.6\% absolute accuracy.},
	number = {{arXiv}:2305.18742},
	publisher = {{arXiv}},
	author = {Li, Shiyang and Gao, Yifan and Jiang, Haoming and Yin, Qingyu and Li, Zheng and Yan, Xifeng and Zhang, Chao and Yin, Bing},
	urldate = {2025-04-13},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2305.18742 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\RURGN7NR\\Li et al. - 2023 - Graph Reasoning for Question Answering with Triplet Retrieval.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\Z5YPZPWJ\\2305.html:text/html},
}

@inproceedings{li_unsupervised_2022,
	title = {Unsupervised Dense Retrieval for Scientific Articles},
	doi = {10.18653/v1/2022.emnlp-industry.32},
	abstract = {In this work, we build a dense retrieval based semantic search engine on scientific articles from Elsevier. The major challenge is that there is no labeled data for training and testing. We apply a state-of-the-art unsupervised dense retrieval model called Generative Pseudo Labeling that generates high-quality pseudo training labels. Furthermore, since the articles are unbalanced across different domains, we select passages from multiple domains to form balanced training data. For the evaluation, we create two test sets: one manually annotated and one automatically created from the meta-information of our data. We compare the semantic search engine with the currently deployed lexical search engine on the two test sets. The results of the experiment show that the semantic search engine trained with pseudo training labels can significantly improve search performance.},
	eventtitle = {{EMNLP} 2022},
	pages = {313--321},
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track},
	publisher = {Association for Computational Linguistics},
	author = {Li, Dan and Yadav, Vikrant and Afzal, Zubair and Tsatsaronis, George},
	date = {2022-12},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PVY5X638\\Li et al. - 2022 - Unsupervised Dense Retrieval for Scientific Articles.pdf:application/pdf},
}

@inproceedings{saxena_improving_2020,
	location = {Online},
	title = {Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings},
	url = {https://aclanthology.org/2020.acl-main.412/},
	doi = {10.18653/v1/2020.acl-main.412},
	abstract = {Knowledge Graphs ({KG}) are multi-relational graphs consisting of entities as nodes and relations among them as typed edges. Goal of the Question Answering over {KG} ({KGQA}) task is to answer natural language queries posed over the {KG}. Multi-hop {KGQA} requires reasoning over multiple edges of the {KG} to arrive at the right answer. {KGs} are often incomplete with many missing links, posing additional challenges for {KGQA}, especially for multi-hop {KGQA}. Recent research on multi-hop {KGQA} has attempted to handle {KG} sparsity using relevant external text, which isn`t always readily available. In a separate line of research, {KG} embedding methods have been proposed to reduce {KG} sparsity by performing missing link prediction. Such {KG} embedding methods, even though highly relevant, have not been explored for multi-hop {KGQA} so far. We fill this gap in this paper and propose {EmbedKGQA}. {EmbedKGQA} is particularly effective in performing multi-hop {KGQA} over sparse {KGs}. {EmbedKGQA} also relaxes the requirement of answer selection from a pre-specified neighborhood, a sub-optimal constraint enforced by previous multi-hop {KGQA} methods. Through extensive experiments on multiple benchmark datasets, we demonstrate {EmbedKGQA}`s effectiveness over other state-of-the-art baselines.},
	eventtitle = {{ACL} 2020},
	pages = {4498--4507},
	booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Saxena, Apoorv and Tripathi, Aditay and Talukdar, Partha},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	urldate = {2025-04-13},
	date = {2020-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\22K6ZH23\\Saxena et al. - 2020 - Improving Multi-hop Question Answering over Knowledge Graphs using Knowledge Base Embeddings.pdf:application/pdf},
}

@misc{hu_empirical_2023-1,
	title = {An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2303.10368},
	doi = {10.48550/arXiv.2303.10368},
	abstract = {Large-scale pre-trained language models ({PLMs}) such as {BERT} have recently achieved great success and become a milestone in natural language processing ({NLP}). It is now the consensus of the {NLP} community to adopt {PLMs} as the backbone for downstream tasks. In recent works on knowledge graph question answering ({KGQA}), {BERT} or its variants have become necessary in their {KGQA} models. However, there is still a lack of comprehensive research and comparison of the performance of different {PLMs} in {KGQA}. To this end, we summarize two basic {KGQA} frameworks based on {PLMs} without additional neural network modules to compare the performance of nine {PLMs} in terms of accuracy and efficiency. In addition, we present three benchmarks for larger-scale {KGs} based on the popular {SimpleQuestions} benchmark to investigate the scalability of {PLMs}. We carefully analyze the results of all {PLMs}-based {KGQA} basic frameworks on these benchmarks and two other popular datasets, {WebQuestionSP} and {FreebaseQA}, and find that knowledge distillation techniques and knowledge enhancement methods in {PLMs} are promising for {KGQA}. Furthermore, we test {ChatGPT}, which has drawn a great deal of attention in the {NLP} community, demonstrating its impressive capabilities and limitations in zero-shot {KGQA}. We have released the code and benchmarks to promote the use of {PLMs} on {KGQA}.},
	number = {{arXiv}:2303.10368},
	publisher = {{arXiv}},
	author = {Hu, Nan and Wu, Yike and Qi, Guilin and Min, Dehai and Chen, Jiaoyan and Pan, Jeff Z. and Ali, Zafar},
	urldate = {2025-04-13},
	date = {2023-03-18},
	eprinttype = {arxiv},
	eprint = {2303.10368 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\HTBP232M\\Hu et al. - 2023 - An Empirical Study of Pre-trained Language Models in Simple Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\GQNZEJUM\\2303.html:text/html},
}

@misc{xu_fusing_2021,
	title = {Fusing Context Into Knowledge Graph for Commonsense Question Answering},
	url = {http://arxiv.org/abs/2012.04808},
	doi = {10.48550/arXiv.2012.04808},
	abstract = {Commonsense question answering ({QA}) requires a model to grasp commonsense and factual knowledge to answer questions about world events. Many prior methods couple language modeling with knowledge graphs ({KG}). However, although a {KG} contains rich structural information, it lacks the context to provide a more precise understanding of the concepts. This creates a gap when fusing knowledge graphs into language modeling, especially when there is insufficient labeled data. Thus, we propose to employ external entity descriptions to provide contextual information for knowledge understanding. We retrieve descriptions of related concepts from Wiktionary and feed them as additional input to pre-trained language models. The resulting model achieves state-of-the-art result in the {CommonsenseQA} dataset and the best result among non-generative models in {OpenBookQA}.},
	number = {{arXiv}:2012.04808},
	publisher = {{arXiv}},
	author = {Xu, Yichong and Zhu, Chenguang and Xu, Ruochen and Liu, Yang and Zeng, Michael and Huang, Xuedong},
	urldate = {2025-04-13},
	date = {2021-08-02},
	eprinttype = {arxiv},
	eprint = {2012.04808 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\TSJI5GL2\\Xu et al. - 2021 - Fusing Context Into Knowledge Graph for Commonsense Question Answering.pdf:application/pdf},
}

@misc{zhang_greaselm_2022,
	title = {{GreaseLM}: Graph {REASoning} Enhanced Language Models for Question Answering},
	url = {http://arxiv.org/abs/2201.08860},
	doi = {10.48550/arXiv.2201.08860},
	shorttitle = {{GreaseLM}},
	abstract = {Answering complex questions about textual narratives requires reasoning over both stated context and the world knowledge that underlies it. However, pretrained language models ({LM}), the foundation of most modern {QA} systems, do not robustly represent latent relationships between concepts, which is necessary for reasoning. While knowledge graphs ({KG}) are often used to augment {LMs} with structured representations of world knowledge, it remains an open question how to effectively fuse and reason over the {KG} representations and the language context, which provides situational constraints and nuances. In this work, we propose {GreaseLM}, a new model that fuses encoded representations from pretrained {LMs} and graph neural networks over multiple layers of modality interaction operations. Information from both modalities propagates to the other, allowing language context representations to be grounded by structured world knowledge, and allowing linguistic nuances (e.g., negation, hedging) in the context to inform the graph representations of knowledge. Our results on three benchmarks in the commonsense reasoning (i.e., {CommonsenseQA}, {OpenbookQA}) and medical question answering (i.e., {MedQA}-{USMLE}) domains demonstrate that {GreaseLM} can more reliably answer questions that require reasoning over both situational constraints and structured knowledge, even outperforming models 8x larger.},
	number = {{arXiv}:2201.08860},
	publisher = {{arXiv}},
	author = {Zhang, Xikun and Bosselut, Antoine and Yasunaga, Michihiro and Ren, Hongyu and Liang, Percy and Manning, Christopher D. and Leskovec, Jure},
	urldate = {2025-04-13},
	date = {2022-01-21},
	eprinttype = {arxiv},
	eprint = {2201.08860 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\SESIHUYC\\Zhang et al. - 2022 - GreaseLM Graph REASoning Enhanced Language Models for Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\2LPVZXK4\\2201.html:text/html},
}

@article{cao_relmkg_2023,
	title = {{ReLMKG}: reasoning with pre-trained language models and knowledge graphs for complex question answering},
	volume = {53},
	issn = {1573-7497},
	url = {https://doi.org/10.1007/s10489-022-04123-w},
	doi = {10.1007/s10489-022-04123-w},
	shorttitle = {{ReLMKG}},
	abstract = {The goal of complex question answering over knowledge bases ({KBQA}) is to find an answer entity in a knowledge graph. Recent information retrieval-based methods have focused on the topology of the knowledge graph, ignoring inconsistencies between knowledge graph embeddings and natural language embeddings, and cannot effectively utilize both implicit and explicit knowledge for reasoning. In this paper, we propose a novel model, {ReLMKG}, to address this challenge. This approach performs joint reasoning on a pre-trained language model and the associated knowledge graph. The complex question and textual paths are encoded by the language model, bridging the gap between the question and the knowledge graph and exploiting implicit knowledge without introducing additional unstructured text. The outputs of different layers in the language model are used as instructions to guide a graph neural network to perform message propagation and aggregation in a step-by-step manner, which utilizes the explicit knowledge contained in the structured knowledge graph. We analyse the reasoning ability of the {ReLMKG} model for knowledge graphs with different degrees of sparseness and evaluate the generalizability of the model. Experiments conducted on the Complex {WebQuestions} and {WebQuestionsSP} datasets demonstrate the effectiveness of our approach on {KBQA} tasks.},
	pages = {12032--12046},
	number = {10},
	journaltitle = {Applied Intelligence},
	shortjournal = {Appl Intell},
	author = {Cao, Xing and Liu, Yun},
	urldate = {2025-04-13},
	date = {2023-05-01},
	langid = {english},
	keywords = {Artificial Intelligence, Knowledge graph, Complex question answering, Joint reasoning, Pre-trained language model},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ZD5WHSP7\\Cao und Liu - 2023 - ReLMKG reasoning with pre-trained language models and knowledge graphs for complex question answeri.pdf:application/pdf},
}

@misc{wang_improving_2018,
	title = {Improving Natural Language Inference Using External Knowledge in the Science Questions Domain},
	url = {http://arxiv.org/abs/1809.05724},
	doi = {10.48550/arXiv.1809.05724},
	abstract = {Natural Language Inference ({NLI}) is fundamental to many Natural Language Processing ({NLP}) applications including semantic search and question answering. The {NLI} problem has gained significant attention thanks to the release of large scale, challenging datasets. Present approaches to the problem largely focus on learning-based methods that use only textual information in order to classify whether a given premise entails, contradicts, or is neutral with respect to a given hypothesis. Surprisingly, the use of methods based on structured knowledge -- a central topic in artificial intelligence -- has not received much attention vis-a-vis the {NLI} problem. While there are many open knowledge bases that contain various types of reasoning information, their use for {NLI} has not been well explored. To address this, we present a combination of techniques that harness knowledge graphs to improve performance on the {NLI} problem in the science questions domain. We present the results of applying our techniques on text, graph, and text-to-graph based models, and discuss implications for the use of external knowledge in solving the {NLI} problem. Our model achieves the new state-of-the-art performance on the {NLI} problem over the {SciTail} science questions dataset.},
	number = {{arXiv}:1809.05724},
	publisher = {{arXiv}},
	author = {Wang, Xiaoyan and Kapanipathi, Pavan and Musa, Ryan and Yu, Mo and Talamadupula, Kartik and Abdelaziz, Ibrahim and Chang, Maria and Fokoue, Achille and Makni, Bassem and Mattei, Nicholas and Witbrock, Michael},
	urldate = {2025-04-13},
	date = {2018-11-20},
	eprinttype = {arxiv},
	eprint = {1809.05724 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\VN9NCYVB\\Wang et al. - 2018 - Improving Natural Language Inference Using External Knowledge in the Science Questions Domain.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\LMUNWG72\\1809.html:text/html},
}

@misc{lin_kagnet_2019,
	title = {{KagNet}: Knowledge-Aware Graph Networks for Commonsense Reasoning},
	url = {http://arxiv.org/abs/1909.02151},
	doi = {10.48550/arXiv.1909.02151},
	shorttitle = {{KagNet}},
	abstract = {Commonsense reasoning aims to empower machines with the human ability to make presumptions about ordinary situations in our daily life. In this paper, we propose a textual inference framework for answering commonsense questions, which effectively utilizes external, structured commonsense knowledge graphs to perform explainable inferences. The framework first grounds a question-answer pair from the semantic space to the knowledge-based symbolic space as a schema graph, a related sub-graph of external knowledge graphs. It represents schema graphs with a novel knowledge-aware graph network module named {KagNet}, and finally scores answers with graph representations. Our model is based on graph convolutional networks and {LSTMs}, with a hierarchical path-based attention mechanism. The intermediate attention scores make it transparent and interpretable, which thus produce trustworthy inferences. Using {ConceptNet} as the only external resource for Bert-based models, we achieved state-of-the-art performance on the {CommonsenseQA}, a large-scale dataset for commonsense reasoning.},
	number = {{arXiv}:1909.02151},
	publisher = {{arXiv}},
	author = {Lin, Bill Yuchen and Chen, Xinyue and Chen, Jamin and Ren, Xiang},
	urldate = {2025-04-13},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.02151 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XQK593H4\\Lin et al. - 2019 - KagNet Knowledge-Aware Graph Networks for Commonsense Reasoning.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\E5XK3LKQ\\1909.html:text/html},
}

@misc{wang_knowledge_2023-1,
	title = {Knowledge Graph Prompting for Multi-Document Question Answering},
	url = {http://arxiv.org/abs/2308.11730},
	doi = {10.48550/arXiv.2308.11730},
	abstract = {The `pre-train, prompt, predict' paradigm of large language models ({LLMs}) has achieved remarkable success in open-domain question answering ({OD}-{QA}). However, few works explore this paradigm in the scenario of multi-document question answering ({MD}-{QA}), a task demanding a thorough understanding of the logical associations among the contents and structures of different documents. To fill this crucial gap, we propose a Knowledge Graph Prompting ({KGP}) method to formulate the right context in prompting {LLMs} for {MD}-{QA}, which consists of a graph construction module and a graph traversal module. For graph construction, we create a knowledge graph ({KG}) over multiple documents with nodes symbolizing passages or document structures (e.g., pages/tables), and edges denoting the semantic/lexical similarity between passages or intra-document structural relations. For graph traversal, we design an {LLM}-based graph traversal agent that navigates across nodes and gathers supporting passages assisting {LLMs} in {MD}-{QA}. The constructed graph serves as the global ruler that regulates the transitional space among passages and reduces retrieval latency. Concurrently, the graph traversal agent acts as a local navigator that gathers pertinent context to progressively approach the question and guarantee retrieval quality. Extensive experiments underscore the efficacy of {KGP} for {MD}-{QA}, signifying the potential of leveraging graphs in enhancing the prompt design for {LLMs}. Our code: https://github.com/{YuWVandy}/{KG}-{LLM}-{MDQA}.},
	number = {{arXiv}:2308.11730},
	publisher = {{arXiv}},
	author = {Wang, Yu and Lipka, Nedim and Rossi, Ryan A. and Siu, Alexa and Zhang, Ruiyi and Derr, Tyler},
	urldate = {2025-04-13},
	date = {2023-12-25},
	eprinttype = {arxiv},
	eprint = {2308.11730 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\MV9ZP8C6\\Wang et al. - 2023 - Knowledge Graph Prompting for Multi-Document Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\M929EXCD\\2308.html:text/html},
}

@misc{sun_oda_2024,
	title = {{ODA}: Observation-Driven Agent for integrating {LLMs} and Knowledge Graphs},
	url = {http://arxiv.org/abs/2404.07677},
	doi = {10.48550/arXiv.2404.07677},
	shorttitle = {{ODA}},
	abstract = {The integration of Large Language Models ({LLMs}) and knowledge graphs ({KGs}) has achieved remarkable success in various natural language processing tasks. However, existing methodologies that integrate {LLMs} and {KGs} often navigate the task-solving process solely based on the {LLM}'s analysis of the question, overlooking the rich cognitive potential inherent in the vast knowledge encapsulated in {KGs}. To address this, we introduce Observation-Driven Agent ({ODA}), a novel {AI} agent framework tailored for tasks involving {KGs}. {ODA} incorporates {KG} reasoning abilities via global observation, which enhances reasoning capabilities through a cyclical paradigm of observation, action, and reflection. Confronting the exponential explosion of knowledge during observation, we innovatively design a recursive observation mechanism. Subsequently, we integrate the observed knowledge into the action and reflection modules. Through extensive experiments, {ODA} demonstrates state-of-the-art performance on several datasets, notably achieving accuracy improvements of 12.87\% and 8.9\%.},
	number = {{arXiv}:2404.07677},
	publisher = {{arXiv}},
	author = {Sun, Lei and Tao, Zhengwei and Li, Youdi and Arakawa, Hiroshi},
	urldate = {2025-04-14},
	date = {2024-06-04},
	eprinttype = {arxiv},
	eprint = {2404.07677 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\XF8673MZ\\Sun et al. - 2024 - ODA Observation-Driven Agent for integrating LLMs and Knowledge Graphs.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\RKJTNPPR\\2404.html:text/html},
}

@misc{cheng_multi-hop_2024,
	title = {Multi-hop Question Answering under Temporal Knowledge Editing},
	url = {http://arxiv.org/abs/2404.00492},
	doi = {10.48550/arXiv.2404.00492},
	abstract = {Multi-hop question answering ({MQA}) under knowledge editing ({KE}) has garnered significant attention in the era of large language models. However, existing models for {MQA} under {KE} exhibit poor performance when dealing with questions containing explicit temporal contexts. To address this limitation, we propose a novel framework, namely {TEMPoral} {knowLEdge} augmented Multi-hop Question Answering ({TEMPLE}-{MQA}). Unlike previous methods, {TEMPLE}-{MQA} first constructs a time-aware graph ({TAG}) to store edit knowledge in a structured manner. Then, through our proposed inference path, structural retrieval, and joint reasoning stages, {TEMPLE}-{MQA} effectively discerns temporal contexts within the question query. Experiments on benchmark datasets demonstrate that {TEMPLE}-{MQA} significantly outperforms baseline models. Additionally, we contribute a new dataset, namely {TKEMQA}, which serves as the inaugural benchmark tailored specifically for {MQA} with temporal scopes.},
	number = {{arXiv}:2404.00492},
	publisher = {{arXiv}},
	author = {Cheng, Keyuan and Lin, Gang and Fei, Haoyang and zhai, Yuxuan and Yu, Lu and Ali, Muhammad Asif and Hu, Lijie and Wang, Di},
	urldate = {2025-04-14},
	date = {2024-03-30},
	eprinttype = {arxiv},
	eprint = {2404.00492 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\JGYF9PLB\\Cheng et al. - 2024 - Multi-hop Question Answering under Temporal Knowledge Editing.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8ZISP5IM\\2404.html:text/html},
}

@misc{choudhary_complex_2024-1,
	title = {Complex Logical Reasoning over Knowledge Graphs using Large Language Models},
	url = {http://arxiv.org/abs/2305.01157},
	doi = {10.48550/arXiv.2305.01157},
	abstract = {Reasoning over knowledge graphs ({KGs}) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs ({LARK}), that formulates complex {KG} reasoning as a combination of contextual {KG} search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models ({LLM}), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art {KG} reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying {LLM}, enabling the integration of the latest advancements in {LLMs} for logical reasoning over {KGs}. Our work presents a new direction for addressing the challenges of complex {KG} reasoning and paves the way for future research in this area.},
	number = {{arXiv}:2305.01157},
	publisher = {{arXiv}},
	author = {Choudhary, Nurendra and Reddy, Chandan K.},
	urldate = {2025-04-14},
	date = {2024-03-31},
	eprinttype = {arxiv},
	eprint = {2305.01157 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Logic in Computer Science},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\LNTGSQZG\\Choudhary und Reddy - 2024 - Complex Logical Reasoning over Knowledge Graphs using Large Language Models.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\L32HGY2Y\\2305.html:text/html},
}

@misc{zhao_graphtext_2023,
	title = {{GraphText}: Graph Reasoning in Text Space},
	url = {http://arxiv.org/abs/2310.01089},
	doi = {10.48550/arXiv.2310.01089},
	shorttitle = {{GraphText}},
	abstract = {Large Language Models ({LLMs}) have gained the ability to assimilate human knowledge and facilitate natural language interactions with both humans and other {LLMs}. However, despite their impressive achievements, {LLMs} have not made significant advancements in the realm of graph machine learning. This limitation arises because graphs encapsulate distinct relational data, making it challenging to transform them into natural language that {LLMs} understand. In this paper, we bridge this gap with a novel framework, {GraphText}, that translates graphs into natural language. {GraphText} derives a graph-syntax tree for each graph that encapsulates both the node attributes and inter-node relationships. Traversal of the tree yields a graph text sequence, which is then processed by an {LLM} to treat graph tasks as text generation tasks. Notably, {GraphText} offers multiple advantages. It introduces training-free graph reasoning: even without training on graph data, {GraphText} with {ChatGPT} can achieve on par with, or even surpassing, the performance of supervised-trained graph neural networks through in-context learning ({ICL}). Furthermore, {GraphText} paves the way for interactive graph reasoning, allowing both humans and {LLMs} to communicate with the model seamlessly using natural language. These capabilities underscore the vast, yet-to-be-explored potential of {LLMs} in the domain of graph machine learning.},
	number = {{arXiv}:2310.01089},
	publisher = {{arXiv}},
	author = {Zhao, Jianan and Zhuo, Le and Shen, Yikang and Qu, Meng and Liu, Kai and Bronstein, Michael and Zhu, Zhaocheng and Tang, Jian},
	urldate = {2025-04-14},
	date = {2023-10-02},
	eprinttype = {arxiv},
	eprint = {2310.01089 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\UVYPTJQW\\Zhao et al. - 2023 - GraphText Graph Reasoning in Text Space.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\AL82J64G\\2310.html:text/html},
}

@misc{xu_generate--graph_2024,
	title = {Generate-on-Graph: Treat {LLM} as both Agent and {KG} in Incomplete Knowledge Graph Question Answering},
	doi = {10.48550/arXiv.2404.14741},
	shorttitle = {Generate-on-Graph},
	abstract = {To address the issues of insufficient knowledge and hallucination in Large Language Models ({LLMs}), numerous studies have explored integrating {LLMs} with Knowledge Graphs ({KGs}). However, these methods are typically evaluated on conventional Knowledge Graph Question Answering ({KGQA}) with complete {KGs}, where all factual triples required for each question are entirely covered by the given {KG}. In such cases, {LLMs} primarily act as an agent to find answer entities within the {KG}, rather than effectively integrating the internal knowledge of {LLMs} and external knowledge sources such as {KGs}. In fact, {KGs} are often incomplete to cover all the knowledge required to answer questions. To simulate these real-world scenarios and evaluate the ability of {LLMs} to integrate internal and external knowledge, we propose leveraging {LLMs} for {QA} under Incomplete Knowledge Graph ({IKGQA}), where the provided {KG} lacks some of the factual triples for each question, and construct corresponding datasets. To handle {IKGQA}, we propose a training-free method called Generate-on-Graph ({GoG}), which can generate new factual triples while exploring {KGs}. Specifically, {GoG} performs reasoning through a Thinking-Searching-Generating framework, which treats {LLM} as both Agent and {KG} in {IKGQA}. Experimental results on two datasets demonstrate that our {GoG} outperforms all previous methods.},
	number = {{arXiv}:2404.14741},
	publisher = {{arXiv}},
	author = {Xu, Yao and He, Shizhu and Chen, Jiabei and Wang, Zihao and Song, Yangqiu and Tong, Hanghang and Liu, Guang and Liu, Kang and Zhao, Jun},
	date = {2024-10-06},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\TSWM7CSC\\Xu et al. - 2024 - Generate-on-Graph Treat LLM as both Agent and KG in Incomplete Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\FF6ZUUQP\\2404.html:text/html},
}

@misc{chen_self-improvement_2024,
	title = {Self-Improvement Programming for Temporal Knowledge Graph Question Answering},
	url = {http://arxiv.org/abs/2404.01720},
	doi = {10.48550/arXiv.2404.01720},
	abstract = {Temporal Knowledge Graph Question Answering ({TKGQA}) aims to answer questions with temporal intent over Temporal Knowledge Graphs ({TKGs}). The core challenge of this task lies in understanding the complex semantic information regarding multiple types of time constraints (e.g., before, first) in questions. Existing end-to-end methods implicitly model the time constraints by learning time-aware embeddings of questions and candidate answers, which is far from understanding the question comprehensively. Motivated by semantic-parsing-based approaches that explicitly model constraints in questions by generating logical forms with symbolic operators, we design fundamental temporal operators for time constraints and introduce a novel self-improvement Programming method for {TKGQA} (Prog-{TQA}). Specifically, Prog-{TQA} leverages the in-context learning ability of Large Language Models ({LLMs}) to understand the combinatory time constraints in the questions and generate corresponding program drafts with a few examples given. Then, it aligns these drafts to {TKGs} with the linking module and subsequently executes them to generate the answers. To enhance the ability to understand questions, Prog-{TQA} is further equipped with a self-improvement strategy to effectively bootstrap {LLMs} using high-quality self-generated drafts. Extensive experiments demonstrate the superiority of the proposed Prog-{TQA} on {MultiTQ} and {CronQuestions} datasets, especially in the Hits@1 metric.},
	number = {{arXiv}:2404.01720},
	publisher = {{arXiv}},
	author = {Chen, Zhuo and Zhang, Zhao and Li, Zixuan and Wang, Fei and Zeng, Yutao and Jin, Xiaolong and Xu, Yongjun},
	urldate = {2025-04-14},
	date = {2024-04-02},
	eprinttype = {arxiv},
	eprint = {2404.01720 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\8L984JJL\\Chen et al. - 2024 - Self-Improvement Programming for Temporal Knowledge Graph Question Answering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\8J3JAUWZ\\2404.html:text/html},
}

@inproceedings{jiang_reasoning_2024,
	title = {Reasoning on Efficient Knowledge Paths: Knowledge Graph Guides Large Language Model for Domain Question Answering},
	url = {https://ieeexplore.ieee.org/abstract/document/10884449},
	doi = {10.1109/ICKG63256.2024.00026},
	shorttitle = {Reasoning on Efficient Knowledge Paths},
	abstract = {Large language models ({LLMs}), such as {GPT}3.5, {GPT}4 and {LLAMA}2 perform surprisingly well and outperform human experts on many tasks. However, in many domain-specific evaluations, these {LLMs} often suffer from hallucination problems due to insufficient training of relevant corpus. Furthermore, fine-tuning large models may face problems such as the {LLMs} are not open source or the construction of high-quality domain instruction is difficult. Therefore, structured knowledge databases such as knowledge graph can better provide domain background knowledge for {LLMs} and make full use of the reasoning and analysis capabilities of {LLMs}. In some previous works, {LLM} was called multiple times to determine whether the current triplet was suitable for inclusion in the subgraph when retrieving subgraphs through a question. Especially for the question that require a multi-hop reasoning path, frequent calls to {LLM} will consume a lot of computing power. Moreover, when choosing the reasoning path, {LLM} will be called once for each step, and if one of the steps is selected incorrectly, it will lead to the accumulation of errors in the following steps. In this paper, we proposed an method of reasoning on efficient knowledge paths ({RoK}). This method can integrated and optimized the pipeline for selecting reasoning paths from {KG} based on {LLM}, which can reduce the dependency on {LLM}. In addition, we propose a simple and effective subgraph retrieval method based on chain of thought ({CoT}) and page rank which can returns the paths most likely to contain the answer. We conduct experiments on three datasets: {GenMedGPT}-5k [14], {WebQuestions} [2], and {CMCQA} [21]. Finally, {RoK} can demonstrate that using fewer {LLM} calls can achieve the same results as previous {SOTAs} models.},
	eventtitle = {2024 {IEEE} International Conference on Knowledge Graph ({ICKG})},
	pages = {142--149},
	booktitle = {2024 {IEEE} International Conference on Knowledge Graph ({ICKG})},
	author = {Jiang, Boran and Wang, Yuqi and Luo, Yi and He, Dawei and Cheng, Peng and Gao, Liangcai},
	urldate = {2025-04-14},
	date = {2024-12},
	keywords = {Knowledge based systems, Pipelines, Knowledge graphs, Large language models, Training, Cognition, Knowledge graph, Question answering (information retrieval), Databases, Faces, Reasoning path},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\EP9J787C\\Jiang et al. - 2024 - Reasoning on Efficient Knowledge Paths Knowledge Graph Guides Large Language Model for Domain Quest.pdf:application/pdf},
}

@inproceedings{dzendzik_english_2021,
	location = {Online and Punta Cana, Dominican Republic},
	title = {English Machine Reading Comprehension Datasets: A Survey},
	url = {https://aclanthology.org/2021.emnlp-main.693/},
	doi = {10.18653/v1/2021.emnlp-main.693},
	shorttitle = {English Machine Reading Comprehension Datasets},
	abstract = {This paper surveys 60 English Machine Reading Comprehension datasets, with a view to providing a convenient resource for other researchers interested in this problem. We categorize the datasets according to their question and answer form and compare them across various dimensions including size, vocabulary, data source, method of creation, human performance level, and first question word. Our analysis reveals that Wikipedia is by far the most common data source and that there is a relative lack of why, when, and where questions across datasets.},
	eventtitle = {{EMNLP} 2021},
	pages = {8784--8804},
	booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	author = {Dzendzik, Daria and Foster, Jennifer and Vogel, Carl},
	editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
	urldate = {2025-04-16},
	date = {2021-11},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\H6YBHJSF\\Dzendzik et al. - 2021 - English Machine Reading Comprehension Datasets A Survey.pdf:application/pdf},
}

@misc{sui_can_2025,
	title = {Can Knowledge Graphs Make Large Language Models More Trustworthy? An Empirical Study Over Open-ended Question Answering},
	url = {http://arxiv.org/abs/2410.08085},
	doi = {10.48550/arXiv.2410.08085},
	shorttitle = {Can Knowledge Graphs Make Large Language Models More Trustworthy?},
	abstract = {Recent works integrating Knowledge Graphs ({KGs}) have led to promising improvements in enhancing the reasoning accuracy of Large Language Models ({LLMs}). However, current benchmarks focus mainly on closed-ended tasks, leaving a gap in the assessment of more complex real-world scenarios. This gap has also obscured the evaluation of {KGs}' potential to mitigate the problem of hallucination in {LLMs}. To fill the gap, we introduce {OKGQA}, a new benchmark specifically designed to assess {LLMs} enhanced with {KGs} under open-ended, real-world question answering scenarios. {OKGQA} is designed to closely reflect the complexities of practical applications using questions from different types, and incorporates specific metrics to measure both hallucination ratio and the enhancement in reasoning capabilities. To consider the scenario in which {KGs} may have varying levels of mistakes, we propose another benchmark variant {OKGQA}-P to assess model performance when the semantics and structure of {KGs} are deliberately perturbed and contaminated. {OKGQA} aims to (1) explore whether {KGs} can make {LLMs} more trustworthy in an open-ended setting, and (2) conduct a comparative analysis to shed light on method design. We believe that this study can facilitate a more complete performance comparison and encourage continuous improvement in integrating {KGs} with {LLMs} to reduce hallucination.},
	number = {{arXiv}:2410.08085},
	publisher = {{arXiv}},
	author = {Sui, Yuan and He, Yufei and Ding, Zifeng and Hooi, Bryan},
	urldate = {2025-04-16},
	date = {2025-02-19},
	eprinttype = {arxiv},
	eprint = {2410.08085 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\ERHPVFW8\\Sui et al. - 2025 - Can Knowledge Graphs Make Large Language Models More Trustworthy An Empirical Study Over Open-ended.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\E8WNK9KU\\2410.html:text/html},
}

@inproceedings{saikh_scholarlyread_2020-1,
	location = {Marseille, France},
	title = {{ScholarlyRead}: A New Dataset for Scientific Article Reading Comprehension},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.675/},
	shorttitle = {{ScholarlyRead}},
	abstract = {We present {ScholarlyRead}, span-of-word-based scholarly articles' Reading Comprehension ({RC}) dataset with approximately 10K manually checked passage-question-answer instances. {ScholarlyRead} was constructed in semi-automatic way. We consider the articles from two popular journals of a reputed publishing house. Firstly, we generate questions from these articles in an automatic way. Generated questions are then manually checked by the human annotators. We propose a baseline model based on Bi-Directional Attention Flow ({BiDAF}) network that yields the F1 score of 37.31\%. The framework would be useful for building Question-Answering ({QA}) systems on scientific articles.},
	eventtitle = {{LREC} 2020},
	pages = {5498--5504},
	booktitle = {Proceedings of the Twelfth Language Resources and Evaluation Conference},
	publisher = {European Language Resources Association},
	author = {Saikh, Tanik and Ekbal, Asif and Bhattacharyya, Pushpak},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	urldate = {2025-04-16},
	date = {2020-05},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\2EQG3GM4\\Saikh et al. - 2020 - ScholarlyRead A New Dataset for Scientific Article Reading Comprehension.pdf:application/pdf},
}

@inproceedings{cai_large-scale_2013,
	location = {Sofia, Bulgaria},
	title = {Large-scale Semantic Parsing via Schema Matching and Lexicon Extension},
	url = {https://aclanthology.org/P13-1042/},
	eventtitle = {{ACL} 2013},
	pages = {423--433},
	booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cai, Qingqing and Yates, Alexander},
	editor = {Schuetze, Hinrich and Fung, Pascale and Poesio, Massimo},
	urldate = {2025-04-17},
	date = {2013-08},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GBMECEGA\\Cai und Yates - 2013 - Large-scale Semantic Parsing via Schema Matching and Lexicon Extension.pdf:application/pdf},
}

@misc{talmor_web_2018,
	title = {The Web as a Knowledge-base for Answering Complex Questions},
	doi = {10.48550/arXiv.1803.06643},
	abstract = {Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, {ComplexWebQuestions}, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.},
	number = {{arXiv}:1803.06643},
	publisher = {{arXiv}},
	author = {Talmor, Alon and Berant, Jonathan},
	date = {2018-03-18},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\PSZMQ8UB\\Talmor und Berant - 2018 - The Web as a Knowledge-base for Answering Complex Questions.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\ZHH4Z49F\\1803.html:text/html},
}

@inproceedings{kaplan_responsible_2025,
	title = {Responsible and Sustainable {AI}: Considering Energy Consumption in Automated Text Classification Evaluation Tasks},
	isbn = {979-8-3315-0569-1},
	url = {https://publikationen.bibliothek.kit.edu/1000180194},
	shorttitle = {Responsible and Sustainable {AI}},
	abstract = {Text classification is one of the typical and fundamental natural language processing tasks. With the advent of large language models ({LLMs}), text classification has evolved much further. Based on the growing sizes of {LLMs} and the increased demands for hardware, and especially energy, questions about sustainability and environmental impacts and responsibility also arise. To assess text classification approaches, researchers usually only use common performance metrics like precision, recall, and f1-score. Green {AI}, i.e., improving environmental aspects while maintaining performance, is regularly disregarded and not a standard in the evaluation of automated text classification approaches. Yet, minor performance improvements might not justify, e.g., much higher energy consumption. In this paper, we aim to raise awareness for this issue and the corresponding trade-off discussions and decisions. Therefore, we present novel sustainability metrics and provide guidelines for text classification approaches that are suitable for Green {AI}. In a text classification use case, we showcase the applicability of our proposed metrics and discuss corresponding trade-off decisions.},
	eventtitle = {International Conference on Software Engineering ({ICSE} 2025), Ottawa, Kanada, 27.04.2025 – 03.05.2025},
	booktitle = {Proceedings of the 2025 \{{IEEE}/{ACM}\} 47th International Conference on Software Engineering: Companion Proceedings, \{{ICSE}\} Companion 2025, Ottawa, 27th April - 3rd May 2025},
	author = {Kaplan, Angelika and Keim, Jan and Greiner, Lukas and Sieger, Ralf and Mirandola, Raffaela and Reussner, Ralf},
	urldate = {2025-04-29},
	date = {2025},
	langid = {german},
	note = {{ISSN}: 1558-1225},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\XE7VDJI9\\Kaplan et al. - 2025 - Responsible and Sustainable AI Considering Energy Consumption in Automated Text Classification Eval.pdf:application/pdf},
}

@article{nickerson_method_2013,
	title = {A method for taxonomy development and its application in information systems},
	volume = {22},
	rights = {http://www.springer.com/tdm},
	issn = {0960-085X, 1476-9344},
	doi = {10.1057/ejis.2012.26},
	abstract = {A fundamental problem in many disciplines is the classification of objects in a domain of interest into a taxonomy. Developing a taxonomy, however, is a complex process that has not been adequately addressed in the information systems ({IS}) literature. The purpose of this paper is to present a method for taxonomy development that can be used in {IS}. First, this paper demonstrates through a comprehensive literature survey that taxonomy development in {IS} has largely been ad hoc. Then the paper defines the problem of taxonomy development. Next, the paper presents a method for taxonomy development that is based on taxonomy development literature in other disciplines and shows that the method has certain desirable qualities. Finally, the paper demonstrates the efficacy of the method by developing a taxonomy in a domain in {IS}.},
	pages = {336--359},
	number = {3},
	journaltitle = {European Journal of Information Systems},
	shortjournal = {European Journal of Information Systems},
	author = {Nickerson, Robert C and Varshney, Upkar and Muntermann, Jan},
	date = {2013-05},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\GQW3HXCL\\Nickerson et al. - 2013 - A method for taxonomy development and its application in information systems.pdf:application/pdf},
}

@article{kundisch_update_2022,
	title = {An Update for Taxonomy Designers},
	volume = {64},
	issn = {1867-0202},
	doi = {10.1007/s12599-021-00723-x},
	abstract = {Taxonomies are classification systems that help researchers conceptualize phenomena based on their dimensions and characteristics. To address the problem of ‘ad-hoc’ taxonomy building, Nickerson et al. (2013) proposed a rigorous taxonomy development method for information systems researchers. Eight years on, however, the status quo of taxonomy research shows that the application of this method lacks consistency and transparency and that further guidance on taxonomy evaluation is needed. To fill these gaps, this study (1) advances existing methodological guidance and (2) extends this guidance with regards to taxonomy evaluation. Informed by insights gained from an analysis of 164 taxonomy articles published in information systems outlets, this study presents an extended taxonomy design process together with 26 operational taxonomy design recommendations. Representing an update for taxonomy designers, it contributes to the prescriptive knowledge on taxonomy design and seeks to augment both rigorous taxonomy building and evaluation.},
	pages = {421--439},
	number = {4},
	journaltitle = {Business \& Information Systems Engineering},
	shortjournal = {Bus Inf Syst Eng},
	author = {Kundisch, Dennis and Muntermann, Jan and Oberländer, Anna Maria and Rau, Daniel and Röglinger, Maximilian and Schoormann, Thorsten and Szopinski, Daniel},
	date = {2022-08-01},
	langid = {english},
	keywords = {Design science research, Research methodology, Taxonomy design, Taxonomy development, Taxonomy evaluation},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\57K7NKN4\\Kundisch et al. - 2022 - An Update for Taxonomy Designers.pdf:application/pdf},
}

@article{bayona-ore_critical_2014,
	title = {Critical success factors taxonomy for software process deployment},
	volume = {22},
	issn = {1573-1367},
	doi = {10.1007/s11219-012-9190-y},
	abstract = {Many organizations have adopted methods, models, and standards to improve their software processes. However, despite these efforts, they can still find it difficult to deploy processes throughout the organization because most of them focus more on the technical rather than human aspects. This paper proposes a taxonomy of critical success factors for software process deployment. A method to create this taxonomy was developed and applied based on a systematic review of existing literature and is complemented with industry experiences where software processes have been deployed or implemented. Finally, the categories, subcategories, and items of this taxonomy are presented.},
	pages = {21--48},
	number = {1},
	journaltitle = {Software Quality Journal},
	shortjournal = {Software Qual J},
	author = {Bayona-Oré, Sussy and Calvo-Manzano, Jose A. and Cuevas, Gonzalo and San-Feliu, Tomas},
	date = {2014-03-01},
	langid = {english},
	keywords = {{CMMI}, Critical success factors, Process deployment, Taxonomy},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\5ER2BCFS\\Bayona-Oré et al. - 2014 - Critical success factors taxonomy for software process deployment.pdf:application/pdf},
}

@article{jin_floating-point_2024,
	title = {Floating-Point Embedding: Enhancing the Mathematical Comprehension of Large Language Models},
	volume = {16},
	rights = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	doi = {10.3390/sym16040478},
	shorttitle = {Floating-Point Embedding},
	abstract = {The processing and comprehension of numerical information in natural language represent pivotal focal points of scholarly inquiry. Across diverse applications spanning text analysis to information retrieval, the adept management and understanding of the numerical content within natural language are indispensable in achieving task success. Specialized encoding and embedding techniques tailored to numerical data offer an avenue toward improved performance in tasks involving masked prediction and numerical reasoning, inherently characterized by numerical values. Consequently, treating numbers in text merely as words is inadequate; their numerical semantics must be underscored. Recent years have witnessed the emergence of a range of specific encoding methodologies designed explicitly for numerical content, demonstrating promising outcomes. We observe similarities between the Transformer architecture and {CPU} architecture, with symmetry playing a crucial role. In light of this observation and drawing inspiration from computer system theory, we introduce a floating-point representation and devise a corresponding embedding module. The numerical representations correspond one-to-one with their semantic vector values, rendering both symmetric regarding intermediate transformation methods. Our proposed methodology facilitates the more comprehensive encoding and embedding of numerical information within a predefined precision range, thereby ensuring a distinctive encoding representation for each numerical entity. Rigorous testing on multiple encoder-only models and datasets yielded results that stand out in terms of competitiveness. In comparison to the default embedding methods employed by models, our approach achieved an improvement of approximately 3.8\% in Top-1 accuracy and a reduction in perplexity of approximately 0.43. These outcomes affirm the efficacy of our proposed method. Furthermore, the enrichment of numerical semantics through a more comprehensive embedding contributes to the augmentation of the model’s capacity for semantic understanding.},
	pages = {478},
	number = {4},
	journaltitle = {Symmetry},
	author = {Jin, Xiaoxiao and Mao, Chenyang and Yue, Dengfeng and Leng, Tuo},
	date = {2024-04},
	langid = {english},
	keywords = {floating-point embedding, large language model, numerical semantics},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\LGD4GHWR\\Jin et al. - 2024 - Floating-Point Embedding Enhancing the Mathematical Comprehension of Large Language Models.pdf:application/pdf},
}

@misc{brown_language_2020,
	title = {Language Models are Few-Shot Learners},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many {NLP} tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current {NLP} systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train {GPT}-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, {GPT}-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. {GPT}-3 achieves strong performance on many {NLP} datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where {GPT}-3's few-shot learning still struggles, as well as some datasets where {GPT}-3 faces methodological issues related to training on large web corpora. Finally, we find that {GPT}-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of {GPT}-3 in general.},
	number = {{arXiv}:2005.14165},
	publisher = {{arXiv}},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and {McCandlish}, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	date = {2020-07-22},
	eprinttype = {arxiv},
	eprint = {2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\EVZG4EWT\\Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\WYJQHF5Z\\2005.html:text/html},
}

@article{sabou_survey_2017,
	title = {Survey on challenges of Question Answering in the Semantic Web},
	volume = {8},
	issn = {1570-0844},
	url = {https://doi.org/10.3233/SW-160247},
	doi = {10.3233/SW-160247},
	abstract = {Semantic Question Answering ({SQA}) removes two major access requirements to the Semantic Web: the mastery of a formal query language like {SPARQL} and knowledge of a specific vocabulary. Because of the complexity of natural language, {SQA} presents difficult challenges and many research opportunities. Instead of a shared effort, however, many essential components are redeveloped, which is an inefficient use of researcher’s time and resources. This survey analyzes 62\&nbsp;different {SQA} systems, which are systematically and manually selected using predefined inclusion and exclusion criteria, leading to 72\&nbsp;selected publications out of 1960 candidates. We identify common challenges, structure solutions, and provide recommendations for future systems. This work is based on publications from the end of 2010 to July 2015 and is also compared to older but similar surveys.},
	pages = {895--920},
	number = {6},
	journaltitle = {Semant. web},
	author = {Sabou, Marta and Höffner, Konrad and Walter, Sebastian and Marx, Edgard and Usbeck, Ricardo and Lehmann, Jens and Ngonga Ngomo, Axel-Cyrille},
	urldate = {2025-05-02},
	date = {2017-01-01},
	file = {Volltext:C\:\\Users\\Marco\\Zotero\\storage\\KMLUKDQY\\Sabou et al. - 2017 - Survey on challenges of Question Answering in the Semantic Web.pdf:application/pdf},
}

@article{hirschman_natural_2001,
	title = {Natural language question answering: the view from here},
	volume = {7},
	issn = {1351-3249},
	url = {https://doi.org/10.1017/S1351324901002807},
	doi = {10.1017/S1351324901002807},
	shorttitle = {Natural language question answering},
	abstract = {As users struggle to navigate the wealth of on-line information now available, the need for automated question answering systems becomes more urgent. We need systems that allow a user to ask a question in everyday language and receive an answer quickly and succinctly, with sufficient context to validate the answer. Current search engines can return ranked lists of documents, but they do not deliver answers to the user.Question answering systems address this problem. Recent successes have been reported in a series of question-answering evaluations that started in 1999 as part of the Text Retrieval Conference ({TREC}). The best systems are now able to answer more than two thirds of factual questions in this evaluation.},
	pages = {275--300},
	number = {4},
	journaltitle = {Nat. Lang. Eng.},
	author = {Hirschman, L. and Gaizauskas, R.},
	urldate = {2025-05-02},
	date = {2001-12-01},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\CPRGMFXX\\Hirschman und Gaizauskas - 2001 - Natural language question answering the view from here.pdf:application/pdf},
}

@article{diefenbach_core_2018-1,
	title = {Core techniques of question answering systems over knowledge bases: a survey},
	volume = {55},
	issn = {0219-1377, 0219-3116},
	url = {http://link.springer.com/10.1007/s10115-017-1100-y},
	doi = {10.1007/s10115-017-1100-y},
	shorttitle = {Core techniques of question answering systems over knowledge bases},
	abstract = {The Semantic Web contains an enormous amount of information in the form of knowledge bases ({KB}). To make this information available, many question answering ({QA}) systems over {KBs} were created in the last years. Building a {QA} system over {KBs} is difﬁcult because there are many different challenges to be solved. In order to address these challenges, {QA} systems generally combine techniques from natural language processing, information retrieval, machine learning and Semantic Web. The aim of this survey is to give an overview of the techniques used in current {QA} systems over {KBs}. We present the techniques used by the {QA} systems which were evaluated on a popular series of benchmarks: Question Answering over Linked Data. Techniques that solve the same task are ﬁrst grouped together and then described. The advantages and disadvantages are discussed for each technique. This allows a direct comparison of similar techniques. Additionally, we point to techniques that are used over {WebQuestions} and {SimpleQuestions}, which are two other popular benchmarks for {QA} systems.},
	pages = {529--569},
	number = {3},
	journaltitle = {Knowledge and Information Systems},
	shortjournal = {Knowl Inf Syst},
	author = {Diefenbach, Dennis and Lopez, Vanessa and Singh, Kamal and Maret, Pierre},
	urldate = {2025-05-02},
	date = {2018-06},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\JQFNRQ57\\Diefenbach et al. - 2018 - Core techniques of question answering systems over knowledge bases a survey.pdf:application/pdf},
}

@article{xiong_knowledge_2021,
	title = {Knowledge Graph Question Answering with semantic oriented fusion model},
	volume = {221},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705121002173},
	doi = {10.1016/j.knosys.2021.106954},
	abstract = {Knowledge Graph Question Answering ({KGQA}) is a major branch of question answering tasks, which can answer fact questions effectively by using the reasonable characteristics of the knowledge graph. Currently, lots of related works combined with a variety of deep learning models are presented for the {KGQA} task. However, there are still some challenges, such as topic entity recognition under ambiguity expression, semantic level representation of natural language, efficient construction of searching space for answers, etc. In this paper, we propose a comprehensive approach for complex question answering over {KG}. Firstly, during the stage of topic entity recognition, a deep transition model is constructed to extract topic entities, and an efficient entity linking strategy is presented, which combines character matching and entity disambiguation model. Secondly, for candidate path ranking, a dynamic candidate path generation algorithm is proposed to efficiently create the candidate answer set. And four dedicated similarity calculation models are designed to handle the intricate condition of complex questions with long sequence and diversity expression. Moreover, a fusion policy is proposed to make decision for the final correct answer. We evaluate our approach on {CKBQA}, a Chinese knowledge base question answering dataset, from {CCKS}2019 competition. Experimental results demonstrate that the improvements in each process are effective and our approach achieves better performance than the best team in {CCKS}2019 competition.},
	pages = {106954},
	journaltitle = {Knowledge-Based Systems},
	shortjournal = {Knowledge-Based Systems},
	author = {Xiong, Haobo and Wang, Shuting and Tang, Mingrong and Wang, Liping and Lin, Xuemin},
	urldate = {2025-05-02},
	date = {2021-06},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\SZT8235B\\Xiong et al. - 2021 - Knowledge Graph Question Answering with semantic oriented fusion model.pdf:application/pdf},
}

@article{yani_better_2022,
	title = {A better entity detection of question for knowledge graph question answering through extracting position-based patterns},
	volume = {9},
	issn = {2196-1115},
	doi = {10.1186/s40537-022-00631-1},
	abstract = {Entity detection task on knowledge graph question answering systems has been studied well on simple questions. However, the task is still challenging on complex questions. It is due to a complex question is composed of more than one fact or triple. This paper proposes a method to detect entities and their position on triples mentioned in a question. Unlike existing approaches that only focus on detecting the entity name, our method can determine in which triple an entity is located. Furthermore, our approach can also define if an entity is a head or a tail of a triple mentioned in a question. We tested our approach to {SimpleQuestions}, {LC}-{QuAD} 2.0, and {QALD} series benchmarks. The experiment result demonstrates that our model outperforms the previous works on {SimpleQuestions} and {QALD} series datasets. 99.15\% accuracy and 96.15\% accuracy on average, respectively. Our model can also improve entity detection performance on {LC}-{QuAD} 2.0 with a merged dataset, namely, 97.4\% accuracy. This paper also presents Wikidata {QALD} series version that is helpful for researchers to assess the knowledge graph question answering system they develop.},
	pages = {80},
	number = {1},
	journaltitle = {Journal of Big Data},
	shortjournal = {Journal of Big Data},
	author = {Yani, Mohammad and Krisnadhi, Adila Alfa and Budi, Indra},
	date = {2022-06-17},
	keywords = {Complex question, Entity detection, Entity recognition, Knowledge graph question answering, Question pattern},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\22K4EIP9\\Yani et al. - 2022 - A better entity detection of question for knowledge graph question answering through extracting posi.pdf:application/pdf},
}

@book{bailey_typologies_2003,
	location = {Thousand Oaks, Calif.},
	edition = {Nachdr.},
	title = {Typologies and taxonomies: an introduction to classification techniques},
	isbn = {978-0-8039-5259-1},
	series = {Sage university papers Quantitative applicatons in the social sciences},
	shorttitle = {Typologies and taxonomies},
	pagetotal = {90},
	number = {102},
	publisher = {Sage Publ},
	author = {Bailey, Kenneth D.},
	date = {2003},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\UNB28K55\\Bailey - 2003 - Typologies and taxonomies an introduction to classification techniques.pdf:application/pdf},
}

@article{bailey_three-level_1984,
	title = {A three-level measurement model},
	volume = {18},
	rights = {http://www.springer.com/tdm},
	issn = {0033-5177, 1573-7845},
	url = {http://link.springer.com/10.1007/BF00156457},
	doi = {10.1007/BF00156457},
	pages = {225--245},
	number = {3},
	journaltitle = {Quality and Quantity},
	shortjournal = {Qual Quant},
	author = {Bailey, Kenneth D.},
	urldate = {2025-05-02},
	date = {1984-05},
	langid = {english},
	file = {PDF:C\:\\Users\\Marco\\Zotero\\storage\\7Q3Z2MI4\\Bailey - 1984 - A three-level measurement model.pdf:application/pdf},
}

@misc{pan_rag_2025,
	title = {A {RAG} Approach for Generating Competency Questions in Ontology Engineering},
	doi = {10.48550/arXiv.2409.08820},
	abstract = {Competency question ({CQ}) formulation is central to several ontology development and evaluation methodologies. Traditionally, the task of crafting these competency questions heavily relies on the effort of domain experts and knowledge engineers which is often time-consuming and labor-intensive. With the emergence of Large Language Models ({LLMs}), there arises the possibility to automate and enhance this process. Unlike other similar works which use existing ontologies or knowledge graphs as input to {LLMs}, we present a retrieval-augmented generation ({RAG}) approach that uses {LLMs} for the automatic generation of {CQs} given a set of scientific papers considered to be a domain knowledge base. We investigate its performance and specifically, we study the impact of different number of papers to the {RAG} and different temperature setting of the {LLM}. We conduct experiments using {GPT}-4 on two domain ontology engineering tasks and compare results against ground-truth {CQs} constructed by domain experts. Empirical assessments on the results, utilizing evaluation metrics (precision and consistency), reveal that compared to zero-shot prompting, adding relevant domain knowledge to the {RAG} improves the performance of {LLMs} on generating {CQs} for concrete ontology engineering tasks.},
	number = {{arXiv}:2409.08820},
	publisher = {{arXiv}},
	author = {Pan, Xueli and Ossenbruggen, Jacco van and Boer, Victor de and Huang, Zhisheng},
	date = {2025-02-11},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\VVRG5BJH\\Pan et al. - 2025 - A RAG Approach for Generating Competency Questions in Ontology Engineering.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\CD8FVEY3\\2409.html:text/html},
}

@misc{hu_unveiling_2024,
	title = {Unveiling {LLM} Evaluation Focused on Metrics: Challenges and Solutions},
	doi = {10.48550/arXiv.2404.09135},
	shorttitle = {Unveiling {LLM} Evaluation Focused on Metrics},
	abstract = {Natural Language Processing ({NLP}) is witnessing a remarkable breakthrough driven by the success of Large Language Models ({LLMs}). {LLMs} have gained significant attention across academia and industry for their versatile applications in text generation, question answering, and text summarization. As the landscape of {NLP} evolves with an increasing number of domain-specific {LLMs} employing diverse techniques and trained on various corpus, evaluating performance of these models becomes paramount. To quantify the performance, it's crucial to have a comprehensive grasp of existing metrics. Among the evaluation, metrics which quantifying the performance of {LLMs} play a pivotal role. This paper offers a comprehensive exploration of {LLM} evaluation from a metrics perspective, providing insights into the selection and interpretation of metrics currently in use. Our main goal is to elucidate their mathematical formulations and statistical interpretations. We shed light on the application of these metrics using recent Biomedical {LLMs}. Additionally, we offer a succinct comparison of these metrics, aiding researchers in selecting appropriate metrics for diverse tasks. The overarching goal is to furnish researchers with a pragmatic guide for effective {LLM} evaluation and metric selection, thereby advancing the understanding and application of these large language models.},
	number = {{arXiv}:2404.09135},
	publisher = {{arXiv}},
	author = {Hu, Taojun and Zhou, Xiao-Hua},
	date = {2024-04-14},
	eprinttype = {arxiv},
	eprint = {2404.09135 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\K4GY84F5\\Hu und Zhou - 2024 - Unveiling LLM Evaluation Focused on Metrics Challenges and Solutions.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\BT29ZZHI\\2404.html:text/html},
}

@misc{mikolov_efficient_2013,
	title = {Efficient Estimation of Word Representations in Vector Space},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	number = {{arXiv}:1301.3781},
	publisher = {{arXiv}},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	date = {2013-09-07},
	eprinttype = {arxiv},
	eprint = {1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\SJQ5JV96\\Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\X83E8ZA3\\1301.html:text/html},
}

@inproceedings{pennington_glove_2014,
	location = {Doha, Qatar},
	title = {{GloVe}: Global Vectors for Word Representation},
	doi = {10.3115/v1/D14-1162},
	shorttitle = {{GloVe}},
	eventtitle = {{EMNLP} 2014},
	pages = {1532--1543},
	booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	editor = {Moschitti, Alessandro and Pang, Bo and Daelemans, Walter},
	date = {2014-10},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\GT7ZNNP4\\Pennington et al. - 2014 - GloVe Global Vectors for Word Representation.pdf:application/pdf},
}

@report{basili_software_1992,
	location = {{USA}},
	title = {Software modeling and measurement: the Goal/Question/Metric paradigm},
	shorttitle = {Software modeling and measurement},
	institution = {University of Maryland at College Park},
	type = {Technical Report},
	author = {Basili, Victor R.},
	date = {1992-08},
	note = {Num Pages: 24},
}

@inproceedings{zhao_butterfly_2020,
	title = {Butterfly Space: An Architectural Approach for Investigating Performance Issues},
	doi = {10.1109/ICSA47634.2020.00027},
	shorttitle = {Butterfly Space},
	abstract = {Performance issues widely exist in modern software systems. Existing performance optimization approaches, such as dynamic profiling, usually fail to consider the impacts of architectural connections among methods on performance issues. This paper contributes an architectural approach, Butterfly Space modeling, to investigate performance issues. Each Butterfly Space is composed of 1) a seed method; 2) methods in the "upper wing" that call the seed directly or transitively; and 3) methods in the "lower wing" that are called by the seed, directly or transitively. The rationale is that the performance of the seed method impacts and is impacted by all the other methods in the space because of the call relationship. As such, developers can more efficiently investigate groups of connected performance improvement opportunities in Butterfly Spaces. We studied three real-world open source Java projects to evaluate such potential. Our findings are three-fold: 1) If the seed method of a Butterfly Space contains performance problems, up to 60\% of the methods in the space also contain performance problems; 2) Butterfly Spaces can potentially help to non-trivially increase the precision/recall and reduce the costs in identifying performance improvement opportunities, compared to dynamic profiling; and 3) Visualizing dynamic profiling metrics with Butterfly Spaces simultaneously help to reveal two typical patterns, namely Expensive Callee and Inefficient Caller, that are responsible for performance problems and provide insights on where to improve next. We believe that Butterfly Space modeling has great potential for investigating performance issues.},
	eventtitle = {2020 {IEEE} International Conference on Software Architecture ({ICSA})},
	pages = {202--213},
	booktitle = {2020 {IEEE} International Conference on Software Architecture ({ICSA})},
	author = {Zhao, Yutong and Xiao, Lu and Wang, Xiao and Chen, Zhifei and Chen, Bihuan and Liu, Yang},
	date = {2020-03},
	keywords = {Computer bugs, Detection algorithms, Extraterrestrial measurements, Optimization, software performance, software architecture, performance optimization, Software systems},
}

@inproceedings{wohlrab_interfaces_2019,
	title = {On Interfaces to Support Agile Architecting in Automotive: An Exploratory Case Study},
	doi = {10.1109/ICSA.2019.00025},
	shorttitle = {On Interfaces to Support Agile Architecting in Automotive},
	abstract = {Practitioners struggle with creating and evolving an architecture when developing complex and safety-critical systems in large-scale agile contexts. A key issue is the trade-off between upfront planning and flexibility to embrace change. In particular, the coordination of interfaces is an important challenge, as interfaces determine and regulate the exchange of information between components, subsystems, and systems, which are often developed by multiple teams. In a fast-changing environment, boundary objects between teams can provide the sufficient stability to align software or systems, while maintaining a sufficient degree of autonomy. However, a better understanding of interfaces as boundary objects is needed to give practical guidance. This paper presents an exploratory case study with an automotive {OEM} to identify characteristics of different interfaces, from non-critical interfaces that can be changed frequently and quickly, to those that are critical and require more stability and a rigorous change process. We identify what dimensions impact how interfaces are changed, what categories of interfaces exist along these dimensions, and how categories of interfaces change over time. We conclude with suggestions for practices to manage the different categories of interfaces in large-scale agile development.},
	eventtitle = {2019 {IEEE} International Conference on Software Architecture ({ICSA})},
	pages = {161--170},
	booktitle = {2019 {IEEE} International Conference on Software Architecture ({ICSA})},
	author = {Wohlrab, Rebekka and Pelliccione, Patrizio and Knauss, Eric and Heldal, Rogardt},
	date = {2019-03},
	keywords = {agile architecture, architectural change, automotive, Automotive engineering, boundary objects, case study, Companies, Computer architecture, empirical software engineering, interfaces, Interviews, large-scale agile development, Safety, Software, Stability analysis},
}

@inproceedings{lu_dense_2024,
	title = {Dense Retrieval for Efficient Paper Retrieval in Academic Question Answering},
	url = {https://openreview.net/forum?id=iH203OQIU5},
	abstract = {The overarching goal of academic data mining is to deepen our comprehension of the development, nature, and trends of science. It offers the potential to unlock enormous scientific, technological, and educational value. To facilitate related research, Tsinghua University and Zhipu {AI} have presented the Open Academic Graph Challenge ({OAG}-Challenge) and published several realistic and challenging datasets. In this paper, we present our solution for the {KDD} Cup 2024 Academic Question Answering ({AQA}) task. Participants are required to retrieve the most relevant papers to answer given professional questions from a pool of candidate papers. To address this challenge, we constructed a bi-encoder model for academic paper retrieval. We conducted extensive experiments, exploring various language models ({LMs}) and ensembling them to boost performance. Additionally, we explored the incorporation of hard negative examples and a reranking model. Our team achieved high-quality results and demonstrated competitive performance in the competition, with mean average precision ({MAP}) scores of 0.20900 (top-6) and 0.18466 (top-7) on the validation and test sets, respectively. We have released our source code.},
	eventtitle = {{KDD} 2024 {OAG}-Challenge Cup},
	author = {Lu, Xuantao and Hu, Xingwu},
	urldate = {2025-05-27},
	date = {2024-07-21},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\4VUPQJMT\\Lu and Hu - 2024 - Dense Retrieval for Efficient Paper Retrieval in Academic Question Answering.pdf:application/pdf},
}

@inproceedings{jiang_structure_2023,
	title = {A Structure and Content Prompt-based Method for Knowledge Graph Question Answering over Scholarly Data},
	url = {https://openreview.net/forum?id=Mxc32gezHC},
	eventtitle = {{QALD}/{SemREC}@{ISWC}},
	author = {Jiang, Longquan and Yan, Xi and Usbeck, Ricardo},
	urldate = {2025-05-27},
	date = {2023-01-01},
	langid = {english},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\PPCZPBSH\\Jiang et al. - 2023 - A Structure and Content Prompt-based Method for Knowledge Graph Question Answering over Scholarly Da.pdf:application/pdf},
}

@misc{priem_openalex_2022,
	title = {{OpenAlex}: A fully-open index of scholarly works, authors, venues, institutions, and concepts},
	doi = {10.48550/arXiv.2205.01833},
	shorttitle = {{OpenAlex}},
	abstract = {{OpenAlex} is a new, fully-open scientific knowledge graph ({SKG}), launched to replace the discontinued Microsoft Academic Graph ({MAG}). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based {GUI}, a full data dump, and high-volume {REST} {API}. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.},
	number = {{arXiv}:2205.01833},
	publisher = {{arXiv}},
	author = {Priem, Jason and Piwowar, Heather and Orr, Richard},
	date = {2022-06-17},
	eprinttype = {arxiv},
	eprint = {2205.01833 [cs]},
	keywords = {Computer Science - Digital Libraries},
	file = {Preprint PDF:C\:\\Users\\Marco\\Zotero\\storage\\WZITIPTS\\Priem et al. - 2022 - OpenAlex A fully-open index of scholarly works, authors, venues, institutions, and concepts.pdf:application/pdf;Snapshot:C\:\\Users\\Marco\\Zotero\\storage\\JFQTGSMQ\\2205.html:text/html},
}

@inproceedings{lin_rouge_2004,
	location = {Barcelona, Spain},
	title = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
	url = {https://aclanthology.org/W04-1013/},
	shorttitle = {{ROUGE}},
	pages = {74--81},
	booktitle = {Text Summarization Branches Out},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Chin-Yew},
	urldate = {2025-05-27},
	date = {2004-07},
	file = {Full Text PDF:C\:\\Users\\Marco\\Zotero\\storage\\EAM7L6T3\\Lin - 2004 - ROUGE A Package for Automatic Evaluation of Summaries.pdf:application/pdf},
}

@inproceedings{diefenbach_question_2017,
	location = {Vienne, Austria},
	title = {Question Answering Benchmarks for Wikidata},
	url = {https://hal.science/hal-01637141},
	abstract = {Wikidata is becoming an increasingly important knowledge base whose usage is spreading in the research community. However, most question answering systems evaluation datasets rely on Freebase or {DBpedia}. We present two new datasets in order to train and benchmark {QA} systems over Wikidata. The first is a translation of the popular {SimpleQuestions} dataset to Wikidata, the second is a dataset created by collecting user feedbacks.},
	booktitle = {{ISWC} 2017},
	author = {Diefenbach, Dennis and Tanon, Thomas Pellissier and Singh, Kamal and Maret, Pierre},
	urldate = {2025-05-27},
	date = {2017-10},
	keywords = {{QALD}, Question Answering Datasets, {SimpleQuestions}, {SimpleQuestionsWikidata}, {WDAquaCore}0Questions, {WebQuestions}, Wikidata},
	file = {HAL PDF Full Text:C\:\\Users\\Marco\\Zotero\\storage\\E5R3W46K\\Diefenbach et al. - 2017 - Question Answering Benchmarks for Wikidata.pdf:application/pdf},
}

@software{schneider_replication_2025,
	location = {Karlsruhe},
	title = {Replication Package of "{HubLink}: Leveraging Language Models for Enhanced Scholarly Information Retrieval on Research Knowledge Graphs"},
	url = {https://gitlab.kit.edu/kit/kastel/sdq/stud/abschlussarbeiten/masterarbeiten/marco-schneider/ma-schneider-implementation.git},
	author = {Schneider, Marco},
	date = {2025},
}
