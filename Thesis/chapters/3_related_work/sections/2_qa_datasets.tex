
\section{Benchmarking KGQA Systems}

To evaluate the performance of \gls{kgqa} systems, numerous datasets have been introduced. A survey on \gls{kgqa} methods and their benchmarks shows that the majority of current research is focused on the open domain \cite{peng_graph_2024}. Most of these benchmarks target general-purpose encyclopedic \glspl{kg} such as Freebase \cite{bollacker_freebase_2008}, DBpedia \cite{auer_dbpedia_2007}, or Wikidata \cite{vrandecic_wikidata_2014}. Notably, there is an underrepresentation of \gls{qa} datasets targeting the academic domain. This domain presents unique challenges, as scholarly articles can be difficult to interpret, even for human experts \cite{saikh_scienceqa_2022}.

In this section, we introduce related work that is relevant to our third Contribution \hyperref[enum:c3]{\textbf{C3}}, the \gls{kgqa} dataset, and the construction approach that we applied. We first introduce \gls{kgqa} datasets that target the open domain. Then, we focus on \gls{kgqa} datasets specifically for the scholarly domain. Last, we introduce dataset construction methodologies related to our approach.

\subsection{Datasets Targeting Open-Domain Knowledge Graphs}

Early \gls{kgqa} datasets focused on the Freebase graph, which was a large open-domain graph hosted by Google \cite{berant_semantic_2013}. \textsc{Free917} \cite{cai_large-scale_2013}, for example, includes 917 questions formulated by two domain experts and manually annotated with formal queries, specifically targeting semantic parsing benchmarks. Additionally, \textsc{WebQuestions} \cite{berant_semantic_2013} consists of 5,810 factoid question-answer pairs targeting Freebase data. The dataset is relatively simple in complexity, as many questions involve only a single class, property, or instance. Later, it was extended to create WebQuestionsSP \cite{yih_value_2016} for semantic parsing benchmarking, associating SPARQL queries with 4,737 questions. A larger dataset focused on Freebase is \textsc{SimpleQuestions} \cite{bordes_large-scale_2015}, which consists of 108,442 factoid questions paired with answers. Due to its popularity, the dataset was later converted to \textsc{SimpleQuestionsWikidata} by \textcite{diefenbach_question_2017}, who translated the question and answer pairs to the Wikidata graph.

The datasets mentioned above primarily feature simple factoid questions. Other datasets were developed to incorporate more complex question types. \textsc{ComplexWebQuestions} \cite{talmor_web_2018} is one of the first datasets featuring questions with superlatives and comparatives. It was created based on the \textsc{WebQuestionsSP} dataset and therefore also targets the Freebase graph. Similar complexity is exhibited by the \textsc{Large-Scale Complex Question Answering Dataset (LC-QuAD)} \cite{trivedi_lc-quad_2017}. It comprises over 5,000 questions coupled with SPARQL queries necessary to obtain the answers from the DBpedia graph. The dataset was further developed by the authors, who released a second version: \textsc{LC-QuAD 2.0} \cite{dubey_lc-quad_2019}. This updated dataset is larger than its predecessor, containing more than 30,000 questions that span both the Wikidata and DBpedia graphs. In addition, it includes a wider variety of question types and multiple paraphrases for each question. The \textsc{Question Answering over Linked Data (QALD)} challenges\footnote{\url{http://qald.aksw.org/} [last accessed 23.09.2024]} aim to advance natural language interfaces for querying knowledge graphs like DBpedia and Wikidata. The most recent benchmark, QALD-10 \cite{usbeck_qald-10_2023}, was published in 2022 and comprises 394 manually created questions derived from Wikidata, covering different levels of complexity.  Each question is annotated with a manually defined SPARQL query and its corresponding output. In addition, the dataset contains questions in various languages, including English, German, Russian, and Chinese. The dataset features simple factual, comparative, superlative, count-based, and temporal questions.

In particular, all \gls{kgqa} datasets in this section are mainly encyclopedic knowledge graphs focusing on the open domain. Our dataset specifically targets the \emph{scholarly literature search} task in the \gls{swa} domain. The dataset contains a classification of questions into eight different retrieval operations and six different use cases specifically for the literature search. This makes it possible to find out how well \gls{kgqa} approaches work, especially with this task.

\subsection{Datasets Targeting the Scholarly Domain}

While most \gls{kgqa} datasets target open-domain knowledge bases, efforts have emerged to construct benchmarks specifically for the scholarly domain. Several initiatives have targeted the \gls{orkg} as the underlying knowledge base. \textsc{ORKG-QA} \cite{jaradeh_question_2020} represents one of the first datasets built upon the \gls{orkg}. The dataset comprises 80 question-answer pairs based on 13 tables extracted from the \gls{orkg}, representing structured comparisons of over 100 academic publications. \textcite{karras_divide_2023} provide a set of 77 competency questions to evaluate empirical research targeting the \gls{swa} domain. However, unlike our proposed dataset, these works do not explicitly categorize questions based on specific information retrieval operations relevant to literature search.

Recent scholarly datasets feature larger scale and greater complexity and include retrieval operation-related types. \textsc{SciQA} \cite{auer_sciqa_2023} provides 2,465 questions in the scholarly domain, with the \gls{orkg} as the underlying knowledge base. These questions are predominantly complex, requiring reasoning over multiple publications. Each question was generated with a template that corresponds to a typical scholarly inquiry to ensure its relevance. A larger dataset, \textsc{DBLP-QuAD} \cite{banerjee_dblp-quad_2023}, consists of over 10,000 question-SPARQL pairs designed for \gls{qa} over the DBLP computer science bibliography graph. The dataset comprises various question types, each presenting different levels of complexity for the retrieval process. Although these datasets address complex questions within the academic domain and also provide retrieval operation types similar to our \gls{kgqa} dataset, the primary distinguishing feature of our dataset is its explicit focus on the literature search process. This is achieved by structuring the questions according to six specific literature search use cases, each incorporating relevant metadata and content constraints, allowing a targeted evaluation of \gls{kgqa} systems for these practical scenarios.

% Notably, our dataset specifically targets the literature search with several questions asking specifically for multiple papers that conform to specific metadata or content data constraints.


\subsection[Methodologies for QA Dataset Construction]{Methodologies for Question Answering Dataset Construction}

Several strategies have been developed for generating \gls{kgqa} datasets. The most straightforward way is to handcraft the datasets. For example, in \textsc{ORKG-QA} \cite{jaradeh_question_2020},  the authors collected 13 tables from the \gls{orkg}, representing structured comparisons, from which they manually formulated 80 natural language questions covering diverse types, including direct, aggregation, and relational queries. Additionally, \textcite{karras_divide_2023} manually developed 77 competency questions for the \gls{orkg}, specifically targeting empirical research in software and requirements engineering. Similarly, \textsc{Free917} \cite{cai_large-scale_2013} was created by two domain experts who manually generated 917 natural language questions accompanied by formal queries.

Another category of approaches employs crowd-sourcing techniques. Although this method typically does not involve domain experts for question creation, outsourcing allows for scaling datasets to larger sizes. For example, \textsc{WebQuestions} \cite{berant_semantic_2013} was created by obtaining 100,000 candidate questions from the Google Suggest API and forwarding a random selection to Amazon Mechanical Turk. The human workers on the platform were assigned to annotate answerable questions using Freebase. The construction of \textsc{SimpleQuestions} \cite{bordes_large-scale_2015} followed a similar crowd-sourcing process. First, a set of \gls{rdf} triples was selected from Freebase. Then, the annotators were asked to write a natural language question for each triple in which the object serves as the answer, incorporating the subject and predicate. Amazon Mechanical Turk workers were also employed for the construction of \textsc{ComplexWebQuestions} \cite{talmor_web_2018}. In this case, their task was to paraphrase existing questions and to identify corresponding answers in Freebase.

A different technique of constructing \gls{kgqa} datasets involves the instantiation of SPARQL templates. For example, \textsc{SciQA} \cite{auer_sciqa_2023} starts with a small set of manually authored question-SPARQL pairs over the \gls{orkg}. SPARQL templates are prepared, and an \gls{llm} is subsequently used to populate these templates automatically with entities and predicates, generating several thousand additional questions. Similarly, \textsc{DBLP-QuAD} \cite{banerjee_dblp-quad_2023} originates from 98 manually defined SPARQL and natural-language query templates. By sampling two-hop subgraphs from the DBLP graph and augmenting them through paraphrasing, the authors synthesized 10,000 questionâ€“query pairs. In a similar manner, \textsc{LC-QuAD} \cite{trivedi_lc-quad_2017}, which uses the DBpedia graph, employs manually defined SPARQL query templates parameterized by seed entities and a predicate whitelist. These templates were instantiated using two-hop subgraphs to produce the dataset.

Although our method also involves manually generated templates, our templates are fundamentally different. They consist of questions in natural language that contain placeholders for missing constraints rather than being based on SPARQL query structures. We provide these natural language templates to an \gls{llm} tasked with generating analogous questions populated with relevant entities and relations derived from the underlying knowledge graph data. A potential limitation of strictly template-based SPARQL generation is that it can constrain the resulting questions to the explicit entities and relations defined in the templates, potentially overlooking semantically related concepts. In contrast, our approach leverages dense vector representations of entities and relations. This allows for the generation of more complex and nuanced questions, including those involving entities that are semantically related rather than just structurally connected according to predefined templates.

However, employing embedding-based techniques for generating questions is not a novel concept. Another research direction leverages \gls{rag} with \glspl{llm} to index the context and generate questions from it. For example, \cite{pan_rag_2024} employs a \gls{rag} approach to specifically generate competency questions during ontology engineering. In their method, relevant text chunks from a corpus of scientific papers are retrieved based on embedding similarity and provided as context to an \gls{llm} to formulate questions. Although this technique uses embeddings for retrieval, its primary knowledge source consists of external documents rather than a \gls{kg}.


% Alternative methods also incorporate multiple sources into the generation process. \textsc{Hybrid-SQuAD} \cite{taffa_hybrid-squad_2024} generated a diverse dataset that contains DBLP and SemOpenAlex for structured content and Wikipedia for unstructured text.

% The approaches that are common for the creation of QA datasets are crowdsourcing, automatic generation, and collecting question-answer pairs from community-based QA platforms \cite{taffa_hybrid-squad_2024}.


% Instead of using SPARQL queries, the \textsc{ScienceQA} \cite{saikh_scienceqa_2022} dataset was semi-automatically created using abstracts from accepted IJCAI conference articles. Each abstract was tokenized, and a parser was applied to extract noun phrases. These phrases have then been used in an answer-aware question generator model, which yields questions that are then reviewed by a human annotator. -> NOT KGQA