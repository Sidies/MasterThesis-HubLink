
\section{Evaluation Framework and Metrics}
\label{sec:exp_prelim_evaluation_framework}

The \gls{kgqa} approaches that we are testing in our thesis conform to the principles of \gls{rag}, because they retrieve information that is then used to improve the generation of an answer. Consequently, we employ an evaluation framework for \gls{rag}-based systems. A popular framework is \gls{rgar} \cite{yu_evaluation_2024}, which will serve as the basis for structuring our evaluation. The framework is divided into three modules. The first module, called \emph{Evaluation Target}, defines the direction of the evaluation. The second module is \emph{Evaluation Dataset}, which outlines how the datasets were created and selected. The last module is \emph{Evaluation Metric}, which presents the specific metrics used to evaluate the targets based on the datasets. 

In the following, we first introduce the evaluation targets differentiating between retrieval and generation objectives. Then, we briefly explain the datasets that were applied in the experiments. Finally, we introduce the \gls{gqm} plan that shows how each target is evaluated using specific questions and metrics.


\subsection{Evaluation Targets}
\label{sec:evaluation_targets}

This section introduces the evaluation targets, which are the constructs intended to be evaluated during the evaluation. Each evaluation target is defined using \glspl{eo} and \glspl{gt}.

\subsubsection{Outputs and Ground Truth}

\glspl{eo} are the actual outputs of the \gls{kgqa} approach, while \glspl{gt} are the expected outputs as defined in the \gls{kgqa} dataset. We define the following \glspl{eo} and \glspl{gt} for our evaluation:

\begin{enumerate}[label={}, leftmargin=5em]
    \item[\textbf{EO1}] \label{enum:eo1} Retrieved Triples
    \item[\textbf{EO2}] \label{enum:eo2} Generated Answer
    \item[\textbf{GT1}] \label{enum:gt1} Golden Triples
    \item[\textbf{GT2}] \label{enum:gt2} Golden Answer
\end{enumerate}

Each \gls{kgqa} approach returns two types of outputs in our setting. The first output is \textbf{EO1}, which represents the triples that have been retrieved from the \gls{orkg} and rated by the approach as relevant to answering the provided question. Second, \textbf{EO2} is the answer that has been generated by an \gls{llm} based on the retrieved triples and the question.

To evaluate the outputs of the retriever, the \gls{kgqa} dataset provides two types of ground truth data. First, \textbf{GT1} are the golden triples, which are exactly those triples that are required to fully answer the question that was asked. Consequently, the retriever has to find all golden triples to be entirely successful at the retrieval task. Second, \textbf{GT2} is a golden answer formulated in natural language, which provides a response to the question and is based on the golden triples. 

\subsubsection{Evaluation Targets}
The combination of \glspl{eo} and \glspl{gt} generates the evaluation targets, which are subdivided into targets for retrieval (\emph{ReT}) and for generation (\emph{GeT}):

\begin{enumerate}[label={}]
    % Triples to Golden Triples
    \item \textbf{ReT1}\label{enum:ret1} EO1 \(\leftrightarrow \) GT1
     % Gen Answer to Golden Answer
    \item \textbf{GeT1}\label{enum:get3} EO2 \(\leftrightarrow \) GT2
    % Gen Answer to Question
    \item \textbf{GeT2} \label{enum:get1} EO2 \(\leftrightarrow \) Question
    % Gen Answer to Triples
    \item \textbf{GeT3} \label{enum:get2} EO2 \(\leftrightarrow \) GT1
\end{enumerate}

The targets introduced above guide the evaluation process. The retrieval target (\hyperref[enum:get1]{\textbf{ReT1}}) assesses the effectiveness of the context retrieval phase by comparing the retrieved triples (\hyperref[enum:eo1]{\textbf{EO1}}) with the golden triples (\hyperref[enum:gt1]{\textbf{GT1}}). This evaluation focuses on accuracy, relevance, and robustness. Here, accuracy is about the extent to which the retrieval approach is capable of fetching the desired triples (\hyperref[enum:get1]{\textbf{GT1}}) from the graph. Relevance refers to the degree to which the retrieved triples (\hyperref[enum:get1]{\textbf{EO1}}) correspond to the golden triples (\hyperref[enum:get1]{\textbf{GT1}}), where more irrelevant triples result in lower overall relevance. Finally, robustness refers to the ability of the retrieval system to maintain consistent performance in terms of accuracy and relevance despite variations encountered in the complexity of input questions (e.g., phrasing, the presence or absence of explicit type information, diverse use case requirements, and structural or lexical variability) across different knowledge graph schemas. The general intent behind \hyperref[enum:get1]{\textbf{ReT1}} is therefore to quantify how well the retriever identifies the correct information and how resilient its performance is under various operational conditions.

With regard to the generation targets, \hyperref[enum:get1]{\textbf{GeT1}} evaluates the quality of the generated answer by assessing its semantic and factual alignment with the reference ground truth answer (\hyperref[enum:get1]{\textbf{GT2}}). This target aims to measure how closely the generated answer of the system (\hyperref[enum:get1]{\textbf{EO2}}) matches an ideal and correct answer derived from the ground truth. The focus is on both the meaning and the accuracy of the information presented. High performance on this target indicates that the system can produce answers that are similar to a known correct answer.

\hyperref[enum:get2]{\textbf{GeT2}} evaluates how well the generated answer (\hyperref[enum:get1]{\textbf{EO2}}) aligns with the intent and content requirements of the input question. This involves assessing whether the answer directly addresses the topic posed by the question and fulfills any specific instructions or constraints given, such as performing a ranking or counting operation. Consequently, this target measures the usefulness and appropriateness of the generated answer in the context of the question.

Finally, \hyperref[enum:get3]{\textbf{GeT3}} evaluates the faithfulness and adherence to the source material of the generated answer (\hyperref[enum:get1]{\textbf{EO2}}) relative to the retrieved triples (\hyperref[enum:get1]{\textbf{EO1}}). It evaluates whether the statements and claims made in the generated answer accurately and exclusively reflect the information present within the retrieved triples. 

\subsection{Evaluation Datasets}
\label{sec:evaluation_datasets}

To evaluate the defined targets, datasets are needed to conduct the experiments. In Section~\ref{sec:implementation_qa_dataset_generation} we document the creation of \gls{kgqa} datasets. These datasets have been used to conduct the experiments and evaluate the targets.

\subsection{Evaluation Plan}
\label{sec:evaluation_goals_and_metrics}

This section details the evaluation plan designed to assess the performance of the proposed HubLink system against baseline approaches. Following the introduction of the evaluation targets in Section~\ref{sec:evaluation_targets}, we adopt the \gls{gqm} method \cite{basili_methodology_1984,basili_software_1992}. Consequently, we present a detailed \gls{gqm} plan that systematically links each evaluation target (serving as a goal) with specific questions, which are in turn addressed using a defined set of quantitative metrics. To aggregate the metrics across all questions, we are using the \emph{macro-averaging} strategy (see Section~\ref{sec:fundamentals_evaluation_rag}) because it attributes equal weight to each individual question without favoring those questions that request more triples.

In the following, we present each of the goals with their corresponding set of questions and metrics. We begin with the retrieval, followed by the generation.



\subsubsection{Evaluation of the Retrieval Performance} 

The initial evaluation target, \hyperref[enum:ret1]{\textbf{ReT1}}, centers on assessing the relevance and robustness of the contexts retrieved by HubLink. Several questions arise from this goal, evaluated using a suite of seven metrics. Standard information retrieval metrics, namely \emph{Precision}, \emph{Recall}, and \emph{F1} are calculated based on the formulas presented by \textcite{yu_evaluation_2024}. The determination of \gls{tp}, \gls{fp}, and \gls{fn} involves comparing the triples retrieved by the system with the ground-truth triples provided within the reference \gls{kgqa} dataset. The \emph{Accuracy} metric is omitted from this evaluation, as its calculation requires knowledge of \gls{tn}, which requires a complete listing of all irrelevant contexts, which is not available in our \gls{kgqa} datasets.

To evaluate the ability of the retriever to rank relevant contexts highly, rank-aware metrics are incorporated. Specifically, \emph{MAP@k} and \emph{MRR@k} are adapted from the implementations of \textcite{tang_multihop-rag_2024}. The \emph{Hits@k} metric is implemented following the definition provided by the AmpliGraph library\footnote{\url{https://docs.ampligraph.org/en/1.2.0/generated/ampligraph.evaluation.hits_at_n_score.html} [last accessed on 17.01.2025]}. In addition, \emph{EM@k} is included, calculated according to the definition of \textcite{ibrahim_survey_2024}. 

With regard to the required parameter $k$ that determines the number of top contexts considered, we have set this parameter to 10. This choice reflects the maximum number of ground truth triples associated with any single question in our \gls{kgqa} datasets, as detailed in Section~\ref{sec:label_based_qa_dataset}.

Furthermore, we test the environmental impact of the retrieval using the absolute, relative, and delta carbon emissions using the metrics specified by \cite{kaplan_responsible_2025}. The tracking of the emissions is facilitated by the Codecarbon library\footnote{\url{https://github.com/mlco2/codecarbon} [Accessed: 2025-01-17]}, which tracks the \emph{CPU Energy Consumption}, \emph{GPU Energy Consumption}, and \emph{RAM Energy Consumption}, measured in watts and then converts those measurements to estimate the total \emph{Carbon Emissions} as \(CO_2\) equivalent. Moreover, to evaluate runtime and token costs, we measure \emph{System Latency} as the total time taken to retrieve the context and generate an answer while also tracking the number of \emph{LLM Tokens} consumed during the retrieval and generation phases.


% Beyond the \gls{gqm} framework, several measurements are tracked to quantify practical operational aspects. \emph{System Latency} is measured as the total time elapsed to retrieve the context and generate an answer. Sustainability considerations are addressed by tracking energy consumption, specifically \emph{CPU Energy Consumption}, \emph{GPU Energy Consumption}, and \emph{RAM Energy Consumption}, measured in watts. These energy measurements are aggregated to estimate the total \emph{Carbon Emissions} as \(CO_2\) equivalent. The tracking of energy and emissions is implemented using the Codecarbon library\footnote{\url{https://github.com/mlco2/codecarbon} [Accessed: 2025-01-17]}. Finally, the computational cost associated with the usage of the \gls{llm} is monitored by tracking the number of \emph{LLM Tokens} consumed during the retrieval and generation phases.


\subsubsection{Evaluation of the Generation Performance}

Transitioning to answer generation, the target \hyperref[enum:get1]{\textbf{GeT1}} focuses on comparing the factual and semantic similarity between the generated answer of the \gls{kgqa} approach and the golden answer in the \gls{kgqa} dataset. Lexical similarity is measured using \emph{BLEU}\footnote{\url{https://pypi.org/project/sacrebleu/} [last accessed on 17.01.2025]} and \emph{ROUGE}\footnote{\url{https://pypi.org/project/rouge-score/} [last accessed on 17.01.2025]}, employing their respective standard Python implementations. Semantic similarity is assessed using \emph{BertScore}\footnote{\url{https://github.com/Tiiiger/bert_score} [last accessed on 17.01.2025]} utilizing the official implementation that provides Precision, Recall, and F1 variants based on contextual embeddings. Further evaluation leverages \gls{llm}-as-a-judge metrics provided by the RAGAS framework\footnote{\url{https://github.com/explodinggradients/ragas} [last accessed on 17.01.2025]}. This includes \emph{Factual Correctness}, where an \gls{llm} decomposes both the generated and reference answers into claims to calculate Precision, Recall, and F1 based on the comparisons of those claims \cite{es_ragas_2023}. We further use the \emph{Semantic Similarity} metric, which calculates the cosine similarity and the \emph{String Similarity} metric, which calculates the Levenshtein distance between the generated answer and the golden answer. Both metrics are also provided by the RAGAS framework.

The subsequent generation target, \hyperref[enum:get2]{\textbf{GeT2}}, addresses the alignment of the generated answers with the intent of the question, encompassing both semantic content and instructional constraints. This target involves two specific questions. The first, referring to semantic alignment, is evaluated using the \emph{Answer Relevancy} metric from RAGAS. This metric uses an \gls{llm} to generate plausible questions based solely on the generated answer and then measures the semantic similarity (cosine similarity) between these generated questions and the original question \cite{es_ragas_2023}. The rationale is that a relevant answer should allow an accurate reconstruction of the original query. The second question, which focuses on adherence to instructional intent (e.g., performing a ranking task as requested), is assessed using a custom implementation that we developed on our own. In this process, an \gls{llm} evaluates the generated answer against the specific instructions embedded within the question to determine the degree of compliance.

The final generation target \hyperref[enum:get3]{\textbf{GeT3}} concerns the faithfulness of the generated answer to the information contained within the retrieved contexts. This is evaluated using the \emph{Faithfulness} metric from the RAGAS framework. This metric utilizes an \gls{llm} to break down the generated answer into individual statements and verifies each statement against the provided retrieved context  \cite{es_ragas_2023}. This means that the metric evaluates whether the answer was actually derived from the context or from the internal knowledge of the \gls{llm}. We use this to evaluate whether the triples that were retrieved are actually reflected in the generated answer. 

\vspace{\medskipamount}
{\large\textbf{Goal-Question-Metric Plan:}} 
\label{sec:evaluation_gqm_plan}

\textbf{Ret1} Assess the relevance and robustness of retrieved contexts in scholarly literature search.
\begin{enumerate}[label={}]
    \item \textbf{Q1} To what extent does the HubLink retrieval algorithm improve context relevance and accuracy compared to baseline KGQA methods in scholarly literature search?
    \begin{enumerate}[label={}]
        \item \textbf{M1.1} Precision, \textbf{M1.2} Recall, \textbf{M1.3} F1, \textbf{M1.4} Hits@k, \textbf{M1.5} EM@k, \textbf{M1.6} MRR@k, \textbf{M1.7} MAP@k
    \end{enumerate}
    \item \textbf{Q2} How does retrieval performance vary with the complexity of operations required by different scholarly questions?
    \begin{enumerate}[label={}]
        \item \textbf{(see Q1.1)}
    \end{enumerate}
    \item \textbf{Q3} How does retrieval performance vary across distinct scholarly literature-search use cases?
    \begin{enumerate}[label={}]
        \item \textbf{(see Q1.1)}
    \end{enumerate}
    \item \textbf{Q4} What impact does the presence or absence of explicit type information in questions have on the retrieval performance?
    \begin{enumerate}[label={}]
        \item \textbf{(see Q1.1)}
    \end{enumerate}
    \item \textbf{Q5} How robust is the proposed approach to structural and lexical variability across alternative knowledge graph schema representations?
    \begin{enumerate}[label={}]
        \item \textbf{(see Q1.1)}
    \end{enumerate}
    \item \textbf{Q6} How efficient is the proposed approach considering runtime and language model tokens required when compared to baseline KGQA methods?
    \begin{enumerate}
        \item \textbf{M1.8} Runtime per question, \textbf{M1.9} LLM tokens per question
    \end{enumerate}
    \item \textbf{Q7} How does the proposed approach compare with regard to the environmental impact when compared to baseline KGQA methods?
    \begin{enumerate}
        \item \textbf{M1.10} Absolute carbon emissions, \textbf{M1.11} Relative carbon emissions, \textbf{M1.12} Delta carbon emissions
    \end{enumerate}
\end{enumerate}
% 
\textbf{GeT1} Evaluate how well the generated answer aligns semantically and factually with reference answers.
\begin{enumerate}[label={}]
    \item \textbf{Q8} How semantically and factually consistent are the generated answers of the proposed approach when compared to answers generated by baseline KGQA approaches?
    \begin{enumerate}[label={}]
        \item \textbf{M2.1} BLEU, \textbf{M2.2} ROUGE, \textbf{M2.3} Semantic Similarity, \textbf{M2.4} String Similarity, \textbf{M2.5} Bert Precision, \textbf{M2.6} Bert Recall, \textbf{M2.7} Bert F1, \textbf{M2.8} Factual Correctness Precision, \textbf{M2.9} Factual Correctness Recall, \textbf{M2.10} Factual Correctness F1 \\
    \end{enumerate}
\end{enumerate}
%
\textbf{GeT2} Evaluate how well the generated answer aligns with the intent and content of the question.
\begin{enumerate}[label={}]
    \item \textbf{Q9} To what extent do the answers generated by HubLink reflect the semantic intent of scholarly questions when compared to baseline KGQA approaches?
    \begin{enumerate}[label={}]
        \item \textbf{M3.1} Answer Relevancy
    \end{enumerate}
    \item \textbf{Q10} To what extent do the generated answers follow the instructional expectations of scholarly questions when compared to baseline KGQA approaches?
    \begin{enumerate}[label={}]
        \item \textbf{M3.2} Instruction Following
    \end{enumerate}
\end{enumerate}
% 
\textbf{GeT3} Evaluate how well the generated answer aligns with the retrieved context.
\begin{enumerate}[label={}]
    \item \textbf{Q11} To what extent are generated answers of HubLink faithful to the retrieved context and free from unsupported claims when compared to baseline KGQA approaches?
    \begin{enumerate}[label={}]
        \item \textbf{M4.1} Faithfulness
    \end{enumerate}
\end{enumerate}



