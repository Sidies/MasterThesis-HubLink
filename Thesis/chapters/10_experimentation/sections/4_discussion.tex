% As indicated by our evaluation, HubLink offers significant advantages in comprehensive information retrieval and handling complex questions within scholarly \gls{kg} retrieval for literature search. Its main challenges lie in precision, ranking refinement, and generating concise, tightly focused answers. This section discusses the key findings from the evaluation of our proposed HubLink approach (\hyperref[enum:c1]{\textbf{C1}}) focusing on the implications of the performance in retrieving information from the \gls{kg} and generating answers for the scholarly literature search task.


\section{Discussion on Evaluation Results}
\label{sec:discussion_on_evaluation_results}


In the following, we discuss the interpretation of the evaluation results concerning our proposed HubLink approach. This discussion focuses on the two primary aspects evaluated: data retrieval from the \gls{kg} and the subsequent answer generation. Furthermore, as the baseline approaches yielded notably low scores, we will discuss their results separately.

\subsection{Analysis of Retrieval Performance}

Our evaluation indicates that HubLink presents a considerable advancement in retrieving scholarly information from \glspl{rkg} compared to existing baseline methods, positioning it as a promising approach for scholarly literature searches. The consistently high recall observed across our tests underscores that the core strength of HubLink lies in its capacity to identify and retrieve a comprehensive set of relevant triples from the \gls{kg}. This capacity remains robust when HubLink is applied to diverse graph structures, showcasing the schema-agnostic characteristic of the approach. The results further demonstrate the capability of HubLink to retrieve relevant information even when questions necessitate traversing multiple hops or when information is spread across broader paths. This multi-hop retrieval capability is particularly important for scholarly literature search tasks, where questions necessitate multi-hop reasoning to connect diverse pieces of information.

The evaluation also demonstrates that HubLink can effectively handle a range of retrieval operations, including basic factual lookups and moderately complex operations such as aggregation and ranking. In addition, the results show that HubLink can operate economically during the retrieval process. Specifically, a faster configuration of the approach achieves performance comparable to that of a more complex configuration while offering the benefits of reduced runtime, lower \gls{llm} token costs, and diminished environmental impact. This efficiency is particularly relevant for real-world applications, where efficiency and cost-effectiveness are crucial considerations.

However, the evaluation also highlights areas where HubLink faces challenges. Although HubLink achieves high recall, the moderate precision and ranking scores indicate difficulty in distinguishing relevant triples from less pertinent ones within the retrieved set. This suggests that HubLink retrieves a broader set of information than required, which includes noise alongside the relevant context. This challenge becomes even more pronounced for questions that require complex logical operations, such as negation or superlatives, and for use cases involving less structured content data. These limitations suggest that, while HubLink excels at retrieving information for answering scholarly questions, downstream filtering or reranking mechanisms may be necessary to refine the results for optimal precision and ranking quality.

\subsection{Analysis of Answer Generation Performance}
% While HubLink excels in the retrieval of relevant facts, the translation into consistently high-quality answers presents challenges.

With regard to the answer generation performance, HubLink demonstrates improved capacity over baseline approaches to generate relevant answers and adhere to specific instructions embedded within the query. This suggests that HubLink captures user intent more effectively, even for complex tasks. 

However, our analysis shows that the translation of retrieved facts into consistently high-quality answers presents limitations. Our evaluations indicate that HubLink only partially incorporates the retrieved factual knowledge into its generated answers. Furthermore, the absolute recall scores for answer generation are lower than those observed during retrieval, suggesting that not all relevant information from the retrieved context is integrated into the final answers. Moreover, lower scores in precision and similarity comparisons suggest that the answers generated by HubLink tend to be less concise and may deviate structurally from the expected reference answers. A reduced faithfulness score also indicates this trend. 

Overall, the generation component exhibits a tendency towards producing comprehensive yet potentially overly verbose answers rather than consistently concise presentations of information. Therefore, an important direction for future improvement is the refinement of the generation strategy to produce more focused and concise answers while still maintaining accuracy and relevance to the context.

\subsection{Analysis of Baseline Performances}

Our experimental results reveal that the evaluated baseline methods generally exhibit substantially lower retrieval performance compared to our proposed HubLink approach. In particular, FiDeLiS and Mindmap demonstrated low performance in the retrieval experiments, while StructGPT and ToG struggled to answer any question during the parameter selection process (see \autoref{ch:parameter_selection_process}). Consequently, we did not use StructGPT and ToG in further testing. In the following subsections, we examine the specific characteristics and limitations of each baseline method to understand these performance differences.

\subsubsection{DiFaR}

Among the baseline approaches evaluated, DiFaR \cite{baek_direct_2023} achieved the best overall retrieval performance across most metrics. This approach, similar to HubLink, is based on leveraging embeddings for retrieval tasks. The relative success of DiFaR compared to the non-embedding-based baselines suggests potential advantages of embedding strategies for the types of questions and graph structures used in our experiments.

However, HubLink consistently outperforms DiFaR. This comparison highlights that, while a general embedding-based strategy is beneficial, the specific retrieval mechanisms and greater complexity introduced by HubLink yield a justifiable improvement in performance over the DiFaR approach. 

\subsubsection{Mindmap}

The Mindmap approach \cite{wen_mindmap_2024} operates by constructing evidence subgraphs. However, it encountered difficulties in our experiments, demonstrating a limited ability to answer the questions correctly. During the analysis of the algorithm, we observed a substantial limitation that renders the Mindmap approach unsuitable for scholarly literature search in our experimental setup. The following example illustrates the issue with a representative question from our dataset. The fact that Mindmap is unable to answer this question highlights its fundamental limitation, which hinders its performance on many other questions in the dataset.

Given is the following question: \enquote{Who are the authors of the paper 'A Taxonomy of Blockchain-Based Systems for Architecture Design'?}. When processing this question, Mindmap first extracts entities from the query. A plausible extraction would yield the terms \emph{Authors}, \emph{Paper}, and the title \emph{A Taxonomy of Blockchain-Based Systems for Architecture Design}. After this term recognition process, the approach queries the graph to collect entities that match these terms with the highest similarity scores. These entities are then used to build evidence subgraphs.

Mindmap constructs two types of evidence subgraphs. The first is the path-based subgraph, which finds the shortest paths between the identified entity nodes and stores them. The second is the neighbor-based subgraph, which collects all one-hop neighbors for each identified entity. However, in our example, only one term in the question corresponds directly to an entity in the graph with a meaningful match: the title of the publication. The other two extracted terms (Authors, Paper) represent semantic types or concepts rather than specific entity nodes within the graph structure, so no corresponding entities that are meaningful are found. Consequently, the only way for Mindmap to answer the question correctly would be if the authors were stored as immediate neighbors of the entity containing the title of the publication. However, in the \gls{orkg} this is not the case, as the triples of the authors are stored deeper in the graph and are therefore not collected by the retrieval.

Consequently, Mindmap performs poorly in our experiments because many questions in the \gls{kgqa} dataset provide only a single known entity and ask for another unknown entity. To be able to correctly answer these questions, it is required to find the path from the known entity to the unknown entity, which the Mindmap approach is unable to do since it builds paths only between entities provided in the question. This highlights a fundamental limitation of the Mindmap retriever and explains why many questions in the \gls{kgqa} dataset have not been answered correctly. 


\subsubsection{Beam Search Retrievers}
The StructGPT \cite{jiang_structgpt_2023}, ToG \cite{sun_think--graph_2024}, and FiDeLiS \cite{sui_fidelis_2024} approaches all rely on the beam search algorithm for graph exploration. Our experiments suggest that these approaches perform poorly on the \gls{kgqa} dataset. During the analysis of these algorithms, we discovered two major issues that explain the low performance, which are discussed below.

\paragraph{Local Information Deficit:} The beam search algorithm explores the knowledge graph by iteratively expanding a limited set of the most promising nodes or paths up to a predefined depth. A key challenge arises from its decision-making process: the choice of which paths to retain in the beam at each step is based on local information associated with the current nodes or their immediate neighbors. If the relevance of a path towards the final answer is not apparent from this local context, the path may be pruned, even if further exploration along that path would eventually lead to the correct answer.

To illustrate, consider the question \enquote{Which papers have used an interview as a method for their evaluation?}. A potentially relevant path might be structured as:
\[
\textit{Paper Title Node} \xrightarrow{\text{hasContribution}} \textit{Contribution Node} \xrightarrow{\text{usesMethod}} \textit{'Interview' Node}
\]

When the beam search exploration reaches a specific \textit{Paper Title Node}, it must decide whether to keep exploring paths originating from it based primarily on information available at that node. The title itself, or even the immediate 1-hop neighbors, may provide insufficient evidence that this specific path will lead to the target \textit{'Interview' node} when traversing further. As suggested by our analysis of the number of hops in Section~\ref{sec:evaluation_robustness_to_structural}, this issue is even more pronounced for questions that require more than three hops to arrive at the answer. Because most of the questions in our experiment require a larger number of hops, this exaggerates the issue, leading to the premature pruning of relevant paths and degrading the overall performance of these retrievers.

\paragraph{Stochastic Selection:} The second challenge comes from the sheer breadth of the graph. Because only a limited number of entities can be expanded at each iteration, the selection of the most relevant candidates is critical. StructGPT and ToG rely on an \gls{llm} to classify the relevance of predicates and then consider entities connected via the selected predicates for the next beam exploration. If this set of candidate entities exceeds the predefined beam width threshold, the approaches randomly prune the entities to reduce the size of the candidate set. This introduces non-determinism and the risk of discarding correct entities or paths purely by chance.

FiDeLiS addresses this challenge by using embedding similarity to score and select entities instead of random sampling. This results in more deterministic and often more relevant entity selection, contributing to its generally better performance compared to StructGPT and ToG in our results. However, FiDeLiS remains constrained by the first limitation, which hinders its effectiveness on questions that require deeper graph traversal where relevance is not immediately apparent.



% Old Discussion on retrieval, more of a summary:

% The evaluation on the retrieval performance consistently demonstrate that HubLink significantly improves the relevance and accuracy of retrieved knowledge from the \gls{kg} compared to baseline \gls{kgqa} approaches. A key strength of HubLink is observed in its recall capability. The results show, that HubLink consistently outperforms all baseline methods in retrieving a larger proportion of the ground truth triples (see  \autoref{tab:q11:relevance_and_accuracy}). This superior recall suggest, that HubLink exhibits a substantially broader coverage of potentially relevant information for answering scholarly questions.

% Furthermore, analysis across different retrieval operations (see \autoref{tab:q12:retrieval_operation}) confirms that HubLink can handle the various operations outlined in the \gls{kgqa} retrieval taxonomy. It performs exceptionally well on basic factual lookup and demonstrates competence in moderately complex operations such as aggregation, counting, and ranking. However, the results also highlight limitations, particularly with operations that require more complex logical reasoning, such as negation, relationship identification, and superlative evaluation. Here, the performance degrades noticeably, especially in precision.

% Regarding the applicability to scholarly literature search tasks, the evaluation across six different use cases (see \autoref{tab:q13:use_cases}) indicates that HubLink effectively retrieves desired information in various scenarios, encompassing metadata-based, content-based and mixed questions. These results underscore the potential utility of the approach in supporting researchers with different search needs. Retrieval performance remains robust in terms of recall in these use cases, although precision and ranking effectiveness vary, with metadata-centric tasks yielding the best results.

% The investigation of the impact of explicit type information in the questions (see \autoref{tab:q14:semi_typed}) reveals only a marginal positive effect on performance. This suggests that HubLink is insensitive to lexical variations of terms and explicit type cues.

% Concerning the robustness to graph structure variability (see \autoref{tab:q5:different_graph_variants}), the evaluations demonstrate that while HubLink operates in a schema-agnostic manner, its performance is influenced by the underlying graph topology. The data suggests that shorter path lengths inside of the hub structures correlate with performance gains, particularly for precision and ranking metrics. Further analysis of the hop count (see \autoref{tab:q5:hops_results}) further reinforces the robustness of HubLink compared to the baselines, especially for information located deeper within the graph. The results show that baseline methods struggle significantly as the hop count increases, but HubLink maintains a comparatively high recall. This showcases the capability of HubLink in multi-hop reasoning tasks.

% Despite the clear advantages in the recall and robustness of HubLink, the evaluation also points to limitations in the assessment of context relevance. The precision values, although superior to baselines, remain relatively low across most experiments. Similarly, ranking metrics indicate that HubLink does not always prioritize the most relevant triples at the top ranks, especially for complex questions or use cases that involve content retrieval. This evaluation suggests that while HubLink successfully retrieves a comprehensive set of candidates, refining the filtering and ranking of these candidates remains an area for improvement.