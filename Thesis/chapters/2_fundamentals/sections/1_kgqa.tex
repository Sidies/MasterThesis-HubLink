
\section{Knowledge Graph Question Answering (KGQA)}

\acrfull{kgqa} presents a research area that focuses on empowering users to access information stored within a \gls{kg} by formulating questions in natural language \cite{banerjee_knowledge_2024,chakraborty_introduction_2021,chakraborty_introduction_2019,pan_unifying_2024,yani_better_2022}. The primary objective of \gls{kgqa} is to retrieve answers from a \gls{kg} based on a question posed in natural language. This eliminates the necessity for users to write formal query languages such as SPARQL or possess intricate knowledge of the underlying structure and vocabulary of the \gls{kg} \cite{chakraborty_introduction_2021,chakraborty_introduction_2019,sabou_survey_2017}.

Historically, the research objective of enabling automated systems to answer questions posed in natural language precedes the modern adoption of \glspl{kg}. Early efforts in \acrfull{qa} explored systems designed to respond to questions by querying structured \acrfull{kb} or databases with early systems dating back to the 1970s and 1980s. A significant surge in interest in natural language question answering occurred with the establishment of a \gls{qa} track at the Text Retrieval Conferences (TREC), beginning in 1999. \cite{hirschman_natural_2001}

Due to advancements in semantic web technologies and automated information processing, the creation of large volumes of structured information became feasible \cite{chakraborty_introduction_2019}. \glspl{kg} emerged as a prime instance of storing structured data, which model facts about entities and their relationships, often in collections of triples \cite{chakraborty_introduction_2019,hogan_knowledge_2022,ji_survey_2022}. Consequently, \gls{kgqa} systems were developed to provide an intuitive natural language interface to query this structured knowledge contained in \glspl{kg}. Over time, several terms have been used for this task, including \acrfull{kbqa} \cite{ehrlinger_towards_2016} and \acrfull{sqa} \cite{sabou_survey_2017}. However, the term \gls{kb} has often been used misleadingly in the literature as a synonym for \glspl{kg} \cite{ehrlinger_towards_2016}.

 As \gls{kgqa} research progressed, the focus from simple questions requiring the retrieval of single facts shifted to more complex ones requiring multi-hop reasoning or constraints \cite{fu_survey_2020,banerjee_knowledge_2024,chakraborty_introduction_2021}. Two primary research directions emerged for building \gls{kgqa} systems: Methods based on \emph{\gls{ir}} gather question-related information from the \gls{kg} and leverage it to optimize the generation process. \emph{\gls{sp}-based methods} focus on the translation of the natural language question into a formal query or logical form that can be executed directly against the \gls{kg}.

 Neural networks became integral to various aspects of \gls{kgqa} systems across both \gls{ir} and \gls{sp} paradigms \cite{chakraborty_introduction_2019,ji_survey_2022}. Employed neural network architectures include embedding models, Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and Graph Neural Networks (GNNs) \cite{chakraborty_introduction_2021,pan_unifying_2024,ji_survey_2022}. More recently, the advent of pre-trained \glspl{llm} has begun to impact research on \gls{kgqa}. Although some research explores whether \glspl{llm} themselves can replace \glspl{kg}, most research is directed at the unification of \glspl{llm} and \glspl{kg} \cite{pan_unifying_2024}. In current research, \glspl{llm} are integrated into \gls{kgqa} systems in various ways. For instance, they can act as entity/relation extractors to extract data from questions, are used in \gls{sp} approaches to translate questions into formal queries, or are used in \gls{ir}-based approaches to function as reasoners over retrieved \gls{kg} subgraphs \cite{pan_unifying_2024}.


% Questions answered by KGQA systems are often divided into the categories—simple and complex. The questions, which can be answered by considering a single fact, without any additional constraints are generally defined as simple questions. For instance, the question “What is the birthplace of Michael Crichton” is a simple question since it can be answered by only considering the following fact: hex:Chicago, ex:birthPlaceOf, ex:Michael_Crichtoni. In contrast, the question “What is the birthplace of Westworld's writer?” is considered complex since answering this question requires knowing both hex:Michael_Crichton, ex:writerOf, ex:Westworldi and hex:Chicago, ex:birthPlaceOf, ex:Michael_Crichtoni. \cite{chakraborty_introduction_2021}

% In the QA community, KGQA is often also called knowledge base question answering (KBQA). \cite{chakraborty_introduction_2021}



% \acrfull{kgqa} is a relatively new research field with no established definition. In some works, \gls{kgqa} is used synonymously with \gls{kbqa} \cite{fu_survey_2020,peng_graph_2024}