
\section{Large Language Models}
\label{sec:fundamentals_language_models}

\acrfullpl{llm} are pre-trained models with hundreds of billions of parameters generated by excessive training of very large corpora of text \cite{chung_scaling_2022}. These models are designed to estimate the probability distribution in a sequence of text, enabling them to perform a wide range of language-related tasks \cite{kojima_large_2023}. Recently, \glspl{llm} have demonstrated powerful capabilities in natural language understanding, expressiveness, general knowledge, and zero-shot generalization \cite{yang_give_2024}.

The architecture of most \glspl{llm} is derived from the \emph{Transformer} design, which includes encoder and decoder modules empowered by a self-attention mechanism \cite{pan_unifying_2024}. Based on their structure, \glspl{llm} can be broadly categorized into \emph{encoder-only}, \emph{decoder-only}, and \emph{encoder-decoder} models. Notable examples of these architectures are Flan T5 \cite{chung_scaling_2022} for the encoder-decoder framework, GPT-4 \cite{openai_gpt-4_2024} for the decoder-only framework, and DeBERTa \cite{he_deberta_2021} for the encoder-only framework. In this thesis, we focus on the decoder-only framework, as many state-of-the-art \glspl{llm} follow this architecture \cite{pan_unifying_2024}.

\subsection{Prompting} 
An important concept in leveraging \glspl{llm} is \emph{prompting}, which involves conditioning the model on a natural language instruction to guide its generation for desired tasks without any further training or gradient updates to its parameters \cite{wei_emergent_2022,kojima_large_2023}. Given this input prompt, the \gls{llm} then generates an answer in natural language. Popular prompting methods are \emph{few-shot prompting}, which involves adding a few examples \cite{brown_language_2020} or only adding instructions describing the task, which is referred to as \emph{zero-shot prompting} \cite{kojima_large_2023}. Another popular prompting method is Chain-of-Thought (CoT), where a chain-of-thought demonstration is provided during prompting \cite{wei_chain--thought_2023}. To maximize the effectiveness of \glspl{llm}, \emph{prompt engineering} is a new field that focuses on the creation and refinement of prompts \cite{pan_unifying_2024}.

\subsection{Limitations} 
Despite their impressive capabilities, \glspl{llm} face certain limitations. They are often described as black-box models, which means that they lack explainability and transparency, making it difficult for users to comprehend the reasoning behind their answers \cite{yang_give_2024,pan_unifying_2024}. Furthermore, \glspl{llm} are prone to generating factual errors or \emph{hallucinations} in which the \gls{llm} provides answers that contradict existing sources or lack evidence to support statements, which is particularly prominent when handling queries beyond their training data or when requiring current information \cite{gao_retrieval-augmented_2024,yang_give_2024}. Moreover, \glspl{llm} may not perform as well on domain-specific tasks as on general ones due to the limited availability of domain-specific corpus \cite{yang_give_2024}.

\subsection{Embedding Models}

A challenge concerning natural language processing is the representation of words and their associated meanings. One paradigm to approach this challenge is the use of vector representations in a continuous space. The fundamental idea behind these representations is that words that appear in similar contexts tend to have similar meanings. Consequently, within such a vector space, the proximity between word representations encodes semantic and syntactic regularities. These dense vectors are typically referred to as \emph{embeddings}. \cite{banerjee_knowledge_2024, pennington_glove_2014, mikolov_efficient_2013}

Early methods for learning representations of words, such as Word2Vec \cite{mikolov_efficient_2013} and GloVe \cite{pennington_glove_2014}, train word embeddings based on the co-occurrence statistics of words within a large text corpus. This class of embeddings is termed \emph{static} because the matrix produced during training is used without alteration. To address this limitation, \emph{dynamic} word embeddings utilize the advances of the transformer model to compute different representations for the same word depending on its context within the given sentence. Consequently, transformer-based pre-trained language models such as BERT \cite{devlin_bert_2019} have become relevant to solving the challenge of word representation. \cite{banerjee_knowledge_2024}


% \subsection{Graph Integration}
% Given these limitations and the complementary strengths of \glspl{kg} which explicitly store rich factual knowledge in a structured format, unifying \glspl{llm} and \glspl{kg} has become an active research direction \cite{pan_unifying_2024,peng_graph_2024}. Such a unification aims to leverage the advantages of both, enhancing the factual reasoning and interpretability of \gls{llm} answers while potentially using \glspl{llm} to aid in \gls{kg} construction and evolution \cite{pan_unifying_2024}.
 




% \subsection{Large Language Models}
% Research on PLMs has shown great success in scaling the model, which improves the capabilities of the model in downstream tasks, leading to surprising emerging abilities. Examples for such abilities that are not present in small models but arise in very large models are few-shot prompting, instruction following, and chain-of-thought \cite{wei_emergent_2022}.  

% Scaling PLMs to a certain threshold that emits these abilities is what distinguishes PLMs from \glspl{llm} \cite{yang_give_2024}. 

% This means that typically \glspl{llm} refer to PLMs that consist of hundreds of billions of parameters, with notable examples being: Flan T5 \cite{chung_scaling_2022} for the encoder-decoder framework, GPT-4 \cite{openai_gpt-4_2024} for the decoder-only framework, and DeBERTa \cite{he_deberta_2021} for the encoder-only framework. 

% 

% \subsection{Categorization}
% The architectural framework used in a PLM can be categorized into three types: encoder-only, decoder-only, and encoder-decoder \cite{wang_pre-trained_2023}. 

% \paragraph{Decoder-Only} 
% In the decoder-only approach, a unidirectional transformer decoder, operating from left to right, serves as the foundational mechanism during pre-training. This method employs an autoregressive technique for token prediction, in which each token is predicted based on its preceding tokens. Specifically, given a text sequence \(x = (x_1, x_2, x_3, ..., x_T)\), where \(x\) is the complete original sentence, \(x_t\) is the t-th token, and \(T\) is the total number of tokens, the model computes the probability of the sequence by the product \(p(x) = \prod^T_{t=1}p(x_t|x_{<t})\), representing the probability of each token given its previous sequence.

% \paragraph{Encoder-Only}
% Frameworks based on transformer encoders, such as the one used by BERT, employ bidirectional transformers to address the challenge of restoring altered tokens within sentences containing randomly masked elements. BERT exemplifies this approach through its use of a Masked Language Model (MLM) methodology, where specific tokens are substituted with a [MASK] token during the pre-training phase. The model then uses the surrounding contextual information to accurately predict the original tokens. Furthermore, BERT enhances its comprehension of text relationships by employing a Next Sentence Prediction (NSP) technique, which evaluates the links between consecutive sentences. This feature is beneficial for tasks that necessitate grasping sentence contexts, such as question answering.

% \paragraph{Encoder-Decoder}
% The encoder-decoder framework is tailored for training sequence-to-sequence (seq2seq) models. This process involves obscuring tokens within the source sequence and subsequently reconstructing them in the target sequence. Broadly, these frameworks are differentiated into two distinct types: 1) the seq2seq encoder-decoders, which integrate a bidirectional transformer encoder with a unidirectional transformer decoder, each posing independent parameters; and 2) the unified encoder-decoders, where both the bidirectional encoder and the unidirectional decoder are trained concurrently with the same set of parameters. This architecture is primarily devoted to tasks in natural language generation (NLG), as it equips the model to simultaneously manage tasks related to understanding and generating language.